<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../_static/doctools.js?v=888ff710"></script>
    <script src="../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>‘’’
Contains implementations of functions to read input data.
‘’’
from .dataset import stream_block_format_dataset</p>
<p>import tensorflow.compat.v1 as tf</p>
<dl>
<dt>def data_record_input_fn(</dt><dd><blockquote>
<div><p>files, batch_size, parse_fn,
num_threads=2, repeat=False, dataset_fn=None,
keep_rate=None, parts_downsampling_rate=None,
shards=None, shard_index=None, shuffle=True, shuffle_files=True, interleave=True,
initializable=False, log_tf_data_summaries=False,
<a href="#id1"><span class="problematic" id="id2">**</span></a>kwargs):</p>
</div></blockquote>
<p>“””
Returns a nested structure of tf.Tensors containing the next element.
Used by <code class="docutils literal notranslate"><span class="pre">train_input_fn</span></code> and <code class="docutils literal notranslate"><span class="pre">eval_input_fn</span></code> in DataRecordTrainer.
By default, works with DataRecord dataset for compressed partition files.</p>
<dl>
<dt>Args:</dt><dd><dl>
<dt>files:</dt><dd><p>List of files that will be parsed.</p>
</dd>
<dt>batch_size:</dt><dd><p>number of samples per batch.</p>
</dd>
<dt>parse_fn:</dt><dd><p>function passed to data loading for parsing individual data records.
Usually one of the decoder functions like <code class="docutils literal notranslate"><span class="pre">parsers.get_sparse_parse_fn</span></code>.</p>
</dd>
<dt>num_threads (optional):</dt><dd><p>number of threads used for loading data. Defaults to 2.</p>
</dd>
<dt>repeat (optional):</dt><dd><p>Repeat the dataset indefinitely. Defaults to False.
Useful when you want to use <code class="docutils literal notranslate"><span class="pre">train_steps</span></code> or <code class="docutils literal notranslate"><span class="pre">eval_steps</span></code>
greater than the size of the dataset
(otherwise Estimator.[train,evaluate] stops when the end of the dataset is reached).</p>
</dd>
<dt>dataset_fn (optional):</dt><dd><p>A function that modifies the dataset after it reads different interleaved parts files.
Defaults to:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">dataset_fn</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">parse_fn</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">parse_fn</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd>
<dt>keep_rate (optional):</dt><dd><p>A float value in (0.0, 1.0] that indicates to drop records according to the Bernoulli
distribution with p = 1 - keep_rate.
Defaults to None (no records dropped).</p>
</dd>
<dt>parts_downsampling_rate (optional):</dt><dd><p>A float value in (0.0, 1.0] that indicates the factor by which to downsample part files.
For example, a value of 0.2 means only 20 percent of part files become part of the dataset.</p>
</dd>
<dt>shards (optional):</dt><dd><p>Number of partitions to shard the dataset into. This is useful for codistillation
(<a class="reference external" href="https://arxiv.org/pdf/1804.03235.pdf">https://arxiv.org/pdf/1804.03235.pdf</a>) and other techniques that require each worker to
train on disjoint partitions of the dataset.
The dataset is not sharded by default.</p>
</dd>
<dt>shard_index (optional):</dt><dd><p>Which partition of the dataset to use if <code class="docutils literal notranslate"><span class="pre">shards</span></code> is set.</p>
</dd>
<dt>shuffle (optional):</dt><dd><p>Whether to shuffle the records. Defaults to True.</p>
</dd>
<dt>shuffle_files (optional):</dt><dd><p>Shuffle the list of files. Defaults to True.
When False, files are iterated in the order they are passed in.</p>
</dd>
<dt>interleave (optional):</dt><dd><p>Interleave records from multiple files in parallel. Defaults to True.</p>
</dd>
<dt>initializable (optional):</dt><dd><p>A boolean indicator. When the Dataset Iterator depends on some resource, e.g. a HashTable or
a Tensor, i.e. it’s an initializable iterator, set it to True. Otherwise, default value (false)
is used for most plain iterators.</p>
<dl class="simple">
<dt>log_tf_data_summaries (optional):</dt><dd><p>A boolean indicator denoting whether to add a <cite>tf.data.experimental.StatsAggregator</cite> to the
tf.data pipeline. This adds summaries of pipeline utilization and buffer sizes to the output
events files. This requires that <cite>initializable</cite> is <cite>True</cite> above.</p>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>Iterator of elements of the dataset.</p>
</dd>
</dl>
<p>“””
if not parse_fn:</p>
<blockquote>
<div><p>raise ValueError(“default_input_fn requires a parse_fn”)</p>
</div></blockquote>
<dl class="simple">
<dt>if log_tf_data_summaries and not initializable:</dt><dd><p>raise ValueError(“Require <cite>initializable</cite> if <cite>log_tf_data_summaries</cite>.”)</p>
</dd>
<dt>dataset = stream_block_format_dataset(</dt><dd><p>files=files,
parse_fn=parse_fn,
batch_size=batch_size,
repeat=repeat,
num_threads=num_threads,
dataset_fn=dataset_fn,
keep_rate=keep_rate,
parts_downsampling_rate=parts_downsampling_rate,
shards=shards,
shard_index=shard_index,
shuffle=shuffle,
shuffle_files=shuffle_files,
interleave=interleave,
<a href="#id3"><span class="problematic" id="id4">**</span></a>kwargs</p>
</dd>
</dl>
<p>)</p>
<p># Add a tf.data.experimental.StatsAggregator
# <a class="reference external" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/data/experimental/StatsAggregator">https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/data/experimental/StatsAggregator</a>
if log_tf_data_summaries:</p>
<blockquote>
<div><p>aggregator = tf.data.experimental.StatsAggregator()
options = tf.data.Options()
options.experimental_stats.aggregator = aggregator
dataset = dataset.with_options(options)
stats_summary = aggregator.get_summary()
tf.add_to_collection(tf.GraphKeys.SUMMARIES, stats_summary)</p>
</div></blockquote>
<dl class="simple">
<dt>if initializable:</dt><dd><p># when the data parsing dpends on some HashTable or Tensor, the iterator is initalizable and
# therefore we need to be run explicitly
iterator = dataset.make_initializable_iterator()
tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)</p>
</dd>
<dt>else:</dt><dd><p>iterator = dataset.make_one_shot_iterator()</p>
</dd>
</dl>
<p>return iterator.get_next()</p>
</dd>
</dl>
<p>default_input_fn = data_record_input_fn  # pylint: disable=invalid-name</p>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../_sources/twml/twml/input_fns.py.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>