<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>import datetime
import os
from typing import Callable, List, Optional, Tuple
import tensorflow as tf</p>
<p>import tml.common.checkpointing.snapshot as snapshot_lib
from tml.common.device import setup_and_get_device
from tml.core import config as tml_config_mod
import tml.core.custom_training_loop as ctl
from tml.core import debug_training_loop
from tml.core import losses
from tml.core.loss_type import LossType
from tml.model import maybe_shard_model</p>
<p>import tml.projects.home.recap.data.dataset as ds
import tml.projects.home.recap.config as recap_config_mod
import tml.projects.home.recap.optimizer as optimizer_mod</p>
<p># from tml.projects.home.recap import feature
import tml.projects.home.recap.model as model_mod
import torchmetrics as tm
import torch
import torch.distributed as dist
from torchrec.distributed.model_parallel import DistributedModelParallel</p>
<p>from absl import app, flags, logging</p>
<p>flags.DEFINE_string(“config_path”, None, “Path to hyperparameters for model.”)
flags.DEFINE_bool(“debug_loop”, False, “Run with debug loop (slow)”)</p>
<p>FLAGS = flags.FLAGS</p>
<dl>
<dt>def run(unused_argv: str, data_service_dispatcher: Optional[str] = None):</dt><dd><p>print(“#” * 100)</p>
<p>config = tml_config_mod.load_config_from_yaml(recap_config_mod.RecapConfig, FLAGS.config_path)
logging.info(“Config: %s”, config.pretty_print())</p>
<p>device = setup_and_get_device()</p>
<p># Always enable tensorfloat on supported devices.
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True</p>
<dl class="simple">
<dt>loss_fn = losses.build_multi_task_loss(</dt><dd><p>loss_type=LossType.BCE_WITH_LOGITS,
tasks=list(config.model.tasks.keys()),
pos_weights=[task.pos_weight for task in config.model.tasks.values()],</p>
</dd>
</dl>
<p>)</p>
<p># Since the prod model doesn’t use large embeddings, for now we won’t support them.
assert config.model.large_embeddings is None</p>
<dl class="simple">
<dt>train_dataset = ds.RecapDataset(</dt><dd><p>data_config=config.train_data,
dataset_service=data_service_dispatcher,
mode=recap_config_mod.JobMode.TRAIN,
compression=config.train_data.dataset_service_compression,
vocab_mapper=None,
repeat=True,</p>
</dd>
</dl>
<p>)</p>
<p>train_iterator = iter(train_dataset.to_dataloader())</p>
<p>torch_element_spec = train_dataset.torch_element_spec</p>
<dl class="simple">
<dt>model = model_mod.create_ranking_model(</dt><dd><p>data_spec=torch_element_spec[0],
config=config,
loss_fn=loss_fn,
device=device,</p>
</dd>
</dl>
<p>)</p>
<p>optimizer, scheduler = optimizer_mod.build_optimizer(model, config.optimizer, None)</p>
<p>model = maybe_shard_model(model, device)</p>
<p>datetime_str = datetime.datetime.now().strftime(“%Y_%m_%d_%H_%M”)
print(f”{datetime_str}n”, end=””)</p>
<dl class="simple">
<dt>if FLAGS.debug_loop:</dt><dd><p>logging.warning(“Running debug mode, slow!”)
train_mod = debug_training_loop</p>
</dd>
<dt>else:</dt><dd><p>train_mod = ctl</p>
</dd>
<dt>train_mod.train(</dt><dd><p>model=model,
optimizer=optimizer,
device=device,
save_dir=config.training.save_dir,
logging_interval=config.training.train_log_every_n,
train_steps=config.training.num_train_steps,
checkpoint_frequency=config.training.checkpoint_every_n,
dataset=train_iterator,
worker_batch_size=config.train_data.global_batch_size,
enable_amp=False,
initial_checkpoint_dir=config.training.initial_checkpoint_dir,
gradient_accumulation=config.training.gradient_accumulation,
scheduler=scheduler,</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>if __name__ == “__main__”:</dt><dd><p>app.run(run)</p>
</dd>
</dl>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../index.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../_sources/projects/home/recap/main.py.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>