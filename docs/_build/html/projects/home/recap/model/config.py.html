<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>“””Configuration for the main Recap model.”””</p>
<p>import enum
from typing import List, Optional, Dict</p>
<p>import tml.core.config as base_config
from tml.projects.home.recap.embedding import config as embedding_config</p>
<p>import pydantic</p>
<dl>
<dt>class DropoutConfig(base_config.BaseConfig):</dt><dd><p>“””Configuration for the dropout layer.”””</p>
<dl class="simple">
<dt>rate: pydantic.PositiveFloat = pydantic.Field(</dt><dd><p>0.1, description=”Fraction of inputs to be dropped.”</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>class LayerNormConfig(base_config.BaseConfig):</dt><dd><p>“””Configruation for the layer normalization.”””</p>
<dl class="simple">
<dt>epsilon: float = pydantic.Field(</dt><dd><p>1e-3, description=”Small float added to variance to avoid dividing by zero.”</p>
</dd>
</dl>
<p>)
axis: int = pydantic.Field(-1, description=”Axis or axes to normalize across.”)
center: bool = pydantic.Field(True, description=”Whether to add learnable center.”)
scale: bool = pydantic.Field(True, description=”Whether to add learnable scale.”)</p>
</dd>
<dt>class BatchNormConfig(base_config.BaseConfig):</dt><dd><p>“””Configuration of the batch normalization layer.”””</p>
<p>epsilon: pydantic.PositiveFloat = 1e-5
momentum: pydantic.PositiveFloat = 0.9
training_mode_at_inference_time: bool = False
use_renorm: bool = False
center: bool = pydantic.Field(True, description=”Whether to add learnable center.”)
scale: bool = pydantic.Field(True, description=”Whether to add learnable scale.”)</p>
</dd>
<dt>class DenseLayerConfig(base_config.BaseConfig):</dt><dd><p>layer_size: pydantic.PositiveInt
dropout: DropoutConfig = pydantic.Field(None, description=”Optional dropout config for layer.”)</p>
</dd>
<dt>class MlpConfig(base_config.BaseConfig):</dt><dd><p>“””Configuration for MLP model.”””</p>
<p>layer_sizes: List[pydantic.PositiveInt] = pydantic.Field(None, one_of=”mlp_layer_definition”)
layers: List[DenseLayerConfig] = pydantic.Field(None, one_of=”mlp_layer_definition”)</p>
</dd>
<dt>class BatchNormConfig(base_config.BaseConfig):</dt><dd><p>“””Configuration for the batch norm layer.”””</p>
<p>affine: bool = pydantic.Field(True, description=”Use affine transformation.”)
momentum: pydantic.PositiveFloat = pydantic.Field(</p>
<blockquote>
<div><p>0.1, description=”Forgetting parameter in moving average.”</p>
</div></blockquote>
<p>)</p>
</dd>
<dt>class DoubleNormLogConfig(base_config.BaseConfig):</dt><dd><p>batch_norm_config: Optional[BatchNormConfig] = pydantic.Field(None)
clip_magnitude: float = pydantic.Field(</p>
<blockquote>
<div><p>5.0, description=”Threshold to clip the normalized input values.”</p>
</div></blockquote>
<p>)
layer_norm_config: Optional[LayerNormConfig] = pydantic.Field(None)</p>
</dd>
<dt>class Log1pAbsConfig(base_config.BaseConfig):</dt><dd><p>“””Simple configuration where only the log transform is performed.”””</p>
</dd>
<dt>class ClipLog1pAbsConfig(base_config.BaseConfig):</dt><dd><dl class="simple">
<dt>clip_magnitude: pydantic.NonNegativeFloat = pydantic.Field(</dt><dd><p>3e38, description=”Threshold to clip the input values.”</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>class ZScoreLogConfig(base_config.BaseConfig):</dt><dd><p>analysis_path: str
schema_path: str = pydantic.Field(</p>
<blockquote>
<div><p>None,
description=”Schema path which feaure statistics are generated with. Can be different from scehma in data config.”,</p>
</div></blockquote>
<p>)
clip_magnitude: float = pydantic.Field(</p>
<blockquote>
<div><p>5.0, description=”Threshold to clip the normalized input values.”</p>
</div></blockquote>
<p>)
use_batch_norm: bool = pydantic.Field(</p>
<blockquote>
<div><p>False, description=”Option to use batch normalization on the inputs.”</p>
</div></blockquote>
<p>)
use_renorm: bool = pydantic.Field(</p>
<blockquote>
<div><p>False, description=”Option to use batch renormalization for trainig and serving consistency.”</p>
</div></blockquote>
<p>)
use_bq_stats: bool = pydantic.Field(</p>
<blockquote>
<div><p>False, description=”Option to load the partitioned json files from BQ as statistics.”</p>
</div></blockquote>
<p>)</p>
</dd>
<dt>class FeaturizationConfig(base_config.BaseConfig):</dt><dd><p>“””Configuration for featurization.”””</p>
<p>log1p_abs_config: Log1pAbsConfig = pydantic.Field(None, one_of=”featurization”)
clip_log1p_abs_config: ClipLog1pAbsConfig = pydantic.Field(None, one_of=”featurization”)
z_score_log_config: ZScoreLogConfig = pydantic.Field(None, one_of=”featurization”)
double_norm_log_config: DoubleNormLogConfig = pydantic.Field(None, one_of=”featurization”)
feature_names_to_concat: List[str] = pydantic.Field(</p>
<blockquote>
<div><p>[“binary”], description=”Feature names to concatenate as raw values with continuous features.”</p>
</div></blockquote>
<p>)</p>
</dd>
<dt>class DropoutConfig(base_config.BaseConfig):</dt><dd><p>“””Configuration for the dropout layer.”””</p>
<dl class="simple">
<dt>rate: pydantic.PositiveFloat = pydantic.Field(</dt><dd><p>0.1, description=”Fraction of inputs to be dropped.”</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>class MlpConfig(base_config.BaseConfig):</dt><dd><p>“””Configuration for MLP model.”””</p>
<p>layer_sizes: List[pydantic.PositiveInt]
batch_norm: BatchNormConfig = pydantic.Field(</p>
<blockquote>
<div><p>None, description=”Optional batch norm configuration.”</p>
</div></blockquote>
<p>)
dropout: DropoutConfig = pydantic.Field(None, description=”Optional dropout configuration.”)
final_layer_activation: bool = pydantic.Field(</p>
<blockquote>
<div><p>False, description=”Whether to include activation on final layer.”</p>
</div></blockquote>
<p>)</p>
</dd>
<dt>class DcnConfig(base_config.BaseConfig):</dt><dd><p>“””Config for DCN model.”””</p>
<p>poly_degree: pydantic.PositiveInt
projection_dim: pydantic.PositiveInt = pydantic.Field(</p>
<blockquote>
<div><p>None, description=”Factorizes main DCN matmul with projection.”</p>
</div></blockquote>
<p>)</p>
<dl class="simple">
<dt>parallel_mlp: Optional[MlpConfig] = pydantic.Field(</dt><dd><p>None, description=”Config for the mlp if used. If None, only the cross layers are used.”</p>
</dd>
</dl>
<p>)
use_parallel: bool = pydantic.Field(True, description=”Whether to use parallel DCN.”)</p>
<p>output_mlp: Optional[MlpConfig] = pydantic.Field(None, description=”Config for the output mlp.”)</p>
</dd>
<dt>class MaskBlockConfig(base_config.BaseConfig):</dt><dd><p>output_size: int
reduction_factor: Optional[pydantic.PositiveFloat] = pydantic.Field(</p>
<blockquote>
<div><p>None, one_of=”aggregation_size”</p>
</div></blockquote>
<p>)
aggregation_size: Optional[pydantic.PositiveInt] = pydantic.Field(</p>
<blockquote>
<div><p>None, description=”Specify the aggregation size directly.”, one_of=”aggregation_size”</p>
</div></blockquote>
<p>)
input_layer_norm: bool</p>
</dd>
<dt>class MaskNetConfig(base_config.BaseConfig):</dt><dd><p>mask_blocks: List[MaskBlockConfig]
mlp: Optional[MlpConfig] = pydantic.Field(None, description=”MLP Configuration for parallel”)
use_parallel: bool = pydantic.Field(False, description=”Whether to use parallel MaskNet.”)</p>
</dd>
<dt>class PositionDebiasConfig(base_config.BaseConfig):</dt><dd><p>“””
Configuration for Position Debias.
“””</p>
<p>max_position: int = pydantic.Field(256, description=”Bucket all later positions.”)
num_dims: pydantic.PositiveInt = pydantic.Field(</p>
<blockquote>
<div><p>64, description=”Number of dimensions in embedding.”</p>
</div></blockquote>
<p>)
drop_probability: float = pydantic.Field(0.5, description=”Probability of dropping position.”)</p>
<p># Currently it should be 51 based on dataset being tested at the time of writing this model
# However, no default provided here to make sure user of the model is aware of its importance.
position_feature_index: int = pydantic.Field(</p>
<blockquote>
<div><p>description=”The index of the position feature in the discrete features”</p>
</div></blockquote>
<p>)</p>
</dd>
<dt>class AffineMap(base_config.BaseConfig):</dt><dd><p>“””An affine map that scales the logits into the appropriate range.”””</p>
<p>scale: float = pydantic.Field(1.0)
bias: float = pydantic.Field(0.0)</p>
</dd>
<dt>class DLRMConfig(base_config.BaseConfig):</dt><dd><dl class="simple">
<dt>bottom_mlp: MlpConfig = pydantic.Field(</dt><dd><p>…,
description=”Bottom mlp, the output to be combined with sparse features and feed to interaction”,</p>
</dd>
</dl>
<p>)
top_mlp: MlpConfig = pydantic.Field(…, description=”Top mlp, generate the final output”)</p>
</dd>
<dt>class TaskModel(base_config.BaseConfig):</dt><dd><p>mlp_config: MlpConfig = pydantic.Field(None, one_of=”architecture”)
dcn_config: DcnConfig = pydantic.Field(None, one_of=”architecture”)
dlrm_config: DLRMConfig = pydantic.Field(None, one_of=”architecture”)
mask_net_config: MaskNetConfig = pydantic.Field(None, one_of=”architecture”)</p>
<dl class="simple">
<dt>affine_map: AffineMap = pydantic.Field(</dt><dd><p>None,
description=”Affine map applied to logits so we can represent a broader range of probabilities.”,</p>
</dd>
</dl>
<p>)
# DANGER DANGER: not implemented yet.
# loss_weight: float = pydantic.Field(1.0, description=”Weight for task in loss.”)
pos_weight: float = pydantic.Field(1.0, description=”Weight of positive in loss.”)</p>
</dd>
<dt>class MultiTaskType(str, enum.Enum):</dt><dd><p>SHARE_NONE = “share_none”  # Tasks are separate.
SHARE_ALL = “share_all”  # Tasks share same backbone.
SHARE_PARTIAL = “share_partial”  # Tasks share some backbone, but have their own portions.</p>
</dd>
<dt>class ModelConfig(base_config.BaseConfig):</dt><dd><p>“””Specify model architecture.”””</p>
<dl class="simple">
<dt>tasks: Dict[str, TaskModel] = pydantic.Field(</dt><dd><p>description=”Specification of architecture per task.”</p>
</dd>
</dl>
<p>)</p>
<p>large_embeddings: embedding_config.LargeEmbeddingsConfig = pydantic.Field(None)
small_embeddings: embedding_config.SmallEmbeddingsConfig = pydantic.Field(None)
# Not implemented yet.
# multi_task_loss_reduction_fn: str = “mean”</p>
<dl class="simple">
<dt>position_debias_config: PositionDebiasConfig = pydantic.Field(</dt><dd><p>default=None, description=”position debias model configuration”</p>
</dd>
</dl>
<p>)</p>
<p>featurization_config: FeaturizationConfig = pydantic.Field(None)</p>
<dl class="simple">
<dt>multi_task_type: MultiTaskType = pydantic.Field(</dt><dd><p>MultiTaskType.SHARE_NONE, description=”Multi task architecture”</p>
</dd>
</dl>
<p>)</p>
<p>backbone: TaskModel = pydantic.Field(None, description=”Type of architecture for the backbone.”)
stratifiers: List[embedding_config.StratifierConfig] = pydantic.Field(</p>
<blockquote>
<div><p>default=None, description=”Discrete features and values to stratify metrics by.”</p>
</div></blockquote>
<p>)</p>
<p>&#64;pydantic.root_validator()
def _validate_mtl(cls, values):</p>
<blockquote>
<div><dl class="simple">
<dt>if values.get(“multi_task_type”, None) is None:</dt><dd><p>return values</p>
</dd>
<dt>elif values[“multi_task_type”] in [MultiTaskType.SHARE_ALL, MultiTaskType.SHARE_PARTIAL]:</dt><dd><dl class="simple">
<dt>if values.get(“backbone”, None) is None:</dt><dd><p>raise ValueError(“Require <cite>backbone</cite> for SHARE_ALL and SHARE_PARTIAL.”)</p>
</dd>
</dl>
</dd>
<dt>elif values[“multi_task_type”] in [</dt><dd><p>MultiTaskType.SHARE_NONE,</p>
</dd>
<dt>]:</dt><dd><dl class="simple">
<dt>if values.get(“backbone”, None) is not None:</dt><dd><p>raise ValueError(“Can not have backbone if the share type is SHARE_NONE”)</p>
</dd>
</dl>
</dd>
</dl>
<p>return values</p>
</div></blockquote>
</dd>
</dl>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../index.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../../_sources/projects/home/recap/model/config.py.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>