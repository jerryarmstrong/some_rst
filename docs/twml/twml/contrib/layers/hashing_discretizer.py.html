<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p># pylint: disable=no-member, attribute-defined-outside-init, too-many-instance-attributes
“””
Implementing HashingDiscretizer Layer
“””</p>
<p>import libtwml
import tensorflow.compat.v1 as tf
import twml
from twml.constants import HashingDiscretizerOptions
from twml.layers.layer import Layer</p>
<dl>
<dt>class HashingDiscretizer(Layer):</dt><dd><p>“””A layer that discretizes continuous features, with hashed feature assignments</p>
<p>HashingDiscretizer converts sparse continuous features into sparse
binary features. Each binary output feature indicates the presence of a
value in a HashingDiscretizer bin.</p>
<p>Each calibrated HashingDiscretizer input feature is converted to n_bin+1 bins.</p>
<ul class="simple">
<li><p>n_bin bin boundaries for each feature (i.e. len(bin_vals[id])==n_bin) defines n_bin+1 bins</p></li>
<li><p>bin assignment = sum(bin_vals&lt;val)</p></li>
</ul>
<p>The difference between this layer and PercentileDiscretizer is that the
HashingDiscretizer always assigns the same output id in the
SparseTensor to the same input (feature id, bin) pair. This is useful if you
want to user transfer learning on pre-trained sparse to dense embedding
layers, but re-calibrate your discretizer on newer data.</p>
<p>If there are no calibrated features, then the discretizer will only apply
twml.util.limit_bits to the the feature keys (aka “feature_ids”). Essentially,
the discretizer will be a “no-operation”, other than obeying <cite>out_bits</cite></p>
<p>Typically, a HashingDiscretizer layer will be generated by calling the
to_layer() method of the HashingDiscretizerCalibrator
“””</p>
<dl>
<dt>def __init__(self, feature_ids, bin_vals, n_bin, out_bits,</dt><dd><blockquote>
<div><p>cost_per_unit=500, options=None, <a href="#id1"><span class="problematic" id="id2">**</span></a>kwargs):</p>
</div></blockquote>
<p>“””
Creates a non-initialized <cite>HashingDiscretizer</cite> object.</p>
<dl>
<dt>Parent class args:</dt><dd><p>see [tf.layers.Layer](<a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/layers/Layer">https://www.tensorflow.org/api_docs/python/tf/layers/Layer</a>)
for documentation of parent class arguments.</p>
</dd>
<dt>Required args:</dt><dd><p>feature_ids (1D int64 numpy array):
- list of feature IDs that have been calibrated and have corresponding</p>
<blockquote>
<div><p>bin boundary values in the bin_vals array</p>
</div></blockquote>
<ul class="simple">
<li><p>bin values for feature feature_ids[i] live at bin_vals[i*n_bin:(i+1)*n_bin]</p></li>
</ul>
<p>bin_vals (1D float numpy array):
- These are the bin boundary values for each calibrated feature
- len(bin_vals) = n_bin*len(feature_ids)
n_bin (int):
- number of HashingDiscretizer bins is actually n_bin + 1
- <strong>*Note*</strong> that if a value N is passed for the value of n_bin to</p>
<blockquote>
<div><p>HashingDiscretizerCalibrator, then HashingDiscretizerCalibrator
will generate N+1 bin boundaries for each feature, and hence there
will actually be N+2 potential bins for each feature</p>
</div></blockquote>
<dl class="simple">
<dt>out_bits (int):</dt><dd><p>Determines the maximum value for output feature IDs.
The dense_shape of the SparseTensor returned by lookup(x)
will be [x.shape[0], 1 &lt;&lt; output_bits].</p>
</dd>
</dl>
</dd>
<dt>Optional args:</dt><dd><p>cost_per_unit (int):
- heuristic for intra op multithreading. approximate nanoseconds per input value.
options (int or None for default):
- Selects behavior of the op. Default is lower_bound and integer_multiplicative_hashing.
- Use values in twml.constants.HashingDiscretizerOptions to select options as follows</p>
<blockquote>
<div><p>choose exactly one of HashingDiscretizerOptions.{SEARCH_LOWER_BOUND, SEARCH_LINEAR, SEARCH_UPPER_BOUND}
choose exactly one of HashingDiscretizerOptions.{HASH_32BIT, HASH_64BIT}
Bitwise OR these together to construct the options input.
For example, <cite>options=(HashingDiscretizerOptions.SEARCH_UPPER_BOUND | HashingDiscretizerOptions.HASH_64BIT)</cite></p>
</div></blockquote>
</dd>
</dl>
<p>“””
super(HashingDiscretizer, self).__init__(<a href="#id3"><span class="problematic" id="id4">**</span></a>kwargs)</p>
<p>self._feature_ids = feature_ids
self._bin_vals = bin_vals
self._n_bin = n_bin
self._out_bits = out_bits
self.cost_per_unit = cost_per_unit
if options is None:</p>
<blockquote>
<div><p>options = HashingDiscretizerOptions.SEARCH_LOWER_BOUND | HashingDiscretizerOptions.HASH_32BIT</p>
</div></blockquote>
<p>self._options = options</p>
<dl class="simple">
<dt>if not self.built:</dt><dd><p>self.build(input_shape=None)</p>
</dd>
</dl>
</dd>
<dt>def build(self, input_shape):  # pylint: disable=unused-argument</dt><dd><p>“””
Creates the variables of the layer
“””
# make sure this is last
self.built = True</p>
</dd>
<dt>def call(self, inputs, <a href="#id5"><span class="problematic" id="id6">**</span></a>kwargs):</dt><dd><p>“””
Implements HashingDiscretizer inference on a twml.SparseTensor.
Alternatively, accepts a tf.SparseTensor that can be converted
to twml.SparseTensor.</p>
<p>Performs discretization of input values.
i.e. bucket_val = bucket(val | feature_id)</p>
<p>This bucket mapping depends on the calibration (i.e. the bin boundaries).
However, (feature_id, bucket_val) pairs are mapped to new_feature_id in
a way that is independent of the calibration procedure</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>inputs: A 2D SparseTensor that is input to HashingDiscretizer for</dt><dd><p>discretization. It has a dense_shape of [batch_size, input_size]</p>
</dd>
</dl>
<p>name: A name for the operation (optional).</p>
</dd>
<dt>Returns:</dt><dd><p>A tf.SparseTensor, created from twml.SparseTensor.to_tf()
Its dense_shape is [shape_input.dense_shape[0], 1 &lt;&lt; output_bits].</p>
</dd>
</dl>
<p>“””
if isinstance(inputs, tf.SparseTensor):</p>
<blockquote>
<div><p>inputs = twml.SparseTensor.from_tf(inputs)</p>
</div></blockquote>
<p>assert(isinstance(inputs, twml.SparseTensor))</p>
<p># sparse column indices
ids = inputs.ids
# sparse row indices
keys = inputs.indices
# sparse values
vals = inputs.values</p>
<dl>
<dt>if len(self._feature_ids) &gt; 0:</dt><dd><p># pass all inputs to the c++ op
# the op determines whether to discretize (when a feature is calibrated),
#   or whether to simply limit bits and pass through (when not calibrated)
# NOTE - Hashing is done in C++
discretizer_keys, discretizer_vals = libtwml.ops.hashing_discretizer(</p>
<blockquote>
<div><p>input_ids=keys,  # Input
input_vals=vals,  # Input
bin_vals=self._bin_vals,  # Input
feature_ids=tf.make_tensor_proto(self._feature_ids),  # Attr
n_bin=self._n_bin,  # Attr
output_bits=self._out_bits,  # Attr
cost_per_unit=self.cost_per_unit,  # Attr
options=self._options,  # Attr</p>
</div></blockquote>
<p>)</p>
</dd>
<dt>else:</dt><dd><p>discretizer_keys = twml.util.limit_bits(keys, self._out_bits)
discretizer_vals = vals</p>
</dd>
</dl>
<p>batch_size = tf.to_int64(inputs.dense_shape[0])
output_size = tf.convert_to_tensor(1 &lt;&lt; self._out_bits, tf.int64)
output_shape = [batch_size, output_size]</p>
<p>return twml.SparseTensor(ids, discretizer_keys, discretizer_vals, output_shape).to_tf()</p>
</dd>
</dl>
</dd>
</dl>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../index.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../../_sources/twml/twml/contrib/layers/hashing_discretizer.py.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>