<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../_static/doctools.js?v=888ff710"></script>
    <script src="../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>from absl import app, flags
import json
from typing import Optional
import os
import sys</p>
<p>import torch</p>
<p># isort: on
from tml.common.device import setup_and_get_device
from tml.common.utils import setup_configuration
import tml.core.custom_training_loop as ctl
import tml.machines.environment as env
from tml.projects.twhin.models.models import apply_optimizers, TwhinModel, TwhinModelAndLoss
from tml.model import maybe_shard_model
from tml.projects.twhin.metrics import create_metrics
from tml.projects.twhin.config import TwhinConfig
from tml.projects.twhin.data.data import create_dataset
from tml.projects.twhin.optimizer import build_optimizer</p>
<p>from tml.ml_logging.torch_logging import logging</p>
<p>import torch.distributed as dist
from torch.nn import functional as F
from torchrec.optim.apply_optimizer_in_backward import apply_optimizer_in_backward
from torchrec.distributed.model_parallel import get_module</p>
<p>FLAGS = flags.FLAGS</p>
<p>flags.DEFINE_bool(“overwrite_save_dir”, False, “Whether to clear preexisting save directories.”)
flags.DEFINE_string(“save_dir”, None, “If provided, overwrites the save directory.”)
flags.DEFINE_string(“config_yaml_path”, None, “Path to hyperparameters for model.”)
flags.DEFINE_string(“task”, None, “Task to run if this is local. Overrides TF_CONFIG etc.”)</p>
<dl>
<dt>def run(</dt><dd><p>all_config: TwhinConfig,
save_dir: Optional[str] = None,</p>
</dd>
<dt>):</dt><dd><p>train_dataset = create_dataset(all_config.train_data, all_config.model)</p>
<dl>
<dt>if env.is_reader():</dt><dd><p>train_dataset.serve()</p>
</dd>
<dt>if env.is_chief():</dt><dd><p>device = setup_and_get_device(tf_ok=False)
logging.info(f”device: {device}”)
logging.info(f”WORLD_SIZE: {dist.get_world_size()}”)</p>
<p># validation_dataset = create_dataset(all_config.validation_data, all_config.model)</p>
<p>global_batch_size = all_config.train_data.per_replica_batch_size * dist.get_world_size()</p>
<p>metrics = create_metrics(device)</p>
<p>model = TwhinModel(all_config.model, all_config.train_data)
apply_optimizers(model, all_config.model)
model = maybe_shard_model(model, device=device)
optimizer, scheduler = build_optimizer(model=model, config=all_config.model)</p>
<p>loss_fn = F.binary_cross_entropy_with_logits
model_and_loss = TwhinModelAndLoss(</p>
<blockquote>
<div><p>model, loss_fn, data_config=all_config.train_data, device=device</p>
</div></blockquote>
<p>)</p>
<dl class="simple">
<dt>ctl.train(</dt><dd><p>model=model_and_loss,
optimizer=optimizer,
device=device,
save_dir=save_dir,
logging_interval=all_config.training.train_log_every_n,
train_steps=all_config.training.num_train_steps,
checkpoint_frequency=all_config.training.checkpoint_every_n,
dataset=train_dataset.dataloader(remote=False),
worker_batch_size=global_batch_size,
num_workers=0,
scheduler=scheduler,
initial_checkpoint_dir=all_config.training.initial_checkpoint_dir,
gradient_accumulation=all_config.training.gradient_accumulation,</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
</dd>
<dt>def main(argv):</dt><dd><p>logging.info(“Starting”)</p>
<p>logging.info(f”parsing config from {FLAGS.config_yaml_path}…”)
all_config = setup_configuration(  # type: ignore[var-annotated]</p>
<blockquote>
<div><p>TwhinConfig,
yaml_path=FLAGS.config_yaml_path,</p>
</div></blockquote>
<p>)</p>
<dl class="simple">
<dt>run(</dt><dd><p>all_config,
save_dir=FLAGS.save_dir,</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>if __name__ == “__main__”:</dt><dd><p>app.run(main)</p>
</dd>
</dl>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../_sources/projects/twhin/run.py.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>