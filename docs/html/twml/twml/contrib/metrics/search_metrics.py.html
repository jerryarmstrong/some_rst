<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>“””
Module containing extra tensorflow metrics used at Twitter.
This module conforms to conventions used by tf.metrics.*.
In particular, each metric constructs two subgraphs: value_op and update_op:</p>
<blockquote>
<div><ul class="simple">
<li><p>The value op is used to fetch the current metric value.</p></li>
<li><p>The update_op is used to accumulate into the metric.</p></li>
</ul>
</div></blockquote>
<p>Note: similar to tf.metrics.*, metrics in here do not support multi-label learning.
We will have to write wrapper classes to create one metric per label.</p>
<p>Note: similar to tf.metrics.*, batches added into a metric via its update_op are cumulative!</p>
<p>“””</p>
<p>from collections import OrderedDict
from functools import partial</p>
<p>import tensorflow.compat.v1 as tf
from tensorflow.python.eager import context
from tensorflow.python.framework import dtypes, ops
from tensorflow.python.ops import array_ops, state_ops
import twml
from twml.contrib.utils import math_fns</p>
<dl>
<dt>def ndcg(labels, predictions,</dt><dd><blockquote>
<div><p>metrics_collections=None,
updates_collections=None,
name=None,
top_k_int=1):</p>
</div></blockquote>
<p># pylint: disable=unused-argument
“””
Compute full normalized discounted cumulative gain (ndcg) based on predictions
ndcg = dcg_k/idcg_k, k is a cut off ranking postion
There are a few variants of ndcg
The dcg (discounted cumulative gain) formula used in
twml.contrib.metrics.ndcg is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>\\<span class="n">sum_</span><span class="p">{</span><span class="n">i</span><span class="o">=</span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="n">k</span> \<span class="n">frac</span><span class="p">{</span><span class="mi">2</span><span class="o">^</span><span class="p">{</span><span class="n">relevance</span>\\<span class="n">_score</span><span class="p">}</span> <span class="o">-</span><span class="mi">1</span><span class="p">}{</span>\\<span class="n">log_</span><span class="p">{</span><span class="mi">2</span><span class="p">}(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)}</span>
</pre></div>
</div>
<p>k is the length of items to be ranked in a batch/query
Notice that whether k will be replaced with a fixed value requires discussions
The scores in predictions are transformed to order and relevance scores to calculate ndcg
A relevance score means how relevant a DataRecord is to a particular query</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>labels: the ground truth value.
predictions: the predicted values, whose shape must match labels. Ignored for CTR computation.
metrics_collections: optional list of collections to add this metric into.
updates_collections: optional list of collections to add the associated update_op into.
name: an optional variable_scope name.</p>
</dd>
<dt>Returns:</dt><dd><p>ndcg: A <cite>Tensor</cite> representing the ndcg score.
update_op: A update operation used to accumulate data into this metric.</p>
</dd>
</dl>
<p>“””
with tf.variable_scope(name, ‘ndcg’, (labels, predictions)):</p>
<blockquote>
<div><p>label_scores = tf.to_float(labels, name=’label_to_float’)
predicted_scores = tf.to_float(predictions, name=’predictions_to_float’)</p>
<dl class="simple">
<dt>if context.executing_eagerly():</dt><dd><dl class="simple">
<dt>raise RuntimeError(‘ndcg is not supported when eager execution ‘</dt><dd><p>‘is enabled.’)</p>
</dd>
</dl>
</dd>
</dl>
<p>total_ndcg = _metric_variable([], dtypes.float32, name=’total_ndcg’)
count_query = _metric_variable([], dtypes.float32, name=’query_count’)</p>
<p># actual ndcg cutoff position top_k_int
max_prediction_size = array_ops.size(predicted_scores)
top_k_int = tf.minimum(max_prediction_size, top_k_int)
# the ndcg score of the batch
ndcg = math_fns.cal_ndcg(label_scores,</p>
<blockquote>
<div><p>predicted_scores, top_k_int=top_k_int)</p>
</div></blockquote>
<p># add ndcg of the current batch to total_ndcg
update_total_op = state_ops.assign_add(total_ndcg, ndcg)
with ops.control_dependencies([ndcg]):</p>
<blockquote>
<div><p># count_query stores the number of queries
# count_query increases by 1 for each batch/query
update_count_op = state_ops.assign_add(count_query, 1)</p>
</div></blockquote>
<p>mean_ndcg = math_fns.safe_div(total_ndcg, count_query, ‘mean_ndcg’)
update_op = math_fns.safe_div(update_total_op, update_count_op, ‘update_mean_ndcg_op’)</p>
<dl class="simple">
<dt>if metrics_collections:</dt><dd><p>ops.add_to_collections(metrics_collections, mean_ndcg)</p>
</dd>
<dt>if updates_collections:</dt><dd><p>ops.add_to_collections(updates_collections, update_op)</p>
</dd>
</dl>
<p>return mean_ndcg, update_op</p>
</div></blockquote>
</dd>
</dl>
<p># Copied from metrics_impl.py with minor modifications.
# <a class="reference external" href="https://github.com/tensorflow/tensorflow/blob/v1.5.0/tensorflow/python/ops/metrics_impl.py#L39">https://github.com/tensorflow/tensorflow/blob/v1.5.0/tensorflow/python/ops/metrics_impl.py#L39</a>
def _metric_variable(shape, dtype, validate_shape=True, name=None):</p>
<blockquote>
<div><p>“””Create variable in <cite>GraphKeys.(LOCAL|METRIC_VARIABLES</cite>) collections.”””</p>
<dl class="simple">
<dt>return tf.Variable(</dt><dd><p>lambda: tf.zeros(shape, dtype),
trainable=False,
collections=[tf.GraphKeys.LOCAL_VARIABLES, tf.GraphKeys.METRIC_VARIABLES],
validate_shape=validate_shape,
name=name)</p>
</dd>
</dl>
</div></blockquote>
<p># binary metric_name: (metric, requires thresholded output)
SUPPORTED_BINARY_CLASS_METRICS = {</p>
<blockquote>
<div><p># TWML binary metrics
‘rce’: (twml.metrics.rce, False),
‘nrce’: (partial(twml.metrics.rce, normalize=True), False),
# CTR measures positive sample ratio. This terminology is inherited from Ads.
‘ctr’: (twml.metrics.ctr, False),
# predicted CTR measures predicted positive ratio.
‘predicted_ctr’: (twml.metrics.predicted_ctr, False),
# thresholded metrics
‘accuracy’: (tf.metrics.accuracy, True),
‘precision’: (tf.metrics.precision, True),
‘recall’: (tf.metrics.recall, True),
# tensorflow metrics
‘roc_auc’: (partial(tf.metrics.auc, curve=’ROC’), False),
‘pr_auc’: (partial(tf.metrics.auc, curve=’PR’), False),</p>
</div></blockquote>
<p>}</p>
<p># search metric_name: metric
SUPPORTED_SEARCH_METRICS = {</p>
<blockquote>
<div><p># TWML search metrics
# ndcg needs the raw prediction scores to sort
‘ndcg’: ndcg,</p>
</div></blockquote>
<p>}</p>
<dl>
<dt>def get_search_metric_fn(binary_metrics=None, search_metrics=None,</dt><dd><p>ndcg_top_ks=[1, 3, 5, 10], use_binary_metrics=False):
“””
Returns a function having signature:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_eval_metric_ops</span><span class="p">(</span><span class="n">graph_output</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
  <span class="o">...</span>
  <span class="k">return</span> <span class="n">eval_metric_ops</span>
</pre></div>
</div>
<p>where the returned eval_metric_ops is a dict of common evaluation metric
Ops for ranking. See <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec">tf.estimator.EstimatorSpec</a>
for a description of eval_metric_ops. The graph_output is a the result
dict returned by build_graph. Labels and weights are tf.Tensors.</p>
<dl>
<dt>The following graph_output keys are recognized:</dt><dd><dl class="simple">
<dt>output:</dt><dd><p>the raw predictions. Required.</p>
</dd>
<dt>threshold:</dt><dd><p>Only used in SUPPORTED_BINARY_CLASS_METRICS
If the lables are 0s and 1s
A value between 0 and 1 used to threshold the output into a hard_output.
Defaults to 0.5 when threshold and hard_output are missing.
Either threshold or hard_output can be provided, but not both.</p>
</dd>
<dt>hard_output:</dt><dd><p>Only used in SUPPORTED_BINARY_CLASS_METRICS
A thresholded output. Either threshold or hard_output can be provided, but not both.</p>
</dd>
</dl>
</dd>
<dt>Arguments:</dt><dd><p>only used in pointwise learning-to-rank</p>
<dl>
<dt>binary_metrics (list of String):</dt><dd><p>a list of metrics of interest. E.g. [‘ctr’, ‘accuracy’, ‘rce’]
These metrics are evaluated and reported to tensorboard <em>during the eval phases only</em>.
Supported metrics:</p>
<blockquote>
<div><ul class="simple">
<li><p>ctr (same as positive sample ratio.)</p></li>
<li><p>rce (cross entropy loss compared to the baseline model of always predicting ctr)</p></li>
<li><p>nrce (normalized rce, do not use this one if you do not understand what it is)</p></li>
<li><p>pr_auc</p></li>
<li><p>roc_auc</p></li>
<li><p>accuracy (percentage of predictions that are correct)</p></li>
<li><p>precision (true positives) / (true positives + false positives)</p></li>
<li><p>recall (true positives) / (true positives + false negatives)</p></li>
</ul>
</div></blockquote>
<p>NOTE: accuracy / precision / recall apply to binary classification problems only.
I.e. a prediction is only considered correct if it matches the label. E.g. if the label
is 1.0, and the prediction is 0.99, it does not get credit.  If you want to use
precision / recall / accuracy metrics with soft predictions, you’ll need to threshold
your predictions into hard 0/1 labels.</p>
<p>When binary_metrics is None (the default), it defaults to all supported metrics</p>
</dd>
<dt>search_metrics (list of String):</dt><dd><p>a list of metrics of interest. E.g. [‘ndcg’]
These metrics are evaluated and reported to tensorboard <em>during the eval phases only</em>.
Supported metrics:</p>
<blockquote>
<div><ul class="simple">
<li><p>ndcg</p></li>
</ul>
</div></blockquote>
<p>NOTE: ndcg works for ranking-relatd problems.
A batch contains all DataRecords that belong to the same query
If pair_in_batch_mode used in scalding – a batch contains a pair of DataRecords
that belong to the same query and have different labels – ndcg does not apply in here.</p>
<p>When search_metrics is None (the default), it defaults to all supported search metrics
currently only ‘ndcg’</p>
</dd>
<dt>ndcg_top_ks (list of integers):</dt><dd><p>The cut-off ranking postions for a query
When ndcg_top_ks is None or empty (the default), it defaults to [1, 3, 5, 10]</p>
</dd>
<dt>use_binary_metrics:</dt><dd><p>False (default)
Only set it to true in pointwise learning-to-rank</p>
</dd>
</dl>
</dd>
</dl>
<p>“””
# pylint: disable=dict-keys-not-iterating</p>
<dl>
<dt>if ndcg_top_ks is None or not ndcg_top_ks:</dt><dd><p>ndcg_top_ks = [1, 3, 5, 10]</p>
</dd>
<dt>if search_metrics is None:</dt><dd><p>search_metrics = list(SUPPORTED_SEARCH_METRICS.keys())</p>
</dd>
<dt>if binary_metrics is None and use_binary_metrics:</dt><dd><p># Added SUPPORTED_BINARY_CLASS_METRICS in twml.metics as well
# they are only used in pointwise learing-to-rank
binary_metrics = list(SUPPORTED_BINARY_CLASS_METRICS.keys())</p>
</dd>
<dt>def get_eval_metric_ops(graph_output, labels, weights):</dt><dd><p>“””
graph_output:</p>
<blockquote>
<div><p>dict that is returned by build_graph given input features.</p>
</div></blockquote>
<dl class="simple">
<dt>labels:</dt><dd><p>target labels associated to batch.</p>
</dd>
<dt>weights:</dt><dd><p>weights of the samples..</p>
</dd>
</dl>
<p>“””</p>
<p>eval_metric_ops = OrderedDict()</p>
<p>preds = graph_output[‘output’]</p>
<p>threshold = graph_output[‘threshold’] if ‘threshold’ in graph_output else 0.5</p>
<p>hard_preds = graph_output.get(‘hard_output’)
# hard_preds is a tensor
# check hard_preds is None and then check if it is empty
if hard_preds is None or tf.equal(tf.size(hard_preds), 0):</p>
<blockquote>
<div><p>hard_preds = tf.greater_equal(preds, threshold)</p>
</div></blockquote>
<p># add search metrics to eval_metric_ops dict
for metric_name in search_metrics:</p>
<blockquote>
<div><p>metric_name = metric_name.lower()  # metric name are case insensitive.</p>
<dl class="simple">
<dt>if metric_name in eval_metric_ops:</dt><dd><p># avoid adding duplicate metrics.
continue</p>
</dd>
</dl>
<p>search_metric_factory = SUPPORTED_SEARCH_METRICS.get(metric_name)
if search_metric_factory:</p>
<blockquote>
<div><dl>
<dt>if metric_name == ‘ndcg’:</dt><dd><dl>
<dt>for top_k in ndcg_top_ks:</dt><dd><p># metric name will show as ndcg_1, ndcg_10, …
metric_name_ndcg_top_k = metric_name + ‘_’ + str(top_k)
top_k_int = tf.constant(top_k, dtype=tf.int32)
# Note: having weights in ndcg does not make much sense
# Because ndcg already has position weights/discounts
# Thus weights are not applied in ndcg metric
value_op, update_op = search_metric_factory(</p>
<blockquote>
<div><p>labels=labels,
predictions=preds,
name=metric_name_ndcg_top_k,
top_k_int=top_k_int)</p>
</div></blockquote>
<p>eval_metric_ops[metric_name_ndcg_top_k] = (value_op, update_op)</p>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>else:</dt><dd><p>raise ValueError(‘Cannot find the search metric named ‘ + metric_name)</p>
</dd>
</dl>
</div></blockquote>
<dl>
<dt>if use_binary_metrics:</dt><dd><p># add binary metrics to eval_metric_ops dict
for metric_name in binary_metrics:</p>
<blockquote>
<div><dl class="simple">
<dt>if metric_name in eval_metric_ops:</dt><dd><p># avoid adding duplicate metrics.
continue</p>
</dd>
</dl>
<p>metric_name = metric_name.lower()  # metric name are case insensitive.
binary_metric_factory, requires_threshold = SUPPORTED_BINARY_CLASS_METRICS.get(metric_name)
if binary_metric_factory:</p>
<blockquote>
<div><dl class="simple">
<dt>value_op, update_op = binary_metric_factory(</dt><dd><p>labels=labels,
predictions=(hard_preds if requires_threshold else preds),
weights=weights,
name=metric_name)</p>
</dd>
</dl>
<p>eval_metric_ops[metric_name] = (value_op, update_op)</p>
</div></blockquote>
<dl class="simple">
<dt>else:</dt><dd><p>raise ValueError(‘Cannot find the binary metric named ‘ + metric_name)</p>
</dd>
</dl>
</div></blockquote>
</dd>
</dl>
<p>return eval_metric_ops</p>
</dd>
</dl>
<p>return get_eval_metric_ops</p>
</dd>
</dl>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../index.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../../_sources/twml/twml/contrib/metrics/search_metrics.py.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>