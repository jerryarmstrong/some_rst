<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../../../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../../../../../" id="documentation_options" src="../../../../../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>package com.twitter.search.earlybird.index;</p>
<p>import java.io.Closeable;
import java.io.File;
import java.io.IOException;
import java.time.Instant;
import java.time.ZoneOffset;
import java.time.ZonedDateTime;
import java.time.format.DateTimeFormatter;
import java.util.List;
import java.util.Map;
import java.util.Objects;
import java.util.concurrent.atomic.AtomicReference;
import javax.annotation.Nullable;</p>
<p>import com.google.common.annotations.VisibleForTesting;
import com.google.common.base.Preconditions;
import com.google.common.collect.HashBasedTable;
import com.google.common.collect.Table;
import com.google.common.collect.Lists;
import com.google.common.collect.Maps;</p>
<p>import org.apache.commons.io.FileUtils;
import org.apache.lucene.document.Document;
import org.apache.lucene.index.DirectoryReader;
import org.apache.lucene.index.IndexWriterConfig;
import org.apache.lucene.index.IndexableField;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.FSDirectory;
import org.apache.lucene.store.IOContext;
import org.apache.lucene.store.IndexOutput;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;</p>
<p>import com.twitter.common.collections.Pair;
import com.twitter.common.util.Clock;
import com.twitter.search.common.metrics.SearchCounter;
import com.twitter.search.common.schema.base.FeatureConfiguration;
import com.twitter.search.common.schema.base.ImmutableSchemaInterface;
import com.twitter.search.common.schema.base.ThriftDocumentUtil;
import com.twitter.search.common.schema.earlybird.EarlybirdCluster;
import com.twitter.search.common.schema.earlybird.EarlybirdEncodedFeatures;
import com.twitter.search.common.schema.earlybird.EarlybirdEncodedFeaturesUtil;
import com.twitter.search.common.schema.earlybird.EarlybirdFieldConstants;
import com.twitter.search.common.schema.earlybird.EarlybirdFieldConstants.EarlybirdFieldConstant;
import com.twitter.search.common.schema.thriftjava.ThriftDocument;
import com.twitter.search.common.schema.thriftjava.ThriftField;
import com.twitter.search.common.schema.thriftjava.ThriftIndexingEvent;
import com.twitter.search.common.schema.thriftjava.ThriftIndexingEventType;
import com.twitter.search.common.util.io.flushable.DataDeserializer;
import com.twitter.search.common.util.io.flushable.DataSerializer;
import com.twitter.search.common.util.io.flushable.FlushInfo;
import com.twitter.search.core.earlybird.index.DocIDToTweetIDMapper;
import com.twitter.search.core.earlybird.index.EarlybirdIndexSegmentAtomicReader;
import com.twitter.search.core.earlybird.index.EarlybirdIndexSegmentData;
import com.twitter.search.core.earlybird.index.EarlybirdIndexSegmentWriter;
import com.twitter.search.core.earlybird.index.column.ColumnStrideFieldIndex;
import com.twitter.search.core.earlybird.index.column.DocValuesUpdate;
import com.twitter.search.core.earlybird.index.extensions.EarlybirdIndexExtensionsFactory;
import com.twitter.search.earlybird.EarlybirdIndexConfig;
import com.twitter.search.earlybird.common.userupdates.UserTable;
import com.twitter.search.earlybird.document.TweetDocument;
import com.twitter.search.earlybird.exception.FlushVersionMismatchException;
import com.twitter.search.earlybird.partition.SearchIndexingMetricSet;
import com.twitter.search.earlybird.partition.SegmentIndexStats;
import com.twitter.search.earlybird.stats.EarlybirdSearcherStats;
import com.twitter.snowflake.id.SnowflakeId;</p>
<dl>
<dt>public class EarlybirdSegment {</dt><dd><p>private static final Logger LOG = LoggerFactory.getLogger(EarlybirdSegment.class);
private static final Logger UPDATES_ERRORS_LOG =</p>
<blockquote>
<div><p>LoggerFactory.getLogger(EarlybirdSegment.class.getName() + “.UpdatesErrors”);</p>
</div></blockquote>
<p>private static final String SUCCESS_FILE = “EARLYBIRD_SUCCESS”;
private static final DateTimeFormatter HOURLY_COUNT_DATE_TIME_FORMATTER =</p>
<blockquote>
<div><p>DateTimeFormatter.ofPattern(“yyyy_MM_dd_HH”);</p>
</div></blockquote>
<p>&#64;VisibleForTesting
public static final String NUM_TWEETS_CREATED_AT_PATTERN = “num_tweets_%s_%s_created_at_%s”;</p>
<dl class="simple">
<dt>private static final String INVALID_FEATURE_UPDATES_DROPPED_PREFIX =</dt><dd><p>“<a href="#id51"><span class="problematic" id="id52">invalid_index_feature_update_dropped_</span></a>”;</p>
</dd>
</dl>
<p>// The number of tweets not indexed because they have been previously indexed.
private static final SearchCounter DUPLICATE_TWEET_SKIPPED_COUNTER =</p>
<blockquote>
<div><p>SearchCounter.export(“duplicate_tweet_skipped”);</p>
</div></blockquote>
<p>// The number of tweets that came out of order.
private static final SearchCounter OUT_OF_ORDER_TWEET_COUNTER =</p>
<blockquote>
<div><p>SearchCounter.export(“out_of_order_tweet”);</p>
</div></blockquote>
<p>// The number partial updates dropped because the field could not be found in the schema.
// This counter is incremented once per field rather than once per partial update event.
// Note: caller may retry update, this counter will be incremented multiple times for same update.
private static final SearchCounter INVALID_FIELDS_IN_PARTIAL_UPDATES =</p>
<blockquote>
<div><p>SearchCounter.export(“invalid_fields_in_partial_updates”);</p>
</div></blockquote>
<p>// The number partial updates dropped because the tweet id could not be found in the segment.
// Note: caller may retry update, this counter will be incremented multiple times for same update.
private static final SearchCounter PARTIAL_UPDATE_FOR_TWEET_NOT_IN_INDEX =</p>
<blockquote>
<div><p>SearchCounter.export(“partial_update_for_tweet_id_not_in_index”);</p>
</div></blockquote>
<p>// The number of partial updates that were applied only partially, because the update could not
// be applied for at least one of the fields.
private static final SearchCounter PARTIAL_UPDATE_PARTIAL_FAILURE =</p>
<blockquote>
<div><p>SearchCounter.export(“partial_update_partial_failure”);</p>
</div></blockquote>
<p>// Both the indexing chain and the index writer are lazily initialized when adding docs for
// the first time.
private final AtomicReference&lt;EarlybirdIndexSegmentWriter&gt; segmentWriterReference =</p>
<blockquote>
<div><p>new AtomicReference&lt;&gt;();</p>
</div></blockquote>
<p>// Stats from the PartitionIndexer / SimpleSegmentIndexer.
private final SegmentIndexStats indexStats;
private final String segmentName;
private final int maxSegmentSize;
private final long timeSliceID;
private final AtomicReference&lt;EarlybirdIndexSegmentAtomicReader&gt; luceneIndexReader =</p>
<blockquote>
<div><p>new AtomicReference&lt;&gt;();</p>
</div></blockquote>
<p>private final Directory luceneDir;
private final File luceneDirFile;
private final EarlybirdIndexConfig indexConfig;
private final List&lt;Closeable&gt; closableResources = Lists.newArrayList();
private long lastInOrderTweetId = 0;</p>
<p>private final EarlybirdIndexExtensionsFactory extensionsFactory;
private final SearchIndexingMetricSet searchIndexingMetricSet;
private final EarlybirdSearcherStats searcherStats;</p>
<p>private final Map&lt;String, SearchCounter&gt; indexedTweetsCounters = Maps.newHashMap();
private final PerFieldCounters perFieldCounters;
private final Clock clock;</p>
<p>&#64;VisibleForTesting
public volatile boolean appendedLuceneIndex = false;</p>
<dl>
<dt>public EarlybirdSegment(</dt><dd><blockquote>
<div><p>String segmentName,
long timeSliceID,
int maxSegmentSize,
Directory luceneDir,
EarlybirdIndexConfig indexConfig,
SearchIndexingMetricSet searchIndexingMetricSet,
EarlybirdSearcherStats searcherStats,
Clock clock) {</p>
</div></blockquote>
<p>this.segmentName = segmentName;
this.maxSegmentSize = maxSegmentSize;
this.timeSliceID = timeSliceID;
this.luceneDir = luceneDir;
this.indexConfig = indexConfig;
this.indexStats = new SegmentIndexStats();
this.perFieldCounters = new PerFieldCounters();
this.extensionsFactory = new TweetSearchIndexExtensionsFactory();</p>
<dl class="simple">
<dt>if (luceneDir != null &amp;&amp; luceneDir instanceof FSDirectory) {</dt><dd><p>// getDirectory() throws if the luceneDir is already closed.
// To delete a directory, we need to close it first.
// Obtain a reference to the File now, so we can delete it later.
// See SEARCH-5281
this.luceneDirFile = ((FSDirectory) luceneDir).getDirectory().toFile();</p>
</dd>
<dt>} else {</dt><dd><p>this.luceneDirFile = null;</p>
</dd>
</dl>
<p>}
this.searchIndexingMetricSet = Preconditions.checkNotNull(searchIndexingMetricSet);
this.searcherStats = searcherStats;
this.clock = clock;</p>
</dd>
</dl>
<p>}</p>
<p>&#64;VisibleForTesting
public Directory getLuceneDirectory() {</p>
<blockquote>
<div><p>return luceneDir;</p>
</div></blockquote>
<p>}</p>
<dl class="simple">
<dt>public SegmentIndexStats getIndexStats() {</dt><dd><p>return indexStats;</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Returns the smallest tweet ID in this segment. If the segment is not loaded yet, or is empty,</p></li>
<li><p>DocIDToTweetIDMapper.ID_NOT_FOUND is returned (-1).</p></li>
<li></li>
<li><p>&#64;return The smallest tweet ID in this segment.</p></li>
</ul>
<p><a href="#id1"><span class="problematic" id="id2">*</span></a>/</p>
</dd>
<dt>public long getLowestTweetId() {</dt><dd><p>EarlybirdIndexSegmentWriter segmentWriter = segmentWriterReference.get();
if (segmentWriter == null) {</p>
<blockquote>
<div><p>return DocIDToTweetIDMapper.ID_NOT_FOUND;</p>
</div></blockquote>
<p>}</p>
<p>DocIDToTweetIDMapper mapper = segmentWriter.getSegmentData().getDocIDToTweetIDMapper();
int highestDocID = mapper.getPreviousDocID(Integer.MAX_VALUE);
return mapper.getTweetID(highestDocID);</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Returns the cardinality (size) sum of the cardinality of each</p></li>
<li><p>query cache set.</p></li>
</ul>
<p><a href="#id3"><span class="problematic" id="id4">*</span></a>/</p>
</dd>
<dt>public long getQueryCachesCardinality() {</dt><dd><p>EarlybirdIndexSegmentWriter writer = getIndexSegmentWriter();
if (writer == null) {</p>
<blockquote>
<div><p>// The segment is not loaded yet, or the query caches for this segment are not built yet.
return -1;</p>
</div></blockquote>
<p>}</p>
<p>EarlybirdIndexSegmentData earlybirdIndexSegmentData = writer.getSegmentData();
return earlybirdIndexSegmentData.getQueryCachesCardinality();</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>public List&lt;Pair&lt;String, Long&gt;&gt; getQueryCachesData() {</dt><dd><p>return getIndexSegmentWriter().getSegmentData().getPerQueryCacheCardinality();</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Returns the highest tweet ID in this segment. If the segment is not loaded yet, or is empty,</p></li>
<li><p>DocIDToTweetIDMapper.ID_NOT_FOUND is returned (-1).</p></li>
<li></li>
<li><p>&#64;return The highest tweet ID in this segment.</p></li>
</ul>
<p><a href="#id5"><span class="problematic" id="id6">*</span></a>/</p>
</dd>
<dt>public long getHighestTweetId() {</dt><dd><p>EarlybirdIndexSegmentWriter segmentWriter = segmentWriterReference.get();
if (segmentWriter == null) {</p>
<blockquote>
<div><p>return DocIDToTweetIDMapper.ID_NOT_FOUND;</p>
</div></blockquote>
<p>}</p>
<p>DocIDToTweetIDMapper mapper = segmentWriter.getSegmentData().getDocIDToTweetIDMapper();
int lowestDocID = mapper.getNextDocID(-1);
return mapper.getTweetID(lowestDocID);</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Optimizes the underlying segment data.</p></li>
</ul>
<p><a href="#id7"><span class="problematic" id="id8">*</span></a>/</p>
</dd>
<dt>public void optimizeIndexes() throws IOException {</dt><dd><p>EarlybirdIndexSegmentWriter unoptimizedWriter = segmentWriterReference.get();
Preconditions.checkNotNull(unoptimizedWriter);</p>
<p>unoptimizedWriter.forceMerge();
unoptimizedWriter.close();</p>
<p>// Optimize our own data structures in the indexing chain
// In the archive this is pretty much a no-op.
// The indexWriter in writeableSegment should no longer be used and referenced, and
// writeableSegment.writer can be garbage collected at this point.
EarlybirdIndexSegmentData optimized = indexConfig.optimize(unoptimizedWriter.getSegmentData());
resetSegmentWriterReference(newWriteableSegment(optimized), true);</p>
<p>addSuccessFile();</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Returns a new, optimized, realtime segment, by copying the data in this segment.</p></li>
</ul>
<p><a href="#id9"><span class="problematic" id="id10">*</span></a>/</p>
</dd>
<dt>public EarlybirdSegment makeOptimizedSegment() throws IOException {</dt><dd><p>EarlybirdIndexSegmentWriter unoptimizedWriter = segmentWriterReference.get();
Preconditions.checkNotNull(unoptimizedWriter);
EarlybirdSegment optimizedSegment = new EarlybirdSegment(</p>
<blockquote>
<div><p>segmentName,
timeSliceID,
maxSegmentSize,
luceneDir,
indexConfig,
searchIndexingMetricSet,
searcherStats,
clock);</p>
</div></blockquote>
<dl class="simple">
<dt>EarlybirdIndexSegmentData optimizedSegmentData =</dt><dd><p>indexConfig.optimize(unoptimizedWriter.getSegmentData());</p>
</dd>
</dl>
<p>LOG.info(“Done optimizing, setting segment data”);</p>
<dl class="simple">
<dt>optimizedSegment.setSegmentData(</dt><dd><p>optimizedSegmentData,
indexStats.getPartialUpdateCount(),
indexStats.getOutOfOrderUpdateCount());</p>
</dd>
</dl>
<p>return optimizedSegment;</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>public String getSegmentName() {</dt><dd><p>return segmentName;</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>public boolean isOptimized() {</dt><dd><p>EarlybirdIndexSegmentWriter segmentWriter = segmentWriterReference.get();
return segmentWriter != null &amp;&amp; segmentWriter.getSegmentData().isOptimized();</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Removes the document for the given tweet ID from this segment, if this segment contains a</p></li>
<li><p>document for this tweet ID.</p></li>
</ul>
<p><a href="#id11"><span class="problematic" id="id12">*</span></a>/</p>
</dd>
<dt>public boolean delete(long tweetID) throws IOException {</dt><dd><p>EarlybirdIndexSegmentWriter segmentWriter = segmentWriterReference.get();
if (!hasDocument(tweetID)) {</p>
<blockquote>
<div><p>return false;</p>
</div></blockquote>
<p>}</p>
<p>segmentWriter.deleteDocuments(new TweetIDQuery(tweetID));
return true;</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>protected void updateDocValues(long tweetID, String field, DocValuesUpdate update)</dt><dd><blockquote>
<div><p>throws IOException {</p>
</div></blockquote>
<p>EarlybirdIndexSegmentWriter segmentWriter = segmentWriterReference.get();
segmentWriter.updateDocValues(new TweetIDQuery(tweetID), field, update);</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Appends the Lucene index from another segment to this segment.</p></li>
</ul>
<p><a href="#id13"><span class="problematic" id="id14">*</span></a>/</p>
</dd>
<dt>public void append(EarlybirdSegment otherSegment) throws IOException {</dt><dd><dl>
<dt>if (indexConfig.isIndexStoredOnDisk()) {</dt><dd><p>EarlybirdIndexSegmentWriter segmentWriter = segmentWriterReference.get();
Preconditions.checkNotNull(segmentWriter);
EarlybirdIndexSegmentWriter otherSegmentWriter = otherSegment.segmentWriterReference.get();
if (otherSegmentWriter != null) {</p>
<blockquote>
<div><p>otherSegmentWriter.close();</p>
</div></blockquote>
<p>}
segmentWriter.addIndexes(otherSegment.luceneDir);
LOG.info(“Calling forceMerge now after appending segment.”);
segmentWriter.forceMerge();
appendedLuceneIndex = true;
LOG.info(“Appended {} docs to segment {}. New doc count = {}”,</p>
<blockquote>
<div><p>otherSegment.indexStats.getStatusCount(), luceneDir.toString(),
indexStats.getStatusCount());</p>
</div></blockquote>
<p>indexStats.setIndexSizeOnDiskInBytes(getSegmentSizeOnDisk());</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Only needed for the on disk archive.</p></li>
<li><p>Creates TwitterIndexReader used for searching. This is shared by all Searchers.</p></li>
<li><p>This method also initializes the Lucene based mappers and CSF for the on disk archive.</p></li>
<li></li>
<li><p>This method should be called after optimizing/loading a segment, but before the segment starts</p></li>
<li><p>to serve search queries.</p></li>
</ul>
<p><a href="#id15"><span class="problematic" id="id16">*</span></a>/</p>
</dd>
<dt>public void warmSegment() throws IOException {</dt><dd><p>EarlybirdIndexSegmentWriter segmentWriter = segmentWriterReference.get();
Preconditions.checkNotNull(segmentWriter);</p>
<p>// only need to pre-create reader and initialize mappers and CSF in the on disk archive cluster
if (indexConfig.isIndexStoredOnDisk() &amp;&amp; luceneIndexReader.get() == null) {</p>
<blockquote>
<div><dl class="simple">
<dt>EarlybirdIndexSegmentAtomicReader luceneAtomicReader =</dt><dd><p>segmentWriter.getSegmentData().createAtomicReader();</p>
</dd>
</dl>
<p>luceneIndexReader.set(luceneAtomicReader);
closableResources.add(luceneAtomicReader);
closableResources.add(luceneDir);</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Create a tweet index searcher on the segment.</p></li>
<li></li>
<li><p>For production search session, the schema snapshot should be always passed in to make sure</p></li>
<li><p>that the schema usage inside scoring is consistent.</p></li>
<li></li>
<li><p>For non-production usage, like one-off debugging search, you can use the function call without</p></li>
<li><p>the schema snapshot.</p></li>
</ul>
<p><a href="#id17"><span class="problematic" id="id18">*</span></a>/</p>
</dd>
</dl>
<p>&#64;Nullable
public EarlybirdSingleSegmentSearcher getSearcher(</p>
<blockquote>
<div><blockquote>
<div><p>UserTable userTable,
ImmutableSchemaInterface schemaSnapshot) throws IOException {</p>
</div></blockquote>
<p>EarlybirdIndexSegmentWriter segmentWriter = segmentWriterReference.get();
if (segmentWriter == null) {</p>
<blockquote>
<div><p>return null;</p>
</div></blockquote>
<p>}
return new EarlybirdSingleSegmentSearcher(</p>
<blockquote>
<div><p>schemaSnapshot, getIndexReader(segmentWriter), userTable, searcherStats, clock);</p>
</div></blockquote>
</div></blockquote>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Returns a new searcher for this segment.</p></li>
</ul>
<p><a href="#id19"><span class="problematic" id="id20">*</span></a>/</p>
</dd>
</dl>
<p>&#64;Nullable
public EarlybirdSingleSegmentSearcher getSearcher(</p>
<blockquote>
<div><blockquote>
<div><p>UserTable userTable) throws IOException {</p>
</div></blockquote>
<p>EarlybirdIndexSegmentWriter segmentWriter = segmentWriterReference.get();
if (segmentWriter == null) {</p>
<blockquote>
<div><p>return null;</p>
</div></blockquote>
<p>}
return new EarlybirdSingleSegmentSearcher(</p>
<blockquote>
<div><p>segmentWriter.getSegmentData().getSchema().getSchemaSnapshot(),
getIndexReader(segmentWriter),
userTable,
searcherStats,
clock);</p>
</div></blockquote>
</div></blockquote>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Returns a new reader for this segment.</p></li>
</ul>
<p><a href="#id21"><span class="problematic" id="id22">*</span></a>/</p>
</dd>
</dl>
<p>&#64;Nullable
public EarlybirdIndexSegmentAtomicReader getIndexReader() throws IOException {</p>
<blockquote>
<div><p>EarlybirdIndexSegmentWriter segmentWriter = segmentWriterReference.get();
if (segmentWriter == null) {</p>
<blockquote>
<div><p>return null;</p>
</div></blockquote>
<p>}
return getIndexReader(segmentWriter);</p>
</div></blockquote>
<p>}</p>
<dl>
<dt>private EarlybirdIndexSegmentAtomicReader getIndexReader(</dt><dd><p>EarlybirdIndexSegmentWriter segmentWriter</p>
</dd>
<dt>) throws IOException {</dt><dd><p>EarlybirdIndexSegmentAtomicReader reader = luceneIndexReader.get();
if (reader != null) {</p>
<blockquote>
<div><p>return reader;</p>
</div></blockquote>
<p>}
Preconditions.checkState(!indexConfig.isIndexStoredOnDisk());</p>
<p>// Realtime EB mode.
return segmentWriter.getSegmentData().createAtomicReader();</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Gets max tweet id in this segment.</p></li>
<li></li>
<li><p>&#64;return the tweet id or -1 if not found.</p></li>
</ul>
<p><a href="#id23"><span class="problematic" id="id24">*</span></a>/</p>
</dd>
<dt>public long getMaxTweetId() {</dt><dd><p>EarlybirdIndexSegmentWriter segmentWriter = segmentWriterReference.get();
if (segmentWriter == null) {</p>
<blockquote>
<div><p>return -1;</p>
</div></blockquote>
<dl>
<dt>} else {</dt><dd><dl class="simple">
<dt>TweetIDMapper tweetIDMapper =</dt><dd><p>(TweetIDMapper) segmentWriter.getSegmentData().getDocIDToTweetIDMapper();</p>
</dd>
</dl>
<p>return tweetIDMapper.getMaxTweetID();</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>private EarlybirdIndexSegmentWriter newWriteableSegment(EarlybirdIndexSegmentData segmentData)</dt><dd><blockquote>
<div><p>throws IOException {</p>
</div></blockquote>
<p>EarlybirdIndexSegmentWriter old = segmentWriterReference.get();
if (old != null) {</p>
<blockquote>
<div><p>old.close();</p>
</div></blockquote>
<p>}</p>
<p>LOG.info(“Creating new segment writer for {} on {}”, segmentName, luceneDir);
IndexWriterConfig indexWriterConfig = indexConfig.newIndexWriterConfig();
return segmentData.createEarlybirdIndexSegmentWriter(indexWriterConfig);</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>private void resetSegmentWriterReference(</dt><dd><blockquote>
<div><p>EarlybirdIndexSegmentWriter segmentWriter, boolean previousSegmentWriterAllowed) {</p>
</div></blockquote>
<dl class="simple">
<dt>EarlybirdIndexSegmentWriter previousSegmentWriter =</dt><dd><p>segmentWriterReference.getAndSet(segmentWriter);</p>
</dd>
<dt>if (!previousSegmentWriterAllowed) {</dt><dd><dl class="simple">
<dt>Preconditions.checkState(</dt><dd><p>previousSegmentWriter == null,
“A previous segment writer must have been set for segment “ + segmentName);</p>
</dd>
</dl>
</dd>
</dl>
<p>}</p>
<p>// Reset the stats for the number of indexed tweets per hour and recompute them.
// See SEARCH-23619
for (SearchCounter indexedTweetsCounter : indexedTweetsCounters.values()) {</p>
<blockquote>
<div><p>indexedTweetsCounter.reset();</p>
</div></blockquote>
<p>}</p>
<dl>
<dt>if (segmentWriter != null) {</dt><dd><p>indexStats.setSegmentData(segmentWriter.getSegmentData());</p>
<dl class="simple">
<dt>if (indexConfig.getCluster() != EarlybirdCluster.FULL_ARCHIVE) {</dt><dd><p>initHourlyTweetCounts(segmentWriterReference.get());</p>
</dd>
</dl>
<p>}</p>
</dd>
<dt>} else {</dt><dd><p>// It’s important to unset segment data so that there are no references to it
// and it can be GC-ed.
indexStats.unsetSegmentDataAndSaveCounts();</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Add a document if it is not already in segment.</p></li>
</ul>
<p><a href="#id25"><span class="problematic" id="id26">*</span></a>/</p>
</dd>
<dt>public void addDocument(TweetDocument doc) throws IOException {</dt><dd><dl class="simple">
<dt>if (indexConfig.isIndexStoredOnDisk()) {</dt><dd><p>addDocumentToArchiveSegment(doc);</p>
</dd>
<dt>} else {</dt><dd><p>addDocumentToRealtimeSegment(doc);</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>private void addDocumentToArchiveSegment(TweetDocument doc) throws IOException {</dt><dd><p>// For archive, the document id should come in order, to drop duplicates, only need to
// compare current id with last one.
long tweetId = doc.getTweetID();
if (tweetId == lastInOrderTweetId) {</p>
<blockquote>
<div><p>LOG.warn(“Dropped duplicate tweet for archive: {}”, tweetId);
DUPLICATE_TWEET_SKIPPED_COUNTER.increment();
return;</p>
</div></blockquote>
<p>}</p>
<dl class="simple">
<dt>if (tweetId &gt; lastInOrderTweetId &amp;&amp; lastInOrderTweetId != 0) {</dt><dd><p>// Archive orders document from newest to oldest, so this shouldn’t happen
LOG.warn(“Encountered out-of-order tweet for archive: {}”, tweetId);
OUT_OF_ORDER_TWEET_COUNTER.increment();</p>
</dd>
<dt>} else {</dt><dd><p>lastInOrderTweetId = tweetId;</p>
</dd>
</dl>
<p>}</p>
<p>addDocumentInternal(doc);</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>private void addDocumentToRealtimeSegment(TweetDocument doc) throws IOException {</dt><dd><p>long tweetId = doc.getTweetID();
boolean outOfOrder = tweetId &lt;= lastInOrderTweetId;
if (outOfOrder) {</p>
<blockquote>
<div><p>OUT_OF_ORDER_TWEET_COUNTER.increment();</p>
</div></blockquote>
<dl class="simple">
<dt>} else {</dt><dd><p>lastInOrderTweetId = tweetId;</p>
</dd>
</dl>
<p>}</p>
<p>// We only need to call hasDocument() for out-of-order tweets.
if (outOfOrder &amp;&amp; hasDocument(tweetId)) {</p>
<blockquote>
<div><p>// We do get duplicates sometimes so you’ll see some amount of these.
DUPLICATE_TWEET_SKIPPED_COUNTER.increment();</p>
</div></blockquote>
<dl class="simple">
<dt>} else {</dt><dd><p>addDocumentInternal(doc);
incrementHourlyTweetCount(doc.getTweetID());</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>private void addDocumentInternal(TweetDocument tweetDocument) throws IOException {</dt><dd><p>Document doc = tweetDocument.getDocument();</p>
<p>// Never write blank documents into the index.
if (doc == null || doc.getFields() == null || doc.getFields().size() == 0) {</p>
<blockquote>
<div><p>return;</p>
</div></blockquote>
<p>}</p>
<p>EarlybirdIndexSegmentWriter segmentWriter = segmentWriterReference.get();
if (segmentWriter == null) {</p>
<blockquote>
<div><dl class="simple">
<dt>EarlybirdIndexSegmentData segmentData = indexConfig.newSegmentData(</dt><dd><p>maxSegmentSize,
timeSliceID,
luceneDir,
extensionsFactory);</p>
</dd>
</dl>
<p>segmentWriter = newWriteableSegment(segmentData);
resetSegmentWriterReference(segmentWriter, false);</p>
</div></blockquote>
<p>}</p>
<dl class="simple">
<dt>Preconditions.checkState(segmentWriter.numDocs() &lt; maxSegmentSize,</dt><dd><p>“Reached max segment size %s”, maxSegmentSize);</p>
</dd>
<dt>IndexableField[] featuresField = doc.getFields(</dt><dd><p>EarlybirdFieldConstants.ENCODED_TWEET_FEATURES_FIELD_NAME);</p>
</dd>
<dt>Preconditions.checkState(featuresField.length == 1,</dt><dd><p>“featuresField.length should be 1, but is %s”, featuresField.length);</p>
</dd>
</dl>
<p>// We require the createdAt field to be set so we can properly filter tweets based on time.
IndexableField[] createdAt =</p>
<blockquote>
<div><p>doc.getFields(EarlybirdFieldConstant.CREATED_AT_FIELD.getFieldName());</p>
</div></blockquote>
<p>Preconditions.checkState(createdAt.length == 1);</p>
<dl class="simple">
<dt>EarlybirdEncodedFeatures features = EarlybirdEncodedFeaturesUtil.fromBytes(</dt><dd><p>indexConfig.getSchema().getSchemaSnapshot(),
EarlybirdFieldConstant.ENCODED_TWEET_FEATURES_FIELD,
featuresField[0].binaryValue().bytes,
featuresField[0].binaryValue().offset);</p>
</dd>
</dl>
<p>boolean currentDocIsOffensive = features.isFlagSet(EarlybirdFieldConstant.IS_OFFENSIVE_FLAG);
perFieldCounters.increment(ThriftIndexingEventType.INSERT, doc);
segmentWriter.addTweet(doc, tweetDocument.getTweetID(), currentDocIsOffensive);</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>private void incrementHourlyTweetCount(long tweetId) {</dt><dd><p>// SEARCH-23619, We won’t attempt to increment the count for pre-snowflake IDs, since
// extracting an exact create time is pretty tricky at this point, and the stat is mostly
// useful for checking realtime tweet indexing.
if (SnowflakeId.isSnowflakeId(tweetId)) {</p>
<blockquote>
<div><p>long tweetCreateTime = SnowflakeId.unixTimeMillisFromId(tweetId);
String tweetHour = HOURLY_COUNT_DATE_TIME_FORMATTER.format(</p>
<blockquote>
<div><p>ZonedDateTime.ofInstant(Instant.ofEpochMilli(tweetCreateTime), ZoneOffset.UTC));</p>
</div></blockquote>
<p>String segmentOptimizedSuffix = isOptimized() ? “optimized” : “unoptimized”;
SearchCounter indexedTweetsCounter = indexedTweetsCounters.computeIfAbsent(</p>
<blockquote>
<div><p>tweetHour + “_” + segmentOptimizedSuffix,
(tweetHourKey) -&gt; SearchCounter.export(String.format(</p>
<blockquote>
<div><p>NUM_TWEETS_CREATED_AT_PATTERN, segmentOptimizedSuffix, segmentName, tweetHour)));</p>
</div></blockquote>
</div></blockquote>
<p>indexedTweetsCounter.increment();</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>private void initHourlyTweetCounts(EarlybirdIndexSegmentWriter segmentWriter) {</dt><dd><p>DocIDToTweetIDMapper mapper = segmentWriter.getSegmentData().getDocIDToTweetIDMapper();
int docId = Integer.MIN_VALUE;
while ((docId = mapper.getNextDocID(docId)) != DocIDToTweetIDMapper.ID_NOT_FOUND) {</p>
<blockquote>
<div><p>incrementHourlyTweetCount(mapper.getTweetID(docId));</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Adds the given document for the given tweet ID to the segment, potentially out of order.</p></li>
</ul>
<p><a href="#id27"><span class="problematic" id="id28">*</span></a>/</p>
</dd>
<dt>public boolean appendOutOfOrder(Document doc, long tweetID) throws IOException {</dt><dd><p>// Never write blank documents into the index.
if (doc == null || doc.getFields() == null || doc.getFields().size() == 0) {</p>
<blockquote>
<div><p>return false;</p>
</div></blockquote>
<p>}</p>
<p>EarlybirdIndexSegmentWriter segmentWriter = segmentWriterReference.get();
if (segmentWriter == null) {</p>
<blockquote>
<div><p>logAppendOutOfOrderFailure(tweetID, doc, “segment is null”);
return false;</p>
</div></blockquote>
<p>}</p>
<dl class="simple">
<dt>if (!indexConfig.supportOutOfOrderIndexing()) {</dt><dd><p>logAppendOutOfOrderFailure(tweetID, doc, “out of order indexing not supported”);
return false;</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>if (!hasDocument(tweetID)) {</dt><dd><p>logAppendOutOfOrderFailure(tweetID, doc, “tweet ID index lookup failed”);
searchIndexingMetricSet.updateOnMissingTweetCounter.increment();
perFieldCounters.incrementTweetNotInIndex(ThriftIndexingEventType.OUT_OF_ORDER_APPEND, doc);
return false;</p>
</dd>
</dl>
<p>}</p>
<p>perFieldCounters.increment(ThriftIndexingEventType.OUT_OF_ORDER_APPEND, doc);
segmentWriter.appendOutOfOrder(new TweetIDQuery(tweetID), doc);
indexStats.incrementOutOfOrderUpdateCount();
return true;</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>private void logAppendOutOfOrderFailure(long tweetID, Document doc, String reason) {</dt><dd><dl class="simple">
<dt>UPDATES_ERRORS_LOG.debug(</dt><dd><p>“appendOutOfOrder() failed to apply update document with hash {} on tweet ID {}: {}”,
Objects.hashCode(doc), tweetID, reason);</p>
</dd>
</dl>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Determines if this segment contains the given tweet ID.</p></li>
</ul>
<p><a href="#id29"><span class="problematic" id="id30">*</span></a>/</p>
</dd>
<dt>public boolean hasDocument(long tweetID) throws IOException {</dt><dd><p>EarlybirdIndexSegmentWriter segmentWriter = segmentWriterReference.get();
if (segmentWriter == null) {</p>
<blockquote>
<div><p>return false;</p>
</div></blockquote>
<p>}</p>
<dl class="simple">
<dt>return segmentWriter.getSegmentData().getDocIDToTweetIDMapper().getDocID(tweetID)</dt><dd><p>!= DocIDToTweetIDMapper.ID_NOT_FOUND;</p>
</dd>
</dl>
</dd>
</dl>
<p>}</p>
<p>private static final String VERSION_PROP_NAME = “version”;
private static final String VERSION_DESC_PROP_NAME = “versionDescription”;
private static final String PARTIAL_UPDATES_COUNT = “partialUpdatesCount”;
private static final String OUT_OF_ORDER_UPDATES_COUNT = “outOfOrderUpdatesCount”;</p>
<dl>
<dt>private void checkIfFlushedDataVersionMatchesExpected(FlushInfo flushInfo) throws IOException {</dt><dd><p>int expectedVersionNumber = indexConfig.getSchema().getMajorVersionNumber();
String expectedVersionDesc = indexConfig.getSchema().getVersionDescription();
int version = flushInfo.getIntProperty(VERSION_PROP_NAME);
final String versionDesc = flushInfo.getStringProperty(VERSION_DESC_PROP_NAME);</p>
<dl class="simple">
<dt>if (version != expectedVersionNumber) {</dt><dd><dl class="simple">
<dt>throw new FlushVersionMismatchException(“Flushed version mismatch. Expected: “</dt><dd><ul class="simple">
<li><p>expectedVersionNumber + “, but was: “ + version);</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
<p>}</p>
<dl>
<dt>if (!expectedVersionDesc.equals(versionDesc)) {</dt><dd><dl class="simple">
<dt>final String message = “Flush version “ + expectedVersionNumber + “ is ambiguous”</dt><dd><ul class="simple">
<li><p>“  Expected: “ + expectedVersionDesc</p></li>
<li><p>“  Found:  “  + versionDesc</p></li>
<li><p>“  Please clean up segments with bad flush version from HDFS and Earlybird local disk.”;</p></li>
</ul>
</dd>
</dl>
<p>throw new FlushVersionMismatchException(message);</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Loads the segment data and properties from the given deserializer and flush info.</p></li>
<li></li>
<li><p>&#64;param in The deserializer from which the segment’s data will be read.</p></li>
<li><p>&#64;param flushInfo The flush info from which the segment’s properties will be read.</p></li>
</ul>
<p><a href="#id31"><span class="problematic" id="id32">*</span></a>/</p>
</dd>
<dt>public void load(DataDeserializer in, FlushInfo flushInfo) throws IOException {</dt><dd><p>checkIfFlushedDataVersionMatchesExpected(flushInfo);</p>
<p>int partialUpdatesCount = flushInfo.getIntProperty(PARTIAL_UPDATES_COUNT);
int outOfOrderUpdatesCount = flushInfo.getIntProperty(OUT_OF_ORDER_UPDATES_COUNT);</p>
<dl class="simple">
<dt>EarlybirdIndexSegmentData loadedSegmentData = indexConfig.loadSegmentData(</dt><dd><p>flushInfo, in, luceneDir, extensionsFactory);</p>
</dd>
</dl>
<p>setSegmentData(loadedSegmentData, partialUpdatesCount, outOfOrderUpdatesCount);</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Update the data backing this EarlyirdSegment.</p></li>
</ul>
<p><a href="#id33"><span class="problematic" id="id34">*</span></a>/</p>
</dd>
<dt>public void setSegmentData(</dt><dd><blockquote>
<div><p>EarlybirdIndexSegmentData segmentData,
int partialUpdatesCount,
int outOfOrderUpdatesCount) throws IOException {</p>
</div></blockquote>
<p>resetSegmentWriterReference(newWriteableSegment(segmentData), false);
try {</p>
<blockquote>
<div><p>warmSegment();</p>
</div></blockquote>
<dl>
<dt>} catch (IOException e) {</dt><dd><dl class="simple">
<dt>LOG.error(“Failed to create IndexReader for segment {}. Will destroy unreadable segment.”,</dt><dd><p>segmentName, e);</p>
</dd>
</dl>
<p>destroyImmediately();
throw e;</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>LOG.info(“Starting segment {} with {} partial updates, {} out of order updates and {} deletes.”,</dt><dd><p>segmentName, partialUpdatesCount, outOfOrderUpdatesCount, indexStats.getDeleteCount());</p>
</dd>
</dl>
<p>indexStats.setPartialUpdateCount(partialUpdatesCount);
indexStats.setOutOfOrderUpdateCount(outOfOrderUpdatesCount);
indexStats.setIndexSizeOnDiskInBytes(getSegmentSizeOnDisk());</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Flushes the this segment’s properties to the given FlushInfo instance, and this segment’s data</p></li>
<li><p>to the given DataSerializer instance.</p></li>
<li></li>
<li><p>&#64;param flushInfo The FlushInfo instance where all segment properties should be added.</p></li>
<li><p>&#64;param out The serializer to which all segment data should be flushed.</p></li>
</ul>
<p><a href="#id35"><span class="problematic" id="id36">*</span></a>/</p>
</dd>
<dt>public void flush(FlushInfo flushInfo, DataSerializer out) throws IOException {</dt><dd><p>flushInfo.addIntProperty(VERSION_PROP_NAME, indexConfig.getSchema().getMajorVersionNumber());
flushInfo.addStringProperty(VERSION_DESC_PROP_NAME,</p>
<blockquote>
<div><p>indexConfig.getSchema().getVersionDescription());</p>
</div></blockquote>
<p>flushInfo.addIntProperty(PARTIAL_UPDATES_COUNT, indexStats.getPartialUpdateCount());
flushInfo.addIntProperty(OUT_OF_ORDER_UPDATES_COUNT, indexStats.getOutOfOrderUpdateCount());
if (segmentWriterReference.get() == null) {</p>
<blockquote>
<div><p>LOG.warn(“Segment writer is null. flushInfo: {}”, flushInfo);</p>
</div></blockquote>
<dl class="simple">
<dt>} else if (segmentWriterReference.get().getSegmentData() == null) {</dt><dd><dl class="simple">
<dt>LOG.warn(“Segment data is null. segment writer: {}, flushInfo: {}”,</dt><dd><p>segmentWriterReference.get(), flushInfo);</p>
</dd>
</dl>
</dd>
</dl>
<p>}
segmentWriterReference.get().getSegmentData().flushSegment(flushInfo, out);
indexStats.setIndexSizeOnDiskInBytes(getSegmentSizeOnDisk());</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Check to see if this segment can be loaded from an on-disk index, and load it if it can be.</p></li>
<li></li>
<li><p>This should only be applicable to the current segment for the on-disk archive. It’s not</p></li>
<li><p>fully flushed until it’s full, but we do have a lucene index on local disk which can be</p></li>
<li><p>used at startup (rather than have to reindex all the current timeslice documents again).</p></li>
<li></li>
<li><p>If loaded, the index reader will be pre-created, and the segment will be marked as</p></li>
<li><p>optimized.</p></li>
<li></li>
<li><p>If the index directory exists but it cannot be loaded, the index directory will be deleted.</p></li>
<li></li>
<li><p>&#64;return true if the index exists on disk, and was loaded.</p></li>
</ul>
<p><a href="#id37"><span class="problematic" id="id38">*</span></a>/</p>
</dd>
<dt>public boolean tryToLoadExistingIndex() throws IOException {</dt><dd><p>Preconditions.checkState(segmentWriterReference.get() == null);
if (indexConfig.isIndexStoredOnDisk()) {</p>
<blockquote>
<div><dl>
<dt>if (DirectoryReader.indexExists(luceneDir) &amp;&amp; checkSuccessFile()) {</dt><dd><p>LOG.info(“Index directory already exists for {} at {}”, segmentName, luceneDir);</p>
<p>// set the optimized flag, since we don’t need to optimize any more, and pre-create
// the index reader (for the on-disk index optimize() is a noop that just sets the
// optimized flag).
EarlybirdIndexSegmentData earlybirdIndexSegmentData = indexConfig.newSegmentData(</p>
<blockquote>
<div><p>maxSegmentSize,
timeSliceID,
luceneDir,
extensionsFactory);</p>
</div></blockquote>
<dl class="simple">
<dt>EarlybirdIndexSegmentData optimizedEarlybirdIndexSegmentData =</dt><dd><p>indexConfig.optimize(earlybirdIndexSegmentData);</p>
</dd>
</dl>
<p>resetSegmentWriterReference(newWriteableSegment(optimizedEarlybirdIndexSegmentData), false);</p>
<p>warmSegment();</p>
<dl class="simple">
<dt>LOG.info(“Used existing lucene index for {} with {} documents”,</dt><dd><p>segmentName, indexStats.getStatusCount());</p>
</dd>
</dl>
<p>indexStats.setIndexSizeOnDiskInBytes(getSegmentSizeOnDisk());</p>
<p>return true;</p>
</dd>
<dt>} else {</dt><dd><p>// Check if there is an existing lucene dir without a SUCCESS file on disk.
// If so, we will remove it and reindex from scratch.
if (moveFSDirectoryIfExists(luceneDir)) {</p>
<blockquote>
<div><p>// Throw here to be cleaned up and retried by SimpleSegmentIndexer.
throw new IOException(“Found invalid existing lucene directory at: “ + luceneDir);</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>}
return false;</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Partially updates a document with the field value(s) specified by event.</p></li>
<li><p>Returns true if all writes were successful and false if one or more writes fail or if</p></li>
<li><p>tweet id isn’t found in the segment.</p></li>
</ul>
<p><a href="#id39"><span class="problematic" id="id40">*</span></a>/</p>
</dd>
<dt>public boolean applyPartialUpdate(ThriftIndexingEvent event) throws IOException {</dt><dd><p>Preconditions.checkArgument(event.getEventType() == ThriftIndexingEventType.PARTIAL_UPDATE);
Preconditions.checkArgument(event.isSetUid());
Preconditions.checkArgument(!ThriftDocumentUtil.hasDuplicateFields(event.getDocument()));
ImmutableSchemaInterface schemaSnapshot = indexConfig.getSchema().getSchemaSnapshot();</p>
<p>long tweetId = event.getUid();
ThriftDocument doc = event.getDocument();</p>
<dl>
<dt>if (!hasDocument(tweetId)) {</dt><dd><p>// no need to attempt field writes, fail early
PARTIAL_UPDATE_FOR_TWEET_NOT_IN_INDEX.increment();</p>
<blockquote>
<div><dl class="simple">
<dt>perFieldCounters.incrementTweetNotInIndex(</dt><dd><p>ThriftIndexingEventType.PARTIAL_UPDATE, doc);</p>
</dd>
</dl>
</div></blockquote>
<p>return false;</p>
</dd>
</dl>
<p>}</p>
<p>int invalidFields = 0;
for (ThriftField field : doc.getFields()) {</p>
<blockquote>
<div><p>String featureName = schemaSnapshot.getFieldName(field.getFieldConfigId());
FeatureConfiguration featureConfig =</p>
<blockquote>
<div><p>schemaSnapshot.getFeatureConfigurationByName(featureName);</p>
</div></blockquote>
<dl class="simple">
<dt>if (featureConfig == null) {</dt><dd><p>INVALID_FIELDS_IN_PARTIAL_UPDATES.increment();
invalidFields++;
continue;</p>
</dd>
</dl>
<p>}</p>
<p>perFieldCounters.increment(ThriftIndexingEventType.PARTIAL_UPDATE, featureName);</p>
<dl class="simple">
<dt>updateDocValues(</dt><dd><p>tweetId,
featureName,
(docValues, docID) -&gt; updateFeatureValue(docID, featureConfig, docValues, field));</p>
</dd>
</dl>
</div></blockquote>
<p>}</p>
<dl class="simple">
<dt>if (invalidFields &gt; 0 &amp;&amp; invalidFields != doc.getFieldsSize()) {</dt><dd><p>PARTIAL_UPDATE_PARTIAL_FAILURE.increment();</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>if (invalidFields == 0) {</dt><dd><p>indexStats.incrementPartialUpdateCount();</p>
</dd>
<dt>} else {</dt><dd><dl class="simple">
<dt>UPDATES_ERRORS_LOG.warn(“Failed to apply update for tweetID {}, found {} invalid fields: {}”,</dt><dd><p>tweetId, invalidFields, event);</p>
</dd>
</dl>
</dd>
</dl>
<p>}</p>
<p>return invalidFields == 0;</p>
</dd>
</dl>
<p>}</p>
<p>&#64;VisibleForTesting
static void updateFeatureValue(int docID,</p>
<blockquote>
<div><blockquote>
<div><p>FeatureConfiguration featureConfig,
ColumnStrideFieldIndex docValues,
ThriftField updateField) {</p>
</div></blockquote>
<p>int oldValue = Math.toIntExact(docValues.get(docID));
int newValue = updateField.getFieldData().getIntValue();</p>
<dl>
<dt>if (!featureConfig.validateFeatureUpdate(oldValue, newValue)) {</dt><dd><p>// Counter values can only increase
SearchCounter.export(</p>
<blockquote>
<div><p>INVALID_FEATURE_UPDATES_DROPPED_PREFIX + featureConfig.getName()).increment();</p>
</div></blockquote>
</dd>
<dt>} else {</dt><dd><p>docValues.setValue(docID, newValue);</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Checks if the provided directory exists and is not empty,</p></li>
<li><p>and if it does moves it out to a diff directory for later inspection.</p></li>
<li><p>&#64;param luceneDirectory the dir to move if it exists.</p></li>
<li><p>&#64;return true iff we found an existing directory.</p></li>
</ul>
<p><a href="#id41"><span class="problematic" id="id42">*</span></a>/</p>
</dd>
<dt>private static boolean moveFSDirectoryIfExists(Directory luceneDirectory) {</dt><dd><p>Preconditions.checkState(luceneDirectory instanceof FSDirectory);
File directory = ((FSDirectory) luceneDirectory).getDirectory().toFile();
if (directory != null &amp;&amp; directory.exists() &amp;&amp; directory.list().length &gt; 0) {</p>
<blockquote>
<div><p>// Save the bad lucene index by moving it out, for later inspection.
File movedDir = new File(directory.getParent(),</p>
<blockquote>
<div><p>directory.getName() + “.failed.” + System.currentTimeMillis());</p>
</div></blockquote>
<dl class="simple">
<dt>LOG.warn(“Moving existing non-successful index for {} from {} to {}”,</dt><dd><p>luceneDirectory, directory, movedDir);</p>
</dd>
</dl>
<p>boolean success = directory.renameTo(movedDir);
if (!success) {</p>
<blockquote>
<div><p>LOG.warn(“Unable to rename non-successful index: {}”, luceneDirectory);</p>
</div></blockquote>
<p>}
return true;</p>
</div></blockquote>
<p>}
return false;</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>For the on-disk archive, if we were able to successfully merge and flush the Lucene index to</p></li>
<li><p>disk, we mark it explicitly with a SUCCESS file, so that it can be safely reused.</p></li>
</ul>
<p><a href="#id43"><span class="problematic" id="id44">*</span></a>/</p>
</dd>
<dt>private void addSuccessFile() throws IOException {</dt><dd><dl class="simple">
<dt>if (indexConfig.isIndexStoredOnDisk()) {</dt><dd><p>IndexOutput successFile = luceneDir.createOutput(SUCCESS_FILE, IOContext.DEFAULT);
successFile.close();</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Returns the current number of documents in this segment.</p></li>
</ul>
<p><a href="#id45"><span class="problematic" id="id46">*</span></a>/</p>
</dd>
<dt>public int getNumDocs() throws IOException {</dt><dd><p>return indexStats.getStatusCount();</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Reclaim resources used by this segment (E.g. closing lucene index reader).</p></li>
<li><p>Resources will be reclaimed within the calling thread with no delay.</p></li>
</ul>
<p><a href="#id47"><span class="problematic" id="id48">*</span></a>/</p>
</dd>
<dt>public void destroyImmediately() {</dt><dd><dl class="simple">
<dt>try {</dt><dd><p>closeSegmentWriter();
maybeDeleteSegmentOnDisk();
unloadSegmentFromMemory();</p>
</dd>
<dt>} finally {</dt><dd><p>indexConfig.getResourceCloser().closeResourcesImmediately(closableResources);</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Close the in-memory resources belonging to this segment. This should allow the in-memory</p></li>
<li><p>segment data to be garbage collected. After closing, the segment is not writable.</p></li>
</ul>
<p><a href="#id49"><span class="problematic" id="id50">*</span></a>/</p>
</dd>
<dt>public void close() {</dt><dd><dl class="simple">
<dt>if (segmentWriterReference.get() == null) {</dt><dd><p>LOG.info(“Segment {} already closed.”, segmentName);
return;</p>
</dd>
</dl>
<p>}</p>
<p>LOG.info(“Closing segment {}.”, segmentName);
try {</p>
<blockquote>
<div><p>closeSegmentWriter();
unloadSegmentFromMemory();</p>
</div></blockquote>
<dl class="simple">
<dt>} finally {</dt><dd><p>indexConfig.getResourceCloser().closeResourcesImmediately(closableResources);</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>private void closeSegmentWriter() {</dt><dd><p>EarlybirdIndexSegmentWriter segmentWriter = segmentWriterReference.get();
if (segmentWriter != null) {</p>
<blockquote>
<div><dl class="simple">
<dt>closableResources.add(() -&gt; {</dt><dd><p>LOG.info(“Closing writer for segment: {}”, segmentName);
segmentWriter.close();</p>
</dd>
</dl>
<p>});</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>private void maybeDeleteSegmentOnDisk() {</dt><dd><dl>
<dt>if (indexConfig.isIndexStoredOnDisk()) {</dt><dd><dl class="simple">
<dt>Preconditions.checkState(</dt><dd><p>luceneDir instanceof FSDirectory,
“On-disk indexes should have an underlying directory that we can close and remove.”);</p>
</dd>
</dl>
<p>closableResources.add(luceneDir);</p>
<dl>
<dt>if (luceneDirFile != null &amp;&amp; luceneDirFile.exists()) {</dt><dd><dl>
<dt>closableResources.add(new Closeable() {</dt><dd><p>&#64;Override
public void close() throws IOException {</p>
<blockquote>
<div><p>FileUtils.deleteDirectory(luceneDirFile);</p>
</div></blockquote>
<p>}</p>
<p>&#64;Override
public String toString() {</p>
<blockquote>
<div><p>return “delete {” + luceneDirFile + “}”;</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>});</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>private void unloadSegmentFromMemory() {</dt><dd><p>// Make sure we don’t retain a reference to the IndexWriter or SegmentData.
resetSegmentWriterReference(null, true);</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>private long getSegmentSizeOnDisk() throws IOException {</dt><dd><p>searchIndexingMetricSet.segmentSizeCheckCount.increment();</p>
<p>long totalSize = 0;
if (luceneDir != null) {</p>
<blockquote>
<div><dl class="simple">
<dt>for (String file<span class="classifier">luceneDir.listAll()) {</span></dt><dd><p>totalSize += luceneDir.fileLength(file);</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>}
return totalSize;</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>public EarlybirdIndexConfig getEarlybirdIndexConfig() {</dt><dd><p>return indexConfig;</p>
</dd>
</dl>
<p>}</p>
<p>&#64;VisibleForTesting
public boolean checkSuccessFile() {</p>
<blockquote>
<div><p>return new File(luceneDirFile, SUCCESS_FILE).exists();</p>
</div></blockquote>
<p>}</p>
<p>&#64;VisibleForTesting
EarlybirdIndexSegmentWriter getIndexSegmentWriter() {</p>
<blockquote>
<div><p>return segmentWriterReference.get();</p>
</div></blockquote>
<p>}</p>
<p>// Helper class to encapsulate counter tables, patterns and various ways to increment
private class PerFieldCounters {</p>
<blockquote>
<div><p>// The number of update/append events for each field in the schema.
private static final String PER_FIELD_EVENTS_COUNTER_PATTERN = “%s_for_field_%s”;
// The number of dropped update/append events for each field due to tweetId not found
private static final String TWEET_NOT_IN_INDEX_PER_FIELD_EVENTS_COUNTER_PATTERN =</p>
<blockquote>
<div><p>“%s_for_tweet_id_not_in_index_for_field_%s”;</p>
</div></blockquote>
<dl>
<dt>private final Table&lt;ThriftIndexingEventType, String, SearchCounter&gt; perFieldTable =</dt><dd><p>HashBasedTable.create();</p>
</dd>
<dt>private final Table&lt;ThriftIndexingEventType, String, SearchCounter&gt; notInIndexPerFieldTable =</dt><dd><p>HashBasedTable.create();</p>
</dd>
<dt>public void increment(</dt><dd><blockquote>
<div><p>ThriftIndexingEventType eventType, ThriftDocument doc) {</p>
</div></blockquote>
<p>ImmutableSchemaInterface schemaSnapshot = indexConfig.getSchema().getSchemaSnapshot();
for (ThriftField field : doc.getFields()) {</p>
<blockquote>
<div><p>String fieldName = schemaSnapshot.getFieldName(field.getFieldConfigId());
incrementForPattern(</p>
<blockquote>
<div><p>eventType, fieldName, perFieldTable, PER_FIELD_EVENTS_COUNTER_PATTERN);</p>
</div></blockquote>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>public void incrementTweetNotInIndex(</dt><dd><blockquote>
<div><p>ThriftIndexingEventType eventType, ThriftDocument doc) {</p>
</div></blockquote>
<p>ImmutableSchemaInterface schemaSnapshot = indexConfig.getSchema().getSchemaSnapshot();
for (ThriftField field : doc.getFields()) {</p>
<blockquote>
<div><p>String fieldName = schemaSnapshot.getFieldName(field.getFieldConfigId());
incrementForPattern(</p>
<blockquote>
<div><p>eventType, fieldName, notInIndexPerFieldTable,
TWEET_NOT_IN_INDEX_PER_FIELD_EVENTS_COUNTER_PATTERN);</p>
</div></blockquote>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>public void increment(ThriftIndexingEventType eventType, Document doc) {</dt><dd><dl class="simple">
<dt>for (IndexableField field<span class="classifier">doc.getFields()) {</span></dt><dd><dl class="simple">
<dt>incrementForPattern(</dt><dd><p>eventType, field.name(),
perFieldTable, PER_FIELD_EVENTS_COUNTER_PATTERN);</p>
</dd>
</dl>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>public void increment(ThriftIndexingEventType eventType, String fieldName) {</dt><dd><p>incrementForPattern(eventType, fieldName, perFieldTable, PER_FIELD_EVENTS_COUNTER_PATTERN);</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>public void incrementTweetNotInIndex(ThriftIndexingEventType eventType, Document doc) {</dt><dd><dl class="simple">
<dt>for (IndexableField field<span class="classifier">doc.getFields()) {</span></dt><dd><dl class="simple">
<dt>incrementForPattern(</dt><dd><p>eventType, field.name(),
notInIndexPerFieldTable,
TWEET_NOT_IN_INDEX_PER_FIELD_EVENTS_COUNTER_PATTERN);</p>
</dd>
</dl>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>private void incrementForPattern(</dt><dd><blockquote>
<div><p>ThriftIndexingEventType eventType, String fieldName,
Table&lt;ThriftIndexingEventType, String, SearchCounter&gt; counterTable, String pattern) {</p>
</div></blockquote>
<p>SearchCounter stat;
if (counterTable.contains(eventType, fieldName)) {</p>
<blockquote>
<div><p>stat = counterTable.get(eventType, fieldName);</p>
</div></blockquote>
<dl class="simple">
<dt>} else {</dt><dd><p>stat = SearchCounter.export(String.format(pattern, eventType, fieldName).toLowerCase());
counterTable.put(eventType, fieldName, stat);</p>
</dd>
</dl>
<p>}
stat.increment();</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../../../../index.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../../../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../../../../../_sources/src/java/com/twitter/search/earlybird/index/EarlybirdSegment.java.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>