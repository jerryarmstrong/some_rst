<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>#[cfg(feature = “tf”)]
pub mod tf {</p>
<blockquote>
<div><p>use arrayvec::ArrayVec;
use itertools::Itertools;
use log::{debug, error, info, warn};
use prost::Message;
use std::fmt;
use std::fmt::Display;
use std::string::String;
use tensorflow::io::{RecordReader, RecordReadError};
use tensorflow::Operation;
use tensorflow::SavedModelBundle;
use tensorflow::SessionOptions;
use tensorflow::SessionRunArgs;
use tensorflow::Tensor;
use tensorflow::{DataType, FetchToken, Graph, TensorInfo, TensorType};</p>
<p>use std::thread::sleep;
use std::time::Duration;</p>
<p>use crate::cli_args::{Args, ARGS, INPUTS, MODEL_SPECS, OUTPUTS};
use crate::tf_proto::tensorflow_serving::prediction_log::LogType;
use crate::tf_proto::tensorflow_serving::{PredictionLog, PredictLog};
use crate::tf_proto::ConfigProto;
use anyhow::{Context, Result};
use serde_json::Value;</p>
<p>use crate::TensorReturnEnum;
use crate::bootstrap::{TensorInput, TensorInputEnum};
use crate::metrics::{</p>
<blockquote>
<div><p>INFERENCE_FAILED_REQUESTS_BY_MODEL, NUM_REQUESTS_FAILED, NUM_REQUESTS_FAILED_BY_MODEL,</p>
</div></blockquote>
<p>};
use crate::predict_service::Model;
use crate::{MAX_NUM_INPUTS, utils};</p>
<p>#[derive(Debug)]
pub enum TFTensorEnum {</p>
<blockquote>
<div><p>String(Tensor&lt;String&gt;),
Int(Tensor&lt;i32&gt;),
Int64(Tensor&lt;i64&gt;),
Float(Tensor&lt;f32&gt;),
Double(Tensor&lt;f64&gt;),
Boolean(Tensor&lt;bool&gt;),</p>
</div></blockquote>
<p>}</p>
<p>#[derive(Debug)]
pub struct TFModel {</p>
<blockquote>
<div><p>pub model_idx: usize,
pub bundle: SavedModelBundle,
pub input_names: ArrayVec&lt;String, MAX_NUM_INPUTS&gt;,
pub input_info: Vec&lt;TensorInfo&gt;,
pub input_ops: Vec&lt;Operation&gt;,
pub output_names: Vec&lt;String&gt;,
pub output_info: Vec&lt;TensorInfo&gt;,
pub output_ops: Vec&lt;Operation&gt;,
pub export_dir: String,
pub version: i64,
pub inter_op: i32,
pub intra_op: i32,</p>
</div></blockquote>
<p>}</p>
<dl>
<dt>impl Display for TFModel {</dt><dd><dl>
<dt>fn fmt(&amp;self, f: &amp;mut fmt::Formatter) -&gt; fmt::Result {</dt><dd><dl class="simple">
<dt>write!(</dt><dd><p>f,
“idx: {}, tensorflow model_name:{}, export_dir:{}, version:{}, inter:{}, intra:{}”,
self.model_idx,
MODEL_SPECS[self.model_idx],
self.export_dir,
self.version,
self.inter_op,
self.intra_op</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>impl TFModel {</dt><dd><dl>
<dt>pub fn new(idx: usize, version: String, model_config: &amp;Value) -&gt; Result&lt;TFModel&gt; {</dt><dd><p>// Create input variables for our addition
let config = ConfigProto {</p>
<blockquote>
<div><dl class="simple">
<dt>intra_op_parallelism_threads: utils::get_config_or(</dt><dd><p>model_config,
“intra_op_parallelism”,
&amp;ARGS.intra_op_parallelism[idx],</p>
</dd>
</dl>
<p>)
.parse()?,
inter_op_parallelism_threads: utils::get_config_or(</p>
<blockquote>
<div><p>model_config,
“inter_op_parallelism”,
&amp;ARGS.inter_op_parallelism[idx],</p>
</div></blockquote>
<p>)
.parse()?,
..Default::default()</p>
</div></blockquote>
<p>};
let mut buf = Vec::new();
buf.reserve(config.encoded_len());
config.encode(&amp;mut buf).unwrap();
let mut opts = SessionOptions::new();
opts.set_config(&amp;buf)?;
let export_dir = format!(“{}/{}”, ARGS.model_dir[idx], version);
let mut graph = Graph::new();
let bundle = SavedModelBundle::load(&amp;opts, [“serve”], &amp;mut graph, &amp;export_dir)</p>
<blockquote>
<div><p>.context(“error load model”)?;</p>
</div></blockquote>
<dl>
<dt>let signature = bundle</dt><dd><p>.meta_graph_def()
.get_signature(&amp;ARGS.serving_sig[idx])
.context(“error finding signature”)?;</p>
</dd>
<dt>let input_names = INPUTS[idx]</dt><dd><dl>
<dt>.get_or_init(|| {</dt><dd><dl class="simple">
<dt>let input_spec = signature</dt><dd><p>.inputs()
.iter()
.map(<a href="#id11"><span class="problematic" id="id12">|p|</span></a> p.0.clone())
.collect::&lt;ArrayVec&lt;String, MAX_NUM_INPUTS&gt;&gt;();</p>
</dd>
<dt>info!(</dt><dd><p>“input not set from cli, now we set from model metadata:{:?}”,
input_spec</p>
</dd>
</dl>
<p>);
input_spec</p>
</dd>
</dl>
<p>})
.clone();</p>
</dd>
<dt>let input_info = input_names</dt><dd><p>.iter()
.map(<a href="#id13"><span class="problematic" id="id14">|i|</span></a> {</p>
<blockquote>
<div><dl class="simple">
<dt>signature</dt><dd><p>.get_input(i)
.context(“error finding input op info”)
.unwrap()
.clone()</p>
</dd>
</dl>
</div></blockquote>
<p>})
.collect_vec();</p>
</dd>
<dt>let input_ops = input_info</dt><dd><p>.iter()
.map(<a href="#id15"><span class="problematic" id="id16">|i|</span></a> {</p>
<blockquote>
<div><dl class="simple">
<dt>graph</dt><dd><p>.operation_by_name_required(&amp;i.name().name)
.context(“error finding input op”)
.unwrap()</p>
</dd>
</dl>
</div></blockquote>
<p>})
.collect_vec();</p>
</dd>
</dl>
<p>info!(“Model Input size: {}”, input_info.len());</p>
<p>let output_names = OUTPUTS[idx].to_vec().clone();</p>
<dl>
<dt>let output_info = output_names</dt><dd><p>.iter()
.map(<a href="#id17"><span class="problematic" id="id18">|o|</span></a> {</p>
<blockquote>
<div><dl class="simple">
<dt>signature</dt><dd><p>.get_output(o)
.context(“error finding output op info”)
.unwrap()
.clone()</p>
</dd>
</dl>
</div></blockquote>
<p>})
.collect_vec();</p>
</dd>
<dt>let output_ops = output_info</dt><dd><p>.iter()
.map(<a href="#id19"><span class="problematic" id="id20">|o|</span></a> {</p>
<blockquote>
<div><dl class="simple">
<dt>graph</dt><dd><p>.operation_by_name_required(&amp;o.name().name)
.context(“error finding output op”)
.unwrap()</p>
</dd>
</dl>
</div></blockquote>
<p>})
.collect_vec();</p>
</dd>
<dt>let tf_model = TFModel {</dt><dd><p>model_idx: idx,
bundle,
input_names,
input_info,
input_ops,
output_names,
output_info,
output_ops,
export_dir,
version: Args::version_str_to_epoch(&amp;version)?,
inter_op: config.inter_op_parallelism_threads,
intra_op: config.intra_op_parallelism_threads,</p>
</dd>
</dl>
<p>};
tf_model.warmup()?;
Ok(tf_model)</p>
</dd>
</dl>
<p>}</p>
<p>#[inline(always)]
fn get_tftensor_dimensions&lt;T&gt;(</p>
<blockquote>
<div><p>t: &amp;[T],
input_size: u64,
batch_size: u64,
input_dims: Option&lt;Vec&lt;i64&gt;&gt;,</p>
</div></blockquote>
<dl>
<dt>) -&gt; Vec&lt;u64&gt; {</dt><dd><p>// if input size is 1, we just specify a single dimension to outgoing tensor matching the
// size of the input tensor. This is for backwards compatiblity with existing Navi clients
// which specify input as a single string tensor (like tfexample) and use batching support.
let mut dims = vec![];
if input_size &gt; 1 {</p>
<blockquote>
<div><dl>
<dt>if batch_size == 1 &amp;&amp; input_dims.is_some() {</dt><dd><p>// client side batching is enabled?
input_dims</p>
<blockquote>
<div><p>.unwrap()
.iter()
.for_each(<a href="#id21"><span class="problematic" id="id22">|axis|</span></a> dims.push(<a href="#id1"><span class="problematic" id="id2">*</span></a>axis as u64));</p>
</div></blockquote>
</dd>
<dt>} else {</dt><dd><p>dims.push(batch_size);
dims.push(t.len() as u64 / batch_size);</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<dl class="simple">
<dt>} else {</dt><dd><p>dims.push(t.len() as u64);</p>
</dd>
</dl>
<p>}
dims</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>fn convert_to_tftensor_enum(</dt><dd><p>input: TensorInput,
input_size: u64,
batch_size: u64,</p>
</dd>
<dt>) -&gt; TFTensorEnum {</dt><dd><dl>
<dt>match input.tensor_data {</dt><dd><dl>
<dt>TensorInputEnum::String(t) =&gt; {</dt><dd><dl>
<dt>let strings = t</dt><dd><p>.into_iter()
.map(<a href="#id23"><span class="problematic" id="id24">|x|</span></a> unsafe { String::from_utf8_unchecked(x) })
.collect_vec();</p>
</dd>
<dt>TFTensorEnum::String(</dt><dd><dl class="simple">
<dt>Tensor::new(&amp;TFModel::get_tftensor_dimensions(</dt><dd><p>strings.as_slice(),
input_size,
batch_size,
input.dims,</p>
</dd>
</dl>
<p>))
.with_values(strings.as_slice())
.unwrap(),</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
<p>}
TensorInputEnum::Int(t) =&gt; TFTensorEnum::Int(</p>
<blockquote>
<div><dl class="simple">
<dt>Tensor::new(&amp;TFModel::get_tftensor_dimensions(</dt><dd><p>t.as_slice(),
input_size,
batch_size,
input.dims,</p>
</dd>
</dl>
<p>))
.with_values(t.as_slice())
.unwrap(),</p>
</div></blockquote>
<p>),
TensorInputEnum::Int64(t) =&gt; TFTensorEnum::Int64(</p>
<blockquote>
<div><dl class="simple">
<dt>Tensor::new(&amp;TFModel::get_tftensor_dimensions(</dt><dd><p>t.as_slice(),
input_size,
batch_size,
input.dims,</p>
</dd>
</dl>
<p>))
.with_values(t.as_slice())
.unwrap(),</p>
</div></blockquote>
<p>),
TensorInputEnum::Float(t) =&gt; TFTensorEnum::Float(</p>
<blockquote>
<div><dl class="simple">
<dt>Tensor::new(&amp;TFModel::get_tftensor_dimensions(</dt><dd><p>t.as_slice(),
input_size,
batch_size,
input.dims,</p>
</dd>
</dl>
<p>))
.with_values(t.as_slice())
.unwrap(),</p>
</div></blockquote>
<p>),
TensorInputEnum::Double(t) =&gt; TFTensorEnum::Double(</p>
<blockquote>
<div><dl class="simple">
<dt>Tensor::new(&amp;TFModel::get_tftensor_dimensions(</dt><dd><p>t.as_slice(),
input_size,
batch_size,
input.dims,</p>
</dd>
</dl>
<p>))
.with_values(t.as_slice())
.unwrap(),</p>
</div></blockquote>
<p>),
TensorInputEnum::Boolean(t) =&gt; TFTensorEnum::Boolean(</p>
<blockquote>
<div><dl class="simple">
<dt>Tensor::new(&amp;TFModel::get_tftensor_dimensions(</dt><dd><p>t.as_slice(),
input_size,
batch_size,
input.dims,</p>
</dd>
</dl>
<p>))
.with_values(t.as_slice())
.unwrap(),</p>
</div></blockquote>
<p>),</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}
fn fetch_output&lt;T: TensorType&gt;(</p>
<blockquote>
<div><p>args: &amp;mut SessionRunArgs,
token_output: &amp;FetchToken,
batch_size: u64,
output_size: u64,</p>
</div></blockquote>
<dl>
<dt>) -&gt; (Tensor&lt;T&gt;, u64) {</dt><dd><p>let tensor_output = args.fetch::&lt;T&gt;(<a href="#id3"><span class="problematic" id="id4">*</span></a>token_output).expect(“fetch output failed”);
let mut tensor_width = tensor_output.dims()[1];
if batch_size == 1 &amp;&amp; output_size &gt; 1 {</p>
<blockquote>
<div><dl class="simple">
<dt>tensor_width = tensor_output.dims().iter().fold(1, <a href="#id25"><span class="problematic" id="id26">|mut total, &amp;val|</span></a> {</dt><dd><p>total <a href="#id5"><span class="problematic" id="id6">*</span></a>= val;
total</p>
</dd>
</dl>
<p>});</p>
</div></blockquote>
<p>}
(tensor_output, tensor_width)</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>impl Model for TFModel {</dt><dd><dl>
<dt>fn warmup(&amp;self) -&gt; Result&lt;()&gt; {</dt><dd><p>// warm up
let warmup_file = format!(</p>
<blockquote>
<div><p>“{}/assets.extra/tf_serving_warmup_requests”,
self.export_dir</p>
</div></blockquote>
<p>);
if std::path::Path::new(&amp;warmup_file).exists() {</p>
<blockquote>
<div><p>use std::io::Cursor;
info!(</p>
<blockquote>
<div><p>“found warmup assets in {}, now perform warming up”,
warmup_file</p>
</div></blockquote>
<p>);
let f = std::fs::File::open(warmup_file).context(“cannot open warmup file”)?;
// let mut buf = Vec::new();
let read = std::io::BufReader::new(f);
let mut reader = RecordReader::new(read);
let mut warmup_cnt = 0;
loop {</p>
<blockquote>
<div><p>let next = reader.read_next_owned();
match next {</p>
<blockquote>
<div><dl>
<dt>Ok(res) =&gt; match res {</dt><dd><dl>
<dt>Some(vec) =&gt; {</dt><dd><p>// info!(“read one tfRecord”);
match PredictionLog::decode(&amp;mut Cursor::new(vec))</p>
<blockquote>
<div><p>.context(“can’t parse PredictonLog”)?</p>
</div></blockquote>
<dl>
<dt>{</dt><dd><dl>
<dt>PredictionLog {</dt><dd><p>log_metadata: _,
log_type:</p>
<blockquote>
<div><dl class="simple">
<dt>Some(LogType::PredictLog(PredictLog {</dt><dd><p>request: Some(mut req),
response: _,</p>
</dd>
</dl>
<p>})),</p>
</div></blockquote>
</dd>
<dt>} =&gt; {</dt><dd><dl>
<dt>if warmup_cnt == ARGS.max_warmup_records {</dt><dd><p>//warm up to max_warmup_records  records
warn!(</p>
<blockquote>
<div><p>“reached max warmup {} records, exit warmup for {}”,
ARGS.max_warmup_records,
MODEL_SPECS[self.model_idx]</p>
</div></blockquote>
<p>);
break;</p>
</dd>
</dl>
<p>}
self.do_predict(</p>
<blockquote>
<div><p>vec![req.take_input_vals(&amp;self.input_names)],
1,</p>
</div></blockquote>
<p>);
sleep(Duration::from_millis(100));
warmup_cnt += 1;</p>
</dd>
</dl>
<p>}
_ =&gt; error!(“some wrong record in warming up file”),</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}
None =&gt; {</p>
<blockquote>
<div><p>info!(“end of warmup file, warmed up with records: {}”, warmup_cnt);
break;</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>},
Err(RecordReadError::CorruptFile)
| Err(RecordReadError::IoError { .. }) =&gt; {</p>
<blockquote>
<div><p>error!(“read tfrecord error for warmup files, skip”);</p>
</div></blockquote>
<p>}
_ =&gt; {}</p>
</div></blockquote>
<p>}</p>
</div></blockquote>
<p>}</p>
</div></blockquote>
<p>}
Ok(())</p>
</dd>
</dl>
<p>}</p>
<p>#[inline(always)]
fn do_predict(</p>
<blockquote>
<div><p>&amp;self,
input_tensors: Vec&lt;Vec&lt;TensorInput&gt;&gt;,
batch_size: u64,</p>
</div></blockquote>
<dl>
<dt>) -&gt; (Vec&lt;TensorReturnEnum&gt;, Vec&lt;Vec&lt;usize&gt;&gt;) {</dt><dd><p>// let mut batch_ends = input_tensors.iter().map(<a href="#id27"><span class="problematic" id="id28">|t|</span></a> t.len()).collect::&lt;Vec&lt;usize&gt;&gt;();
let output_size = self.output_names.len() as u64;
let input_size = self.input_names.len() as u64;
debug!(</p>
<blockquote>
<div><p>“Request for Tensorflow with batch size: {} and input_size: {}”,
batch_size, input_size</p>
</div></blockquote>
<p>);
// build a set of input TF tensors</p>
<dl class="simple">
<dt>let batch_end = (1usize..=input_tensors.len() as usize)</dt><dd><p>.into_iter()
.collect_vec();</p>
</dd>
</dl>
<p>let mut batch_ends = vec![batch_end; output_size as usize];</p>
<dl class="simple">
<dt>let batched_tensors = TensorInputEnum::merge_batch(input_tensors)</dt><dd><p>.into_iter()
.enumerate()
.map(<a href="#id29"><span class="problematic" id="id30">|(_, i)|</span></a> TFModel::convert_to_tftensor_enum(i, input_size, batch_size))
.collect_vec();</p>
</dd>
</dl>
<p>let mut args = SessionRunArgs::new();
for (index, tf_tensor) in batched_tensors.iter().enumerate() {</p>
<blockquote>
<div><dl class="simple">
<dt>match tf_tensor {</dt><dd><p>TFTensorEnum::String(inner) =&gt; args.add_feed(&amp;self.input_ops[index], 0, inner),
TFTensorEnum::Int(inner) =&gt; args.add_feed(&amp;self.input_ops[index], 0, inner),
TFTensorEnum::Int64(inner) =&gt; args.add_feed(&amp;self.input_ops[index], 0, inner),
TFTensorEnum::Float(inner) =&gt; args.add_feed(&amp;self.input_ops[index], 0, inner),
TFTensorEnum::Double(inner) =&gt; args.add_feed(&amp;self.input_ops[index], 0, inner),
TFTensorEnum::Boolean(inner) =&gt; args.add_feed(&amp;self.input_ops[index], 0, inner),</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>}
// For output ops, we receive the same op object by name. Actual tensor tokens are available at different offsets.
// Since indices are ordered, its important to specify output flag to Navi in the same order.
let token_outputs = self</p>
<blockquote>
<div><p>.output_ops
.iter()
.enumerate()
.map(<a href="#id31"><span class="problematic" id="id32">|(idx, op)|</span></a> args.request_fetch(op, idx as i32))
.collect_vec();</p>
</div></blockquote>
<dl>
<dt>match self.bundle.session.run(&amp;mut args) {</dt><dd><p>Ok(_) =&gt; (),
Err(e) =&gt; {</p>
<blockquote>
<div><p>NUM_REQUESTS_FAILED.inc_by(batch_size);
NUM_REQUESTS_FAILED_BY_MODEL</p>
<blockquote>
<div><p>.with_label_values(&amp;[&amp;MODEL_SPECS[self.model_idx]])
.inc_by(batch_size);</p>
</div></blockquote>
<dl class="simple">
<dt>INFERENCE_FAILED_REQUESTS_BY_MODEL</dt><dd><p>.with_label_values(&amp;[&amp;MODEL_SPECS[self.model_idx]])
.inc_by(batch_size);</p>
</dd>
</dl>
<p>panic!(“{model}: {e:?}”, model = MODEL_SPECS[self.model_idx], e = e);</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}
let mut predict_return = vec![];
// Check the output.
for (index, token_output) in token_outputs.iter().enumerate() {</p>
<blockquote>
<div><p>// same ops, with type info at different offsets.
let (res, width) = match self.output_ops[index].output_type(index) {</p>
<blockquote>
<div><dl>
<dt>DataType::Float =&gt; {</dt><dd><dl class="simple">
<dt>let (tensor_output, tensor_width) =</dt><dd><p>TFModel::fetch_output(&amp;mut args, token_output, batch_size, output_size);</p>
</dd>
</dl>
<dl class="simple">
<dt>(</dt><dd><p>TensorReturnEnum::FloatTensorReturn(Box::new(tensor_output)),
tensor_width,</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
<p>}
DataType::Int64 =&gt; {</p>
<blockquote>
<div><dl class="simple">
<dt>let (tensor_output, tensor_width) =</dt><dd><p>TFModel::fetch_output(&amp;mut args, token_output, batch_size, output_size);</p>
</dd>
</dl>
<dl class="simple">
<dt>(</dt><dd><p>TensorReturnEnum::Int64TensorReturn(Box::new(tensor_output)),
tensor_width,</p>
</dd>
</dl>
<p>)</p>
</div></blockquote>
<p>}
DataType::Int32 =&gt; {</p>
<blockquote>
<div><dl class="simple">
<dt>let (tensor_output, tensor_width) =</dt><dd><p>TFModel::fetch_output(&amp;mut args, token_output, batch_size, output_size);</p>
</dd>
</dl>
<dl class="simple">
<dt>(</dt><dd><p>TensorReturnEnum::Int32TensorReturn(Box::new(tensor_output)),
tensor_width,</p>
</dd>
</dl>
<p>)</p>
</div></blockquote>
<p>}
DataType::String =&gt; {</p>
<blockquote>
<div><dl class="simple">
<dt>let (tensor_output, tensor_width) =</dt><dd><p>TFModel::fetch_output(&amp;mut args, token_output, batch_size, output_size);</p>
</dd>
</dl>
<dl class="simple">
<dt>(</dt><dd><p>TensorReturnEnum::StringTensorReturn(Box::new(tensor_output)),
tensor_width,</p>
</dd>
</dl>
<p>)</p>
</div></blockquote>
<p>}
_ =&gt; panic!(“Unsupported return type!”),</p>
</div></blockquote>
<p>};
let width = width as usize;
for b in batch_ends[index].iter_mut() {</p>
<blockquote>
<div><p><a href="#id7"><span class="problematic" id="id8">*</span></a>b <a href="#id9"><span class="problematic" id="id10">*</span></a>= width;</p>
</div></blockquote>
<p>}
predict_return.push(res)</p>
</div></blockquote>
<p>}
//TODO: remove in the future
//TODO: support actual mtl model outputs
(predict_return, batch_ends)</p>
</dd>
</dl>
<p>}
#[inline(always)]
fn model_idx(&amp;self) -&gt; usize {</p>
<blockquote>
<div><p>self.model_idx</p>
</div></blockquote>
<p>}
#[inline(always)]
fn version(&amp;self) -&gt; i64 {</p>
<blockquote>
<div><p>self.version</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>}</p>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../index2.rst.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index2.rst.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../_sources/navi/navi/src/tf_model.rs.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>