<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../../../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../../../../../" id="documentation_options" src="../../../../../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>package com.twitter.recos.hose.common</p>
<p>import com.twitter.finagle.stats.StatsReceiver
import com.twitter.finatra.kafka.consumers.FinagleKafkaConsumerBuilder
import com.twitter.graphjet.bipartite.LeftIndexedMultiSegmentBipartiteGraph
import com.twitter.graphjet.bipartite.segment.LeftIndexedBipartiteGraphSegment
import com.twitter.kafka.client.processor.{AtLeastOnceProcessor, ThreadSafeKafkaConsumerClient}
import com.twitter.logging.Logger
import com.twitter.recos.internal.thriftscala.RecosHoseMessage
import java.util.concurrent.atomic.AtomicBoolean
import java.util.concurrent.{ConcurrentLinkedQueue, ExecutorService, Executors, Semaphore}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>The class submits a number of graph writer threads, BufferedEdgeWriter,</p></li>
<li><p>during service startup. One of them is live writer thread, and the other $(numBootstrapWriters - 1)</p></li>
<li><p>are catchup writer threads. All of them consume kafka events from an internal concurrent queue,</p></li>
<li><p>which is populated by kafka reader threads. At bootstrap time, the kafka reader threads look</p></li>
<li><p>back kafka offset from several hours ago and populate the internal concurrent queue.</p></li>
<li><p>Each graph writer thread writes to an individual graph segment separately.</p></li>
<li><p>The (numBootstrapWriters - 1) catchup writer threads will stop once all events</p></li>
<li><p>between current system time at startup and the time in memcache are processed.</p></li>
<li><p>The live writer thread will continue to write all incoming kafka events.</p></li>
<li><p>It lives through the entire life cycle of recos graph service.</p></li>
</ul>
<p><a href="#id1"><span class="problematic" id="id2">*</span></a>/</p>
</dd>
<dt>trait UnifiedGraphWriter[</dt><dd><p>TSegment &lt;: LeftIndexedBipartiteGraphSegment,
TGraph &lt;: LeftIndexedMultiSegmentBipartiteGraph[TSegment]] { writer =&gt;</p>
<p>import UnifiedGraphWriter._</p>
<p>def shardId: String
def env: String
def hosename: String
def bufferSize: Int
def consumerNum: Int
def catchupWriterNum: Int
def kafkaConsumerBuilder: FinagleKafkaConsumerBuilder[String, RecosHoseMessage]
def clientId: String
def statsReceiver: StatsReceiver</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Adds a RecosHoseMessage to the graph. used by live writer to insert edges to the</p></li>
<li><p>current segment</p></li>
</ul>
<p><a href="#id3"><span class="problematic" id="id4">*</span></a>/</p>
</dd>
</dl>
<p>def addEdgeToGraph(graph: TGraph, recosHoseMessage: RecosHoseMessage): Unit</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Adds a RecosHoseMessage to the given segment in the graph. Used by catch up writers to</p></li>
<li><p>insert edges to non-current (old) segments</p></li>
</ul>
<p><a href="#id5"><span class="problematic" id="id6">*</span></a>/</p>
</dd>
</dl>
<p>def addEdgeToSegment(segment: TSegment, recosHoseMessage: RecosHoseMessage): Unit</p>
<p>private val log = Logger()
private val isRunning: AtomicBoolean = new AtomicBoolean(true)
private val initialized: AtomicBoolean = new AtomicBoolean(false)
private var processors: Seq[AtLeastOnceProcessor[String, RecosHoseMessage]] = Seq.empty
private var consumers: Seq[ThreadSafeKafkaConsumerClient[String, RecosHoseMessage]] = Seq.empty
private val threadPool: ExecutorService = Executors.newCachedThreadPool()</p>
<dl>
<dt>def shutdown(): Unit = {</dt><dd><dl class="simple">
<dt>processors.foreach { processor =&gt;</dt><dd><p>processor.close()</p>
</dd>
</dl>
<p>}
processors = Seq.empty
consumers.foreach { consumer =&gt;</p>
<blockquote>
<div><p>consumer.close()</p>
</div></blockquote>
<p>}
consumers = Seq.empty
threadPool.shutdown()
isRunning.set(false)</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>def initHose(liveGraph: TGraph): Unit = this.synchronized {</dt><dd><dl>
<dt>if (!initialized.get) {</dt><dd><p>initialized.set(true)</p>
<dl class="simple">
<dt>val queue: java.util.Queue[Array[RecosHoseMessage]] =</dt><dd><p>new ConcurrentLinkedQueue[Array[RecosHoseMessage]]()</p>
</dd>
</dl>
<p>val queuelimit: Semaphore = new Semaphore(1024)</p>
<p>initRecosHoseKafka(queue, queuelimit)
initGrpahWriters(liveGraph, queue, queuelimit)</p>
</dd>
<dt>} else {</dt><dd><p>throw new RuntimeException(“attempt to re-init kafka hose”)</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>private def initRecosHoseKafka(</dt><dd><p>queue: java.util.Queue[Array[RecosHoseMessage]],
queuelimit: Semaphore,</p>
</dd>
<dt>): Unit = {</dt><dd><dl>
<dt>try {</dt><dd><dl class="simple">
<dt>consumers = (0 until consumerNum).map { index =&gt;</dt><dd><dl class="simple">
<dt>new ThreadSafeKafkaConsumerClient(</dt><dd><p>kafkaConsumerBuilder.clientId(s”clientId-$index”).enableAutoCommit(false).config)</p>
</dd>
</dl>
</dd>
</dl>
<p>}
processors = consumers.zipWithIndex.map {</p>
<blockquote>
<div><dl>
<dt>case (consumer, index) =&gt;</dt><dd><p>val bufferedWriter = BufferedEdgeCollector(bufferSize, queue, queuelimit, statsReceiver)
val processor = RecosEdgeProcessor(bufferedWriter)(statsReceiver)</p>
<dl class="simple">
<dt>AtLeastOnceProcessor[String, RecosHoseMessage](</dt><dd><p>s”recos-injector-kafka-$index”,
hosename,
consumer,
processor.process,
maxPendingRequests = MaxPendingRequests * bufferSize,
workerThreads = ProcessorThreads,
commitIntervalMs = CommitIntervalMs,
statsReceiver = statsReceiver</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
</div></blockquote>
<p>}</p>
<p>log.info(s”starting ${processors.size} recosKafka processors”)
processors.foreach { processor =&gt;</p>
<blockquote>
<div><p>processor.start()</p>
</div></blockquote>
<p>}</p>
</dd>
<dt>} catch {</dt><dd><dl>
<dt>case e: Throwable =&gt;</dt><dd><p>e.printStackTrace()
log.error(e, e.toString)
processors.foreach { processor =&gt;</p>
<blockquote>
<div><p>processor.close()</p>
</div></blockquote>
<p>}
processors = Seq.empty
consumers.foreach { consumer =&gt;</p>
<blockquote>
<div><p>consumer.close()</p>
</div></blockquote>
<p>}
consumers = Seq.empty</p>
</dd>
</dl>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Initialize the graph writers,</p></li>
<li><p>by first creating catch up writers to bootstrap the older segments,</p></li>
<li><p>and then assigning a live writer to populate the live segment.</p></li>
</ul>
<p><a href="#id7"><span class="problematic" id="id8">*</span></a>/</p>
</dd>
<dt>private def initGrpahWriters(</dt><dd><p>liveGraph: TGraph,
queue: java.util.Queue[Array[RecosHoseMessage]],
queuelimit: Semaphore</p>
</dd>
<dt>): Unit = {</dt><dd><p>// define a number of (numBootstrapWriters - 1) catchup writer threads, each of which will write
// to a separate graph segment.
val catchupWriters = (0 until (catchupWriterNum - 1)).map { index =&gt;</p>
<blockquote>
<div><p>val segment = liveGraph.getLiveSegment
liveGraph.rollForwardSegment()
getCatchupWriter(segment, queue, queuelimit, index)</p>
</div></blockquote>
<p>}
val threadPool: ExecutorService = Executors.newCachedThreadPool()</p>
<p>// define one live writer thread
val liveWriter = getLiveWriter(liveGraph, queue, queuelimit)
log.info(“starting live graph writer that runs until service shutdown”)
threadPool.submit(liveWriter)
log.info(</p>
<blockquote>
<div><p>“starting catchup graph writer, which will terminate as soon as the catchup segment is full”</p>
</div></blockquote>
<p>)
catchupWriters.map(threadPool.submit(_))</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>private def getLiveWriter(</dt><dd><p>liveGraph: TGraph,
queue: java.util.Queue[Array[RecosHoseMessage]],
queuelimit: Semaphore</p>
</dd>
<dt>): BufferedEdgeWriter = {</dt><dd><dl class="simple">
<dt>val liveEdgeCollector = new EdgeCollector {</dt><dd><p>override def addEdge(message: RecosHoseMessage): Unit = addEdgeToGraph(liveGraph, message)</p>
</dd>
</dl>
<p>}
BufferedEdgeWriter(</p>
<blockquote>
<div><p>queue,
queuelimit,
liveEdgeCollector,
statsReceiver.scope(“liveWriter”),
isRunning.get</p>
</div></blockquote>
<p>)</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>private def getCatchupWriter(</dt><dd><p>segment: TSegment,
queue: java.util.Queue[Array[RecosHoseMessage]],
queuelimit: Semaphore,
catchupWriterIndex: Int</p>
</dd>
<dt>): BufferedEdgeWriter = {</dt><dd><dl>
<dt>val catchupEdgeCollector = new EdgeCollector {</dt><dd><p>var currentNumEdges = 0</p>
<dl class="simple">
<dt>override def addEdge(message: RecosHoseMessage): Unit = {</dt><dd><p>currentNumEdges += 1
addEdgeToSegment(segment, message)</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}
val maxEdges = segment.getMaxNumEdges</p>
<dl class="simple">
<dt>def runCondition(): Boolean = {</dt><dd><p>isRunning.get &amp;&amp; ((maxEdges - catchupEdgeCollector.currentNumEdges) &gt; bufferSize)</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>BufferedEdgeWriter(</dt><dd><p>queue,
queuelimit,
catchupEdgeCollector,
statsReceiver.scope(”<a href="#id9"><span class="problematic" id="id10">catcher_</span></a>” + catchupWriterIndex),
runCondition</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<p>private object UnifiedGraphWriter {</p>
<blockquote>
<div><p>// The RecosEdgeProcessor is not thread-safe. Only use one thread to process each instance.
val ProcessorThreads = 1
// Each one cache at most 1000 * bufferSize requests.
val MaxPendingRequests = 1000
// Short Commit MS to reduce duplicate messages.
val CommitIntervalMs: Long = 5000 // 5 seconds, Default Kafka value.</p>
</div></blockquote>
<p>}</p>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../../../../index2.rst.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../../../../index2.rst.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../../../../../_sources/src/scala/com/twitter/recos/hose/common/UnifiedGraphWriter.scala.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>