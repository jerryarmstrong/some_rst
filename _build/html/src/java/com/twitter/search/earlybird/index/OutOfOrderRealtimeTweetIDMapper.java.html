<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../../../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../../../../../" id="documentation_options" src="../../../../../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>package com.twitter.search.earlybird.index;</p>
<p>import java.io.IOException;
import java.util.Arrays;</p>
<p>import com.google.common.annotations.VisibleForTesting;
import com.google.common.base.Preconditions;</p>
<p>import org.slf4j.Logger;
import org.slf4j.LoggerFactory;</p>
<p>import com.twitter.search.common.metrics.SearchRateCounter;
import com.twitter.search.common.partitioning.snowflakeparser.SnowflakeIdParser;
import com.twitter.search.common.util.io.flushable.DataDeserializer;
import com.twitter.search.common.util.io.flushable.DataSerializer;
import com.twitter.search.common.util.io.flushable.FlushInfo;
import com.twitter.search.common.util.io.flushable.Flushable;
import com.twitter.search.core.earlybird.index.DocIDToTweetIDMapper;</p>
<p>import it.unimi.dsi.fastutil.ints.Int2ByteOpenHashMap;
import it.unimi.dsi.fastutil.ints.Int2LongMap;
import it.unimi.dsi.fastutil.ints.Int2LongOpenHashMap;</p>
<dl>
<dt>/**</dt><dd><ul>
<li><p>A mapper that maps tweet IDs to doc IDs based on the tweet timestamps. This mapper guarantees</p></li>
<li><p>that if creationTime(A) &gt; creationTime(B), then docId(A) &lt; docId(B), no matter in which order</p></li>
<li><p>the tweets are added to this mapper. However, if creationTime(A) == creationTime(B), then there</p></li>
<li><p>is no guarantee on the order between docId(A) and docId(B).</p></li>
<li></li>
<li><p>Essentially, this mapper guarantees that tweets with a later creation time are mapped to smaller</p></li>
<li><p>doc IDs, but it does not provide any ordering for tweets with the same timestamp (down to</p></li>
<li><p>millisecond granularity, which is what Snowflake provides). Our claim is that ordering tweets</p></li>
<li><p>with the same timestamp is not needed, because for the purposes of realtime search, the only</p></li>
<li><p>significant part of the tweet ID is the timestamp. So any such ordering would just be an ordering</p></li>
<li><p>for the Snowflake shards and/or sequence numbers, rather than a time based ordering for tweets.</p></li>
<li></li>
<li><p>The mapper uses the following scheme to assign docIDs to tweets:</p></li>
<li><table class="docutils align-default">
<tbody>
</tbody>
</table>
</li>
<li><div class="line-block">
<div class="line">Bit 0    | Bits 1 - 27                 | Bits 28 - 31                 |</div>
</div>
</li>
<li><ul class="simple">
<li><p>———+—————————–+——————————+</p></li>
</ul>
</li>
<li><div class="line-block">
<div class="line">sign     | tweet ID timestamp -        | Allow 16 tweets to be posted |</div>
</div>
</li>
<li><div class="line-block">
<div class="line">always 0 | segment boundary timestamp  | on the same millisecond      |</div>
</div>
</li>
<li><ul class="simple">
<li><p>———+—————————–+——————————+</p></li>
</ul>
</li>
<li></li>
<li><p>Important assumptions:</p></li>
<li><ul class="simple">
<li><p>Snowflake IDs have millisecond granularity. Therefore, 27 bits is enough to represent a time</p></li>
</ul>
</li>
<li><p>period of 2^27 / (3600 * 100) = ~37 hours, which is more than enough to cover one realtime</p></li>
<li><p>segment (our realtime segments currently span ~13 hours).</p></li>
<li><ul class="simple">
<li><p>At peak times, the tweet posting rate is less than 10,000 tps. Given our current partitioning</p></li>
</ul>
</li>
<li><p>scheme (22 partitions), each realtime earlybird should expect to get less than 500 tweets per</p></li>
<li><p>second, which comes down to less than 1 tweet per millisecond, assuming the partitioning hash</p></li>
<li><p>function distributes the tweets fairly randomly independent of their timestamps. Therefore,</p></li>
<li><p>providing space for 16 tweets (4 bits) in every millisecond should be more than enough to</p></li>
<li><p>accommodate the current requirements, and any potential future changes (higher tweet rate,</p></li>
<li><p>fewer partitions, etc.).</p></li>
<li></li>
<li><p>How the mapper works:</p></li>
<li><ul class="simple">
<li><p>The tweetId -&gt; docId conversion is implicit (using the tweet’s timestamp).</p></li>
</ul>
</li>
<li><ul class="simple">
<li><p>We use a IntToByteMap to store the number of tweets for each timestamp, so that we can</p></li>
</ul>
</li>
<li><p>allocate different doc IDs to tweets posted on the same millisecond. The size of this map is:</p></li>
<li><p>segmentSize * 2 (load factor) * 1 (size of byte) = 16MB</p></li>
<li><ul class="simple">
<li><p>The docId -&gt; tweetId mappings are stored in an IntToLongMap. The size of this map is:</p></li>
</ul>
</li>
<li><p>segmentSize * 2 (load factor) * 8 (size of long) = 128MB</p></li>
<li><ul class="simple">
<li><p>The mapper takes the “segment boundary” (the timestamp of the timeslice ID) as a parameter.</p></li>
</ul>
</li>
<li><p>This segment boundary determines the earliest tweet that this mapper can correctly index</p></li>
<li><p>(it is subtracted from the timestamp of all tweets added to the mapper). Therefore, in order</p></li>
<li><p>to correctly handle late tweets, we move back this segment boundary by twelve hour.</p></li>
<li><ul class="simple">
<li><p>Tweets created before (segment boundary - 12 hours) are stored as if their timestamp was the</p></li>
</ul>
</li>
<li><p>segment boundary.</p></li>
<li><ul class="simple">
<li><p>The largest timestamp that the mapper can store is:</p></li>
</ul>
</li>
<li><p>LARGEST_RELATIVE_TIMESTAMP = (1 &lt;&lt; TIMESTAMP_BITS) - LUCENE_TIMESTAMP_BUFFER.</p></li>
<li><p>Tweets created after (segmentBoundaryTimestamp + LARGEST_RELATIVE_TIMESTAMP) are stored as if</p></li>
<li><p>their timestamp was (segmentBoundaryTimestamp + LARGEST_RELATIVE_TIMESTAMP).</p></li>
<li><ul class="simple">
<li><p>When a tweet is added, we compute its doc ID as:</p></li>
</ul>
</li>
<li><p>int relativeTimestamp = tweetTimestamp - segmentBoundaryTimestamp;</p></li>
<li><p>int docIdTimestamp = LARGEST_RELATIVE_TIMESTAMP - relativeTimestamp;</p></li>
<li><p>int numTweetsForTimestamp = tweetsPerTimestamp.get(docIdTimestamp);</p></li>
<li><p>int docId = (docIdTimestamp &lt;&lt; DOC_ID_BITS)</p></li>
<li><ul class="simple">
<li><p>MAX_DOCS_PER_TIMESTAMP - numTweetsForTimestamp - 1</p></li>
</ul>
</li>
<li></li>
<li><p>This doc ID distribution scheme guarantees that tweets created later will be assigned smaller doc</p></li>
<li><p>IDs (as long as we don’t have more than 16 tweets created in the same millisecond). However,</p></li>
<li><p>there is no ordering guarantee for tweets created at the same timestamp – they are assigned doc</p></li>
<li><p>IDs in the order in which they’re added to the mapper.</p></li>
<li></li>
<li><p>If we have more than 16 tweets created at time T, the mapper will still gracefully handle that</p></li>
<li><p>case: the “extra” tweets will be assigned doc IDs from the pool of doc IDs for timestamp (T + 1).</p></li>
<li><p>However, the ordering guarantee might no longer hold for those “extra” tweets. Also, the “extra”</p></li>
<li><p>tweets might be missed by certain since_id/max_id queries (the findDocIdBound() method might not</p></li>
<li><p>be able to correctly work for these tweet IDs).</p></li>
</ul>
<p><a href="#id1"><span class="problematic" id="id2">*</span></a>/</p>
</dd>
<dt>public class OutOfOrderRealtimeTweetIDMapper extends TweetIDMapper {</dt><dd><p>private static final Logger LOG = LoggerFactory.getLogger(OutOfOrderRealtimeTweetIDMapper.class);</p>
<p>// The number of bits used to represent the tweet timestamp.
private static final int TIMESTAMP_BITS = 27;</p>
<p>// The number of bits used to represent the number of tweets with a certain timestamp.
&#64;VisibleForTesting
static final int DOC_ID_BITS = Integer.SIZE - TIMESTAMP_BITS - 1;</p>
<p>// The maximum number of tweets/docs that we can store per timestamp.
&#64;VisibleForTesting
static final int MAX_DOCS_PER_TIMESTAMP = 1 &lt;&lt; DOC_ID_BITS;</p>
<p>// Lucene has some logic that doesn’t deal well with doc IDs close to Integer.MAX_VALUE.
// For example, BooleanScorer has a SIZE constant set to 2048, which gets added to the doc IDs
// inside the score() method. So when the doc IDs are close to Integer.MAX_VALUE, this causes an
// overflow, which can send Lucene into an infinite loop. Therefore, we need to make sure that
// we do not assign doc IDs close to Integer.MAX_VALUE.
private static final int LUCENE_TIMESTAMP_BUFFER = 1 &lt;&lt; 16;</p>
<p>&#64;VisibleForTesting
public static final int LATE_TWEETS_TIME_BUFFER_MILLIS = 12 * 3600 * 1000;  // 12 hours</p>
<p>// The largest relative timestamp that this mapper can store.
&#64;VisibleForTesting
static final int LARGEST_RELATIVE_TIMESTAMP = (1 &lt;&lt; TIMESTAMP_BITS) - LUCENE_TIMESTAMP_BUFFER;</p>
<p>private final long segmentBoundaryTimestamp;
private final int segmentSize;</p>
<p>private final Int2LongOpenHashMap tweetIds;
private final Int2ByteOpenHashMap tweetsPerTimestamp;</p>
<dl>
<dt>private static final SearchRateCounter BAD_BUCKET_RATE =</dt><dd><p>SearchRateCounter.export(“tweets_assigned_to_bad_timestamp_bucket”);</p>
</dd>
<dt>private static final SearchRateCounter TWEETS_NOT_ASSIGNED_RATE =</dt><dd><p>SearchRateCounter.export(“tweets_not_assigned”);</p>
</dd>
<dt>private static final SearchRateCounter OLD_TWEETS_DROPPED =</dt><dd><p>SearchRateCounter.export(“old_tweets_dropped”);</p>
</dd>
<dt>public OutOfOrderRealtimeTweetIDMapper(int segmentSize, long timesliceID) {</dt><dd><p>long firstTimestamp = SnowflakeIdParser.getTimestampFromTweetId(timesliceID);
// Leave a buffer so that we can handle tweets that are up to twelve hours late.
this.segmentBoundaryTimestamp = firstTimestamp - LATE_TWEETS_TIME_BUFFER_MILLIS;
this.segmentSize = segmentSize;</p>
<p>tweetIds = new Int2LongOpenHashMap(segmentSize);
tweetIds.defaultReturnValue(ID_NOT_FOUND);</p>
<p>tweetsPerTimestamp = new Int2ByteOpenHashMap(segmentSize);
tweetsPerTimestamp.defaultReturnValue((byte) ID_NOT_FOUND);</p>
</dd>
</dl>
<p>}</p>
<p>&#64;VisibleForTesting
int getDocIdTimestamp(long tweetId) {</p>
<blockquote>
<div><p>long tweetTimestamp = SnowflakeIdParser.getTimestampFromTweetId(tweetId);
if (tweetTimestamp &lt; segmentBoundaryTimestamp) {</p>
<blockquote>
<div><p>return ID_NOT_FOUND;</p>
</div></blockquote>
<p>}</p>
<p>long relativeTimestamp = tweetTimestamp - segmentBoundaryTimestamp;
if (relativeTimestamp &gt; LARGEST_RELATIVE_TIMESTAMP) {</p>
<blockquote>
<div><p>relativeTimestamp = LARGEST_RELATIVE_TIMESTAMP;</p>
</div></blockquote>
<p>}</p>
<p>return LARGEST_RELATIVE_TIMESTAMP - (int) relativeTimestamp;</p>
</div></blockquote>
<p>}</p>
<dl class="simple">
<dt>private int getDocIdForTimestamp(int docIdTimestamp, byte docIndexInTimestamp) {</dt><dd><p>return (docIdTimestamp &lt;&lt; DOC_ID_BITS) + MAX_DOCS_PER_TIMESTAMP - docIndexInTimestamp;</p>
</dd>
</dl>
<p>}</p>
<p>&#64;VisibleForTesting
long[] getTweetsForDocIdTimestamp(int docIdTimestamp) {</p>
<blockquote>
<div><p>byte numDocsForTimestamp = tweetsPerTimestamp.get(docIdTimestamp);
if (numDocsForTimestamp == ID_NOT_FOUND) {</p>
<blockquote>
<div><p>// This should never happen in prod, but better to be safe.
return new long[0];</p>
</div></blockquote>
<p>}</p>
<p>long[] tweetIdsInBucket = new long[numDocsForTimestamp];
int startingDocId = (docIdTimestamp &lt;&lt; DOC_ID_BITS) + MAX_DOCS_PER_TIMESTAMP - 1;
for (int i = 0; i &lt; numDocsForTimestamp; ++i) {</p>
<blockquote>
<div><p>tweetIdsInBucket[i] = tweetIds.get(startingDocId - i);</p>
</div></blockquote>
<p>}
return tweetIdsInBucket;</p>
</div></blockquote>
<p>}</p>
<dl>
<dt>private int newDocId(long tweetId) {</dt><dd><p>int expectedDocIdTimestamp = getDocIdTimestamp(tweetId);
if (expectedDocIdTimestamp == ID_NOT_FOUND) {</p>
<blockquote>
<div><dl class="simple">
<dt>LOG.info(“Dropping tweet {} because it is from before the segment boundary timestamp {}”,</dt><dd><p>tweetId,
segmentBoundaryTimestamp);</p>
</dd>
</dl>
<p>OLD_TWEETS_DROPPED.increment();
return ID_NOT_FOUND;</p>
</div></blockquote>
<p>}</p>
<p>int docIdTimestamp = expectedDocIdTimestamp;
byte numDocsForTimestamp = tweetsPerTimestamp.get(docIdTimestamp);</p>
<dl class="simple">
<dt>if (numDocsForTimestamp == MAX_DOCS_PER_TIMESTAMP) {</dt><dd><p>BAD_BUCKET_RATE.increment();</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>while ((docIdTimestamp &gt; 0) &amp;&amp; (numDocsForTimestamp == MAX_DOCS_PER_TIMESTAMP)) {</dt><dd><p>–docIdTimestamp;
numDocsForTimestamp = tweetsPerTimestamp.get(docIdTimestamp);</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>if (numDocsForTimestamp == MAX_DOCS_PER_TIMESTAMP) {</dt><dd><p>// The relative timestamp 0 already has MAX_DOCS_PER_TIMESTAMP. Can’t add more docs.
LOG.error(“Tweet {} could not be assigned a doc ID in any bucket, because the bucket for “</p>
<blockquote>
<div><ul class="simple">
<li><p>“timestamp 0 is already full: {}”,</p></li>
</ul>
<p>tweetId, Arrays.toString(getTweetsForDocIdTimestamp(0)));</p>
</div></blockquote>
<p>TWEETS_NOT_ASSIGNED_RATE.increment();
return ID_NOT_FOUND;</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>if (docIdTimestamp != expectedDocIdTimestamp) {</dt><dd><dl>
<dt>LOG.warn(“Tweet {} could not be assigned a doc ID in the bucket for its timestamp {}, “</dt><dd><ul class="simple">
<li><p>“because this bucket is full. Instead, it was assigned a doc ID in the bucket for “</p></li>
<li><p>“timestamp {}. The tweets in the correct bucket are: {}”,</p></li>
</ul>
<p>tweetId,
expectedDocIdTimestamp,
docIdTimestamp,
Arrays.toString(getTweetsForDocIdTimestamp(expectedDocIdTimestamp)));</p>
</dd>
</dl>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>if (numDocsForTimestamp == ID_NOT_FOUND) {</dt><dd><p>numDocsForTimestamp = 0;</p>
</dd>
</dl>
<p>}
++numDocsForTimestamp;
tweetsPerTimestamp.put(docIdTimestamp, numDocsForTimestamp);</p>
<p>return getDocIdForTimestamp(docIdTimestamp, numDocsForTimestamp);</p>
</dd>
</dl>
<p>}</p>
<p>&#64;Override
public int getDocID(long tweetId) {</p>
<blockquote>
<div><p>int docIdTimestamp = getDocIdTimestamp(tweetId);
while (docIdTimestamp &gt;= 0) {</p>
<blockquote>
<div><p>int numDocsForTimestamp = tweetsPerTimestamp.get(docIdTimestamp);
int startingDocId = (docIdTimestamp &lt;&lt; DOC_ID_BITS) + MAX_DOCS_PER_TIMESTAMP - 1;
for (int docId = startingDocId; docId &gt; startingDocId - numDocsForTimestamp; –docId) {</p>
<blockquote>
<div><dl class="simple">
<dt>if (tweetIds.get(docId) == tweetId) {</dt><dd><p>return docId;</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>}</p>
<p>// If we have MAX_DOCS_PER_TIMESTAMP docs with this timestamp, then we might’ve mis-assigned
// a tweet to the previous docIdTimestamp bucket. In that case, we need to keep searching.
// Otherwise, the tweet is not in the index.
if (numDocsForTimestamp &lt; MAX_DOCS_PER_TIMESTAMP) {</p>
<blockquote>
<div><p>break;</p>
</div></blockquote>
<p>}</p>
<p class="attribution">—docIdTimestamp;</p>
</div></blockquote>
<p>}</p>
<p>return ID_NOT_FOUND;</p>
</div></blockquote>
<p>}</p>
<p>&#64;Override
protected int getNextDocIDInternal(int docId) {</p>
<blockquote>
<div><p>// Check if docId + 1 is an assigned doc ID in this mapper. This might be the case when we have
// multiple tweets posted on the same millisecond.
if (tweetIds.get(docId + 1) != ID_NOT_FOUND) {</p>
<blockquote>
<div><p>return docId + 1;</p>
</div></blockquote>
<p>}</p>
<p>// If (docId + 1) is not assigned, then it means we do not have any more tweets posted at the
// timestamp corresponding to docId. We need to find the next relative timestamp for which this
// mapper has tweets, and return the first tweet for that timestamp. Note that iterating over
// the space of all possible timestamps is faster than iterating over the space of all possible
// doc IDs (it’s MAX_DOCS_PER_TIMESTAMP times faster).
int nextDocIdTimestamp = (docId &gt;&gt; DOC_ID_BITS) + 1;
byte numDocsForTimestamp = tweetsPerTimestamp.get(nextDocIdTimestamp);
int maxDocIdTimestamp = getMaxDocID() &gt;&gt; DOC_ID_BITS;
while ((nextDocIdTimestamp &lt;= maxDocIdTimestamp)</p>
<blockquote>
<div><blockquote>
<div><p>&amp;&amp; (numDocsForTimestamp == ID_NOT_FOUND)) {</p>
</div></blockquote>
<p>++nextDocIdTimestamp;
numDocsForTimestamp = tweetsPerTimestamp.get(nextDocIdTimestamp);</p>
</div></blockquote>
<p>}</p>
<dl class="simple">
<dt>if (numDocsForTimestamp != ID_NOT_FOUND) {</dt><dd><p>return getDocIdForTimestamp(nextDocIdTimestamp, numDocsForTimestamp);</p>
</dd>
</dl>
<p>}</p>
<p>return ID_NOT_FOUND;</p>
</div></blockquote>
<p>}</p>
<p>&#64;Override
protected int getPreviousDocIDInternal(int docId) {</p>
<blockquote>
<div><p>// Check if docId - 1 is an assigned doc ID in this mapper. This might be the case when we have
// multiple tweets posted on the same millisecond.
if (tweetIds.get(docId - 1) != ID_NOT_FOUND) {</p>
<blockquote>
<div><p>return docId - 1;</p>
</div></blockquote>
<p>}</p>
<p>// If (docId - 1) is not assigned, then it means we do not have any more tweets posted at the
// timestamp corresponding to docId. We need to find the previous relative timestamp for which
// this mapper has tweets, and return the first tweet for that timestamp. Note that iterating
// over the space of all possible timestamps is faster than iterating over the space of all
// possible doc IDs (it’s MAX_DOCS_PER_TIMESTAMP times faster).
int previousDocIdTimestamp = (docId &gt;&gt; DOC_ID_BITS) - 1;
byte numDocsForTimestamp = tweetsPerTimestamp.get(previousDocIdTimestamp);
int minDocIdTimestamp = getMinDocID() &gt;&gt; DOC_ID_BITS;
while ((previousDocIdTimestamp &gt;= minDocIdTimestamp)</p>
<blockquote>
<div><blockquote>
<div><p>&amp;&amp; (numDocsForTimestamp == ID_NOT_FOUND)) {</p>
</div></blockquote>
<p>–previousDocIdTimestamp;
numDocsForTimestamp = tweetsPerTimestamp.get(previousDocIdTimestamp);</p>
</div></blockquote>
<p>}</p>
<dl class="simple">
<dt>if (numDocsForTimestamp != ID_NOT_FOUND) {</dt><dd><p>return getDocIdForTimestamp(previousDocIdTimestamp, (byte) 1);</p>
</dd>
</dl>
<p>}</p>
<p>return ID_NOT_FOUND;</p>
</div></blockquote>
<p>}</p>
<p>&#64;Override
public long getTweetID(int docId) {</p>
<blockquote>
<div><p>return tweetIds.get(docId);</p>
</div></blockquote>
<p>}</p>
<p>&#64;Override
protected int addMappingInternal(long tweetId) {</p>
<blockquote>
<div><p>int docId = newDocId(tweetId);
if (docId == ID_NOT_FOUND) {</p>
<blockquote>
<div><p>return ID_NOT_FOUND;</p>
</div></blockquote>
<p>}</p>
<p>tweetIds.put(docId, tweetId);
return docId;</p>
</div></blockquote>
<p>}</p>
<p>&#64;Override
protected int findDocIDBoundInternal(long tweetId, boolean findMaxDocId) {</p>
<blockquote>
<div><p>// Note that it would be incorrect to lookup the doc ID for the given tweet ID and return that
// doc ID, as we would skip over tweets created in the same millisecond but with a lower doc ID.
int docIdTimestamp = getDocIdTimestamp(tweetId);</p>
<p>// The docIdTimestamp is ID_NOT_FOUND only if the tweet is from before the segment boundary and
// this should never happen here because TweetIDMapper.findDocIdBound ensures that the tweet id
// passed into this method is &gt;= minTweetID which means the tweet is from after the segment
// boundary.
Preconditions.checkState(</p>
<blockquote>
<div><p>docIdTimestamp != ID_NOT_FOUND,
“Tried to find doc id bound for tweet %d which is from before the segment boundary %d”,
tweetId,
segmentBoundaryTimestamp);</p>
</div></blockquote>
<p>// It’s OK to return a doc ID that doesn’t correspond to any tweet ID in the index,
// as the doc ID is simply used as a starting point and ending point for range queries,
// not a source of truth.
if (findMaxDocId) {</p>
<blockquote>
<div><p>// Return the largest possible doc ID for the timestamp.
return getDocIdForTimestamp(docIdTimestamp, (byte) 1);</p>
</div></blockquote>
<dl class="simple">
<dt>} else {</dt><dd><p>// Return the smallest possible doc ID for the timestamp.
byte tweetsInTimestamp = tweetsPerTimestamp.getOrDefault(docIdTimestamp, (byte) 0);
return getDocIdForTimestamp(docIdTimestamp, tweetsInTimestamp);</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Returns the array of all tweet IDs stored in this mapper in a sorted (descending) order.</p></li>
<li><p>Essentially, this method remaps all tweet IDs stored in this mapper to a compressed doc ID</p></li>
<li><p>space of [0, numDocs).</p></li>
<li></li>
<li><p>Note that this method is not thread safe, and it’s meant to be called only at segment</p></li>
<li><p>optimization time. If addMappingInternal() is called during the execution of this method,</p></li>
<li><p>the behavior is undefined (it will most likely return bad results or throw an exception).</p></li>
<li></li>
<li><p>&#64;return An array of all tweet IDs stored in this mapper, in a sorted (descending) order.</p></li>
</ul>
<p><a href="#id3"><span class="problematic" id="id4">*</span></a>/</p>
</dd>
<dt>public long[] sortTweetIds() {</dt><dd><p>int numDocs = getNumDocs();
if (numDocs == 0) {</p>
<blockquote>
<div><p>return new long[0];</p>
</div></blockquote>
<p>}</p>
<p>// Add all tweets stored in this mapper to sortTweetIds.
long[] sortedTweetIds = new long[numDocs];
int sortedTweetIdsIndex = 0;
for (int docId = getMinDocID(); docId != ID_NOT_FOUND; docId = getNextDocID(docId)) {</p>
<blockquote>
<div><p>sortedTweetIds[sortedTweetIdsIndex++] = getTweetID(docId);</p>
</div></blockquote>
<p>}
Preconditions.checkState(sortedTweetIdsIndex == numDocs,</p>
<blockquote>
<div><p>“Could not traverse all documents in the mapper. Expected to find ”
+ numDocs + “ docs, but found only “ + sortedTweetIdsIndex);</p>
</div></blockquote>
<p>// Sort sortedTweetIdsIndex in descending order. There’s no way to sort a primitive array in
// descending order, so we have to sort it in ascending order and then reverse it.
Arrays.sort(sortedTweetIds);
for (int i = 0; i &lt; numDocs / 2; ++i) {</p>
<blockquote>
<div><p>long tmp = sortedTweetIds[i];
sortedTweetIds[i] = sortedTweetIds[numDocs - 1 - i];
sortedTweetIds[numDocs - 1 - i] = tmp;</p>
</div></blockquote>
<p>}</p>
<p>return sortedTweetIds;</p>
</dd>
</dl>
<p>}</p>
<p>&#64;Override
public DocIDToTweetIDMapper optimize() throws IOException {</p>
<blockquote>
<div><p>return new OptimizedTweetIDMapper(this);</p>
</div></blockquote>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Returns the largest Tweet ID that this doc ID mapper could handle. The returned Tweet ID</p></li>
<li><p>would be safe to put into the mapper, but any larger ones would not be correctly handled.</p></li>
</ul>
<p><a href="#id5"><span class="problematic" id="id6">*</span></a>/</p>
</dd>
<dt>public static long calculateMaxTweetID(long timesliceID) {</dt><dd><p>long numberOfUsableTimestamps = LARGEST_RELATIVE_TIMESTAMP - LATE_TWEETS_TIME_BUFFER_MILLIS;
long firstTimestamp = SnowflakeIdParser.getTimestampFromTweetId(timesliceID);
long lastTimestamp = firstTimestamp + numberOfUsableTimestamps;
return SnowflakeIdParser.generateValidStatusId(</p>
<blockquote>
<div><p>lastTimestamp, SnowflakeIdParser.RESERVED_BITS_MASK);</p>
</div></blockquote>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Evaluates whether two instances of OutOfOrderRealtimeTweetIDMapper are equal by value. It is</p></li>
<li><p>slow because it has to check every tweet ID/doc ID in the map.</p></li>
</ul>
<p><a href="#id7"><span class="problematic" id="id8">*</span></a>/</p>
</dd>
</dl>
<p>&#64;VisibleForTesting
boolean verySlowEqualsForTests(OutOfOrderRealtimeTweetIDMapper that) {</p>
<blockquote>
<div><dl class="simple">
<dt>return getMinTweetID() == that.getMinTweetID()</dt><dd><p>&amp;&amp; getMaxTweetID() == that.getMaxTweetID()
&amp;&amp; getMinDocID() == that.getMinDocID()
&amp;&amp; getMaxDocID() == that.getMaxDocID()
&amp;&amp; segmentBoundaryTimestamp == that.segmentBoundaryTimestamp
&amp;&amp; segmentSize == that.segmentSize
&amp;&amp; tweetsPerTimestamp.equals(that.tweetsPerTimestamp)
&amp;&amp; tweetIds.equals(that.tweetIds);</p>
</dd>
</dl>
</div></blockquote>
<p>}</p>
<p>&#64;Override
public OutOfOrderRealtimeTweetIDMapper.FlushHandler getFlushHandler() {</p>
<blockquote>
<div><p>return new OutOfOrderRealtimeTweetIDMapper.FlushHandler(this);</p>
</div></blockquote>
<p>}</p>
<dl>
<dt>private OutOfOrderRealtimeTweetIDMapper(</dt><dd><p>long minTweetID,
long maxTweetID,
int minDocID,
int maxDocID,
long segmentBoundaryTimestamp,
int segmentSize,
int[] docIDs,
long[] tweetIDList</p>
</dd>
<dt>) {</dt><dd><p>super(minTweetID, maxTweetID, minDocID, maxDocID, docIDs.length);</p>
<p>Preconditions.checkState(docIDs.length == tweetIDList.length);</p>
<p>this.segmentBoundaryTimestamp = segmentBoundaryTimestamp;
this.segmentSize = segmentSize;</p>
<p>tweetIds = new Int2LongOpenHashMap(segmentSize);
tweetIds.defaultReturnValue(ID_NOT_FOUND);</p>
<p>tweetsPerTimestamp = new Int2ByteOpenHashMap(segmentSize);
tweetsPerTimestamp.defaultReturnValue((byte) ID_NOT_FOUND);</p>
<dl>
<dt>for (int i = 0; i &lt; docIDs.length; i++) {</dt><dd><p>int docID = docIDs[i];
long tweetID = tweetIDList[i];
tweetIds.put(docID, tweetID);</p>
<p>int timestampBucket = docID &gt;&gt; DOC_ID_BITS;
if (tweetsPerTimestamp.containsKey(timestampBucket)) {</p>
<blockquote>
<div><p>tweetsPerTimestamp.addTo(timestampBucket, (byte) 1);</p>
</div></blockquote>
<dl class="simple">
<dt>} else {</dt><dd><p>tweetsPerTimestamp.put(timestampBucket, (byte) 1);</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>public static class FlushHandler extends Flushable.Handler&lt;OutOfOrderRealtimeTweetIDMapper&gt; {</dt><dd><p>private static final String MIN_TWEET_ID_PROP_NAME = “MinTweetID”;
private static final String MAX_TWEET_ID_PROP_NAME = “MaxTweetID”;
private static final String MIN_DOC_ID_PROP_NAME = “MinDocID”;
private static final String MAX_DOC_ID_PROP_NAME = “MaxDocID”;
private static final String SEGMENT_BOUNDARY_TIMESTAMP_PROP_NAME = “SegmentBoundaryTimestamp”;
private static final String SEGMENT_SIZE_PROP_NAME = “SegmentSize”;</p>
<dl class="simple">
<dt>public FlushHandler() {</dt><dd><p>super();</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>public FlushHandler(OutOfOrderRealtimeTweetIDMapper objectToFlush) {</dt><dd><p>super(objectToFlush);</p>
</dd>
</dl>
<p>}</p>
<p>&#64;Override
protected void doFlush(FlushInfo flushInfo, DataSerializer serializer) throws IOException {</p>
<blockquote>
<div><p>OutOfOrderRealtimeTweetIDMapper mapper = getObjectToFlush();</p>
<p>flushInfo.addLongProperty(MIN_TWEET_ID_PROP_NAME, mapper.getMinTweetID());
flushInfo.addLongProperty(MAX_TWEET_ID_PROP_NAME, mapper.getMaxTweetID());
flushInfo.addIntProperty(MIN_DOC_ID_PROP_NAME, mapper.getMinDocID());
flushInfo.addIntProperty(MAX_DOC_ID_PROP_NAME, mapper.getMaxDocID());
flushInfo.addLongProperty(SEGMENT_BOUNDARY_TIMESTAMP_PROP_NAME,</p>
<blockquote>
<div><p>mapper.segmentBoundaryTimestamp);</p>
</div></blockquote>
<p>flushInfo.addIntProperty(SEGMENT_SIZE_PROP_NAME, mapper.segmentSize);</p>
<p>serializer.writeInt(mapper.tweetIds.size());
for (Int2LongMap.Entry entry : mapper.tweetIds.int2LongEntrySet()) {</p>
<blockquote>
<div><p>serializer.writeInt(entry.getIntKey());
serializer.writeLong(entry.getLongValue());</p>
</div></blockquote>
<p>}</p>
</div></blockquote>
<p>}</p>
<p>&#64;Override
protected OutOfOrderRealtimeTweetIDMapper doLoad(FlushInfo flushInfo, DataDeserializer in)</p>
<blockquote>
<div><blockquote>
<div><p>throws IOException {</p>
</div></blockquote>
<p>int size = in.readInt();
int[] docIds = new int[size];
long[] tweetIds = new long[size];
for (int i = 0; i &lt; size; i++) {</p>
<blockquote>
<div><p>docIds[i] = in.readInt();
tweetIds[i] = in.readLong();</p>
</div></blockquote>
<p>}</p>
<dl class="simple">
<dt>return new OutOfOrderRealtimeTweetIDMapper(</dt><dd><p>flushInfo.getLongProperty(MIN_TWEET_ID_PROP_NAME),
flushInfo.getLongProperty(MAX_TWEET_ID_PROP_NAME),
flushInfo.getIntProperty(MIN_DOC_ID_PROP_NAME),
flushInfo.getIntProperty(MAX_DOC_ID_PROP_NAME),
flushInfo.getLongProperty(SEGMENT_BOUNDARY_TIMESTAMP_PROP_NAME),
flushInfo.getIntProperty(SEGMENT_SIZE_PROP_NAME),
docIds,
tweetIds);</p>
</dd>
</dl>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../../../../index.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../../../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../../../../../_sources/src/java/com/twitter/search/earlybird/index/OutOfOrderRealtimeTweetIDMapper.java.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>