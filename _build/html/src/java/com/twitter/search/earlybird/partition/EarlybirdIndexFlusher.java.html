<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../../../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../../../../../" id="documentation_options" src="../../../../../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>package com.twitter.search.earlybird.partition;</p>
<p>import java.io.File;
import java.io.IOException;
import java.io.OutputStreamWriter;
import java.text.DateFormat;
import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.time.Duration;
import java.util.ArrayList;
import java.util.Date;
import java.util.SortedMap;
import java.util.TreeMap;
import java.util.concurrent.TimeoutException;</p>
<p>import scala.runtime.BoxedUnit;</p>
<p>import com.google.common.base.Preconditions;</p>
<p>import org.apache.commons.compress.utils.Lists;
import org.apache.commons.lang.RandomStringUtils;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;</p>
<p>import com.twitter.common.util.Clock;
import com.twitter.search.common.config.Config;
import com.twitter.search.common.metrics.SearchCounter;
import com.twitter.search.common.schema.earlybird.FlushVersion;
import com.twitter.search.common.util.io.flushable.DataSerializer;
import com.twitter.search.common.util.io.flushable.FlushInfo;
import com.twitter.search.earlybird.common.NonPagingAssert;
import com.twitter.search.earlybird.util.ActionLogger;
import com.twitter.search.earlybird.util.CoordinatedEarlybirdActionInterface;
import com.twitter.search.earlybird.util.CoordinatedEarlybirdActionLockFailed;
import com.twitter.search.earlybird.util.ParallelUtil;</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Flushes an EarlybirdIndex to HDFS, so that when Earlybird starts, it can read the index from</p></li>
<li><p>HDFS instead of indexing from scratch.</p></li>
<li></li>
<li><p>The path looks like:</p></li>
<li><p>/smf1/rt2/user/search/earlybird/loadtest/realtime/indexes/flush_version_158/partition_8/index_2020_02_25_02</p></li>
</ul>
<p><a href="#id1"><span class="problematic" id="id2">*</span></a>/</p>
</dd>
<dt>public class EarlybirdIndexFlusher {</dt><dd><dl class="simple">
<dt>public enum FlushAttemptResult {</dt><dd><p>CHECKED_RECENTLY,
FOUND_INDEX,
FLUSH_ATTEMPT_MADE,
FAILED_LOCK_ATTEMPT,
HADOOP_TIMEOUT</p>
</dd>
</dl>
<p>}</p>
<p>&#64;FunctionalInterface
public interface PostFlushOperation {</p>
<blockquote>
<div><dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Run this after we finish flushing an index, before we rejoin the serverset.</p></li>
</ul>
<p><a href="#id3"><span class="problematic" id="id4">*</span></a>/</p>
</dd>
</dl>
<p>void execute();</p>
</div></blockquote>
<p>}</p>
<p>private static final Logger LOG = LoggerFactory.getLogger(EarlybirdIndexFlusher.class);</p>
<dl class="simple">
<dt>private static final SearchCounter FLUSH_SUCCESS_COUNTER =</dt><dd><p>SearchCounter.export(“successfully_flushed_index”);</p>
</dd>
</dl>
<p>public static final String TWEET_KAFKA_OFFSET = “tweet_kafka_offset”;
public static final String UPDATE_KAFKA_OFFSET = “update_kafka_offset”;
public static final String FLUSHED_FROM_REPLICA = “flushed_from_replica”;
public static final String SEGMENTS = “segments”;
public static final String TIMESLICE_ID = “timeslice_id”;</p>
<p>public static final String DATA_SUFFIX = “.data”;
public static final String INFO_SUFFIX = “.info”;
public static final String INDEX_INFO = “earlybird_index.info”;</p>
<p>private static final String INDEX_PATH_FORMAT = “%s/flush_version_%d/partition_%d”;
public static final DateFormat INDEX_DATE_SUFFIX = new SimpleDateFormat(“yyyy_MM_dd_HH”);
public static final String INDEX_PREFIX = “<a href="#id11"><span class="problematic" id="id12">index_</span></a>”;
public static final String TMP_PREFIX = “<a href="#id13"><span class="problematic" id="id14">tmp_</span></a>”;</p>
<p>// Check if we need to flush every five minutes.
private static final long FLUSH_CHECK_PERIOD = Duration.ofMinutes(5).toMillis();</p>
<p>// Make sure we don’t keep more than 3 copies of the index in HDFS, so that we don’t run out of
// HDFS space.
private static final int INDEX_COPIES = 3;</p>
<dl class="simple">
<dt>private static final NonPagingAssert FLUSHING_TOO_MANY_NON_OPTIMIZED_SEGMENTS =</dt><dd><p>new NonPagingAssert(“flushing_too_many_non_optimized_segments”);</p>
</dd>
</dl>
<p>private final CoordinatedEarlybirdActionInterface actionCoordinator;
private final FileSystem fileSystem;
private final Path indexPath;
private final Clock clock;
private final SegmentManager segmentManager;
private final int replicaId;
private final TimeLimitedHadoopExistsCall timeLimitedHadoopExistsCall;
private final OptimizationAndFlushingCoordinationLock optimizationAndFlushingCoordinationLock;</p>
<p>private long checkedAt = 0;</p>
<dl class="simple">
<dt>public EarlybirdIndexFlusher(</dt><dd><p>CoordinatedEarlybirdActionInterface actionCoordinator,
FileSystem fileSystem,
String indexHDFSPath,
SegmentManager segmentManager,
PartitionConfig partitionConfig,
Clock clock,
TimeLimitedHadoopExistsCall timeLimitedHadoopExistsCall,
OptimizationAndFlushingCoordinationLock optimizationAndFlushingCoordinationLock</p>
</dd>
<dt>) {</dt><dd><p>this.actionCoordinator = actionCoordinator;
this.fileSystem = fileSystem;
this.indexPath = buildPathToIndexes(indexHDFSPath, partitionConfig);
this.segmentManager = segmentManager;
this.clock = clock;
this.replicaId = partitionConfig.getHostPositionWithinHashPartition();
this.timeLimitedHadoopExistsCall = timeLimitedHadoopExistsCall;
this.optimizationAndFlushingCoordinationLock = optimizationAndFlushingCoordinationLock;</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Periodically checks if an index needs to be uploaded to HDFS, and uploads it if necessary.</p></li>
<li><p>Skips flush if unable to acquire the optimizationAndFlushingCoordinationLock.</p></li>
</ul>
<p><a href="#id5"><span class="problematic" id="id6">*</span></a>/</p>
</dd>
<dt>public FlushAttemptResult flushIfNecessary(</dt><dd><blockquote>
<div><p>long tweetOffset,
long updateOffset,
PostFlushOperation postFlushOperation) throws Exception {</p>
</div></blockquote>
<p>long now = clock.nowMillis();
if (now - checkedAt &lt; FLUSH_CHECK_PERIOD) {</p>
<blockquote>
<div><p>return FlushAttemptResult.CHECKED_RECENTLY;</p>
</div></blockquote>
<p>}</p>
<p>checkedAt = now;</p>
<p>// Try to aqcuire lock to ensure that we are not in the gc_before_optimization or the
// post_optimization_rebuilds step of optimization. If the lock is not available, then skip
// flushing.
if (!optimizationAndFlushingCoordinationLock.tryLock()) {</p>
<blockquote>
<div><p>return FlushAttemptResult.FAILED_LOCK_ATTEMPT;</p>
</div></blockquote>
<p>}
// Acquired the lock, so wrap the flush in a try/finally block to ensure we release the lock
try {</p>
<blockquote>
<div><p>Path flushPath = pathForHour();</p>
<dl>
<dt>try {</dt><dd><p>// If this doesn’t execute on time, it will throw an exception and this function
// finishes its execution.
boolean result = timeLimitedHadoopExistsCall.exists(flushPath);</p>
<dl class="simple">
<dt>if (result) {</dt><dd><p>return FlushAttemptResult.FOUND_INDEX;</p>
</dd>
</dl>
<p>}</p>
</dd>
<dt>} catch (TimeoutException e) {</dt><dd><p>LOG.warn(“Timeout while calling hadoop”, e);
return FlushAttemptResult.HADOOP_TIMEOUT;</p>
</dd>
</dl>
<p>}</p>
<p>boolean flushedIndex = false;
try {</p>
<blockquote>
<div><p>// this function returns a boolean.
actionCoordinator.execute(“index_flushing”, isCoordinated -&gt;</p>
<blockquote>
<div><p>flushIndex(flushPath, isCoordinated, tweetOffset, updateOffset, postFlushOperation));</p>
</div></blockquote>
<p>flushedIndex = true;</p>
</div></blockquote>
<dl class="simple">
<dt>} catch (CoordinatedEarlybirdActionLockFailed e) {</dt><dd><p>// This only happens when we fail to grab the lock, which is fine because another Earlybird
// is already working on flushing this index, so we don’t need to.
LOG.debug(“Failed to grab lock”, e);</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>if (flushedIndex) {</dt><dd><p>// We don’t return with a guarantee that we actually flushed something. It’s possible
// that the .execute() function above was not able to leave the server set to flush.
return FlushAttemptResult.FLUSH_ATTEMPT_MADE;</p>
</dd>
<dt>} else {</dt><dd><p>return FlushAttemptResult.FAILED_LOCK_ATTEMPT;</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<dl class="simple">
<dt>} finally {</dt><dd><p>optimizationAndFlushingCoordinationLock.unlock();</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Create a subpath to the directory with many indexes in it. Will have an index for each hour.</p></li>
</ul>
<p><a href="#id7"><span class="problematic" id="id8">*</span></a>/</p>
</dd>
<dt>public static Path buildPathToIndexes(String root, PartitionConfig partitionConfig) {</dt><dd><dl class="simple">
<dt>return new Path(String.format(</dt><dd><p>INDEX_PATH_FORMAT,
root,
FlushVersion.CURRENT_FLUSH_VERSION.getVersionNumber(),
partitionConfig.getIndexingHashPartitionID()));</p>
</dd>
</dl>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Returns a sorted map from the unix time in millis an index was flushed to the path of an index.</p></li>
<li><p>The last element will be the path of the most recent index.</p></li>
</ul>
<p><a href="#id9"><span class="problematic" id="id10">*</span></a>/</p>
</dd>
<dt>public static SortedMap&lt;Long, Path&gt; getIndexPathsByTime(</dt><dd><p>Path indexPath,
FileSystem fileSystem</p>
</dd>
<dt>) throws IOException, ParseException {</dt><dd><p>LOG.info(“Getting index paths from file system: {}”, fileSystem.getUri().toASCIIString());</p>
<p>SortedMap&lt;Long, Path&gt; pathByTime = new TreeMap&lt;&gt;();
Path globPattern = indexPath.suffix(“/” + EarlybirdIndexFlusher.INDEX_PREFIX + “*”);
LOG.info(“Lookup glob pattern: {}”, globPattern);</p>
<dl class="simple">
<dt>for (FileStatus indexDir<span class="classifier">fileSystem.globStatus(globPattern)) {</span></dt><dd><p>String name = new File(indexDir.getPath().toString()).getName();
String dateString = name.substring(EarlybirdIndexFlusher.INDEX_PREFIX.length());
Date date = EarlybirdIndexFlusher.INDEX_DATE_SUFFIX.parse(dateString);
pathByTime.put(date.getTime(), indexDir.getPath());</p>
</dd>
</dl>
<p>}
LOG.info(“Found {} files matching the pattern.”, pathByTime.size());</p>
<p>return pathByTime;</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>private boolean flushIndex(</dt><dd><p>Path flushPath,
boolean isCoordinated,
long tweetOffset,
long updateOffset,
PostFlushOperation postFlushOperation</p>
</dd>
<dt>) throws Exception {</dt><dd><p>Preconditions.checkState(isCoordinated);</p>
<dl class="simple">
<dt>if (fileSystem.exists(flushPath)) {</dt><dd><p>return false;</p>
</dd>
</dl>
<p>}</p>
<p>LOG.info(“Starting index flush”);</p>
<p>// In case the process is killed suddenly, we wouldn’t be able to clean up the temporary
// directory, and we don’t want other processes to reuse it, so add some randomness.
Path tmpPath = indexPath.suffix(“/” + TMP_PREFIX + RandomStringUtils.randomAlphabetic(8));
boolean creationSucceed = fileSystem.mkdirs(tmpPath);
if (!creationSucceed) {</p>
<blockquote>
<div><p>throw new IOException(“Couldn’t create HDFS directory at “ + flushPath);</p>
</div></blockquote>
<p>}</p>
<p>LOG.info(“Temp path: {}”, tmpPath);
try {</p>
<blockquote>
<div><dl class="simple">
<dt>ArrayList&lt;SegmentInfo&gt; segmentInfos = Lists.newArrayList(segmentManager.getSegmentInfos(</dt><dd><p>SegmentManager.Filter.Enabled, SegmentManager.Order.NEW_TO_OLD).iterator());</p>
</dd>
</dl>
<p>segmentManager.logState(“Before flushing”);
EarlybirdIndex index = new EarlybirdIndex(segmentInfos, tweetOffset, updateOffset);
ActionLogger.run(</p>
<blockquote>
<div><p>“Flushing index to “ + tmpPath,
() -&gt; flushIndex(tmpPath, index));</p>
</div></blockquote>
</div></blockquote>
<dl>
<dt>} catch (Exception e) {</dt><dd><p>LOG.error(“Exception while flushing index. Rethrowing.”);</p>
<dl class="simple">
<dt>if (fileSystem.delete(tmpPath, true)) {</dt><dd><p>LOG.info(“Successfully deleted temp output”);</p>
</dd>
<dt>} else {</dt><dd><p>LOG.error(“Couldn’t delete temp output”);</p>
</dd>
</dl>
<p>}</p>
<p>throw e;</p>
</dd>
</dl>
<p>}</p>
<p>// We flush it to a temporary directory, then rename the temporary directory so that it the
// change is atomic, and other Earlybirds will either see the old indexes, or the new, complete
// index, but never an in progress index.
boolean renameSucceeded = fileSystem.rename(tmpPath, flushPath);
if (!renameSucceeded) {</p>
<blockquote>
<div><p>throw new IOException(“Couldn’t rename HDFS from “ + tmpPath + “ to “ + flushPath);</p>
</div></blockquote>
<p>}
LOG.info(“Flushed index to {}”, flushPath);</p>
<p>cleanupOldIndexes();</p>
<p>FLUSH_SUCCESS_COUNTER.increment();</p>
<p>LOG.info(“Executing post flush operation…”);
postFlushOperation.execute();</p>
<p>return true;</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>private void cleanupOldIndexes() throws Exception {</dt><dd><p>LOG.info(“Looking up whether we need to clean up old indexes…”);
SortedMap&lt;Long, Path&gt; pathsByTime =</p>
<blockquote>
<div><p>EarlybirdIndexFlusher.getIndexPathsByTime(indexPath, fileSystem);</p>
</div></blockquote>
<dl>
<dt>while (pathsByTime.size() &gt; INDEX_COPIES) {</dt><dd><p>Long key = pathsByTime.firstKey();
Path oldestHourPath = pathsByTime.remove(key);
LOG.info(“Deleting old index at path ‘{}’.”, oldestHourPath);</p>
<dl class="simple">
<dt>if (fileSystem.delete(oldestHourPath, true)) {</dt><dd><p>LOG.info(“Successfully deleted old index”);</p>
</dd>
<dt>} else {</dt><dd><p>LOG.error(“Couldn’t delete old index”);</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>private Path pathForHour() {</dt><dd><p>Date date = new Date(clock.nowMillis());
String time = INDEX_DATE_SUFFIX.format(date);
return indexPath.suffix(“/” + INDEX_PREFIX + time);</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>private void flushIndex(Path flushPath, EarlybirdIndex index) throws Exception {</dt><dd><p>int numOfNonOptimized = index.numOfNonOptimizedSegments();
if (numOfNonOptimized &gt; EarlybirdIndex.MAX_NUM_OF_NON_OPTIMIZED_SEGMENTS) {</p>
<blockquote>
<div><dl class="simple">
<dt>LOG.error(</dt><dd><p>“Found {} non-optimized segments when flushing to disk!”, numOfNonOptimized);</p>
</dd>
</dl>
<p>FLUSHING_TOO_MANY_NON_OPTIMIZED_SEGMENTS.assertFailed();</p>
</div></blockquote>
<p>}</p>
<p>int numSegments = index.getSegmentInfoList().size();
int flushingThreadPoolSize = numSegments;</p>
<dl class="simple">
<dt>if (Config.environmentIsTest()) {</dt><dd><p>// SEARCH-33763: Limit the thread pool size for tests to avoid using too much memory on scoot.
flushingThreadPoolSize = 2;</p>
</dd>
</dl>
<p>}</p>
<p>LOG.info(“Flushing index using a thread pool size of {}”, flushingThreadPoolSize);</p>
<dl class="simple">
<dt>ParallelUtil.parmap(“flush-index”, flushingThreadPoolSize, si -&gt; ActionLogger.call(</dt><dd><p>“Flushing segment “ + si.getSegmentName(),
() -&gt; flushSegment(flushPath, si)), index.getSegmentInfoList());</p>
</dd>
</dl>
<p>FlushInfo indexInfo = new FlushInfo();
indexInfo.addLongProperty(UPDATE_KAFKA_OFFSET, index.getUpdateOffset());
indexInfo.addLongProperty(TWEET_KAFKA_OFFSET, index.getTweetOffset());
indexInfo.addIntProperty(FLUSHED_FROM_REPLICA, replicaId);</p>
<p>FlushInfo segmentFlushInfos = indexInfo.newSubProperties(SEGMENTS);
for (SegmentInfo segmentInfo : index.getSegmentInfoList()) {</p>
<blockquote>
<div><p>FlushInfo segmentFlushInfo = segmentFlushInfos.newSubProperties(segmentInfo.getSegmentName());
segmentFlushInfo.addLongProperty(TIMESLICE_ID, segmentInfo.getTimeSliceID());</p>
</div></blockquote>
<p>}</p>
<p>Path indexInfoPath = flushPath.suffix(“/” + INDEX_INFO);
try (FSDataOutputStream infoOutputStream = fileSystem.create(indexInfoPath)) {</p>
<blockquote>
<div><p>OutputStreamWriter infoFileWriter = new OutputStreamWriter(infoOutputStream);
FlushInfo.flushAsYaml(indexInfo, infoFileWriter);</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>private BoxedUnit flushSegment(Path flushPath, SegmentInfo segmentInfo) throws Exception {</dt><dd><p>Path segmentPrefix = flushPath.suffix(“/” + segmentInfo.getSegmentName());
Path segmentPath = segmentPrefix.suffix(DATA_SUFFIX);</p>
<p>FlushInfo flushInfo = new FlushInfo();</p>
<dl class="simple">
<dt>try (FSDataOutputStream outputStream = fileSystem.create(segmentPath)) {</dt><dd><p>DataSerializer out = new DataSerializer(segmentPath.toString(), outputStream);
segmentInfo.getIndexSegment().flush(flushInfo, out);</p>
</dd>
</dl>
<p>}</p>
<p>Path infoPath = segmentPrefix.suffix(INFO_SUFFIX);</p>
<dl class="simple">
<dt>try (FSDataOutputStream infoOutputStream = fileSystem.create(infoPath)) {</dt><dd><p>OutputStreamWriter infoFileWriter = new OutputStreamWriter(infoOutputStream);
FlushInfo.flushAsYaml(flushInfo, infoFileWriter);</p>
</dd>
</dl>
<p>}
return BoxedUnit.UNIT;</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../../../../index2.rst.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../../../../index2.rst.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../../../../../_sources/src/java/com/twitter/search/earlybird/partition/EarlybirdIndexFlusher.java.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>