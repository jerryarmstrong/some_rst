<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../../../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../../../../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../../../../../../" id="documentation_options" src="../../../../../../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../../../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>package com.twitter.search.earlybird.partition.freshstartup;</p>
<p>import java.io.IOException;
import java.time.Duration;
import java.util.ArrayList;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;</p>
<p>import com.google.common.base.Stopwatch;
import com.google.common.base.Verify;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableMap;
import com.google.common.collect.Lists;</p>
<p>import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.consumer.OffsetAndTimestamp;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.errors.ApiException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;</p>
<p>import com.twitter.search.common.indexing.thriftjava.ThriftVersionedEvents;
import static com.twitter.search.common.util.LogFormatUtil.formatInt;</p>
<p>import com.twitter.search.common.util.GCUtil;
import com.twitter.common.util.Clock;
import com.twitter.search.common.util.LogFormatUtil;
import com.twitter.search.earlybird.common.NonPagingAssert;
import com.twitter.search.earlybird.common.config.EarlybirdConfig;
import com.twitter.search.earlybird.exception.CriticalExceptionHandler;
import com.twitter.search.earlybird.exception.EarlybirdStartupException;
import com.twitter.search.earlybird.exception.WrappedKafkaApiException;
import com.twitter.search.earlybird.factory.EarlybirdKafkaConsumersFactory;
import com.twitter.search.earlybird.partition.EarlybirdIndex;
import com.twitter.search.earlybird.partition.SegmentInfo;
import com.twitter.search.earlybird.partition.SegmentManager;
import com.twitter.search.earlybird.util.ParallelUtil;</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Bootstraps an index by indexing tweets and updates in parallel.</p></li>
<li></li>
<li><p>DEVELOPMENT</p></li>
<li></li>
<li></li>
<li><ol class="arabic simple">
<li><p>In earlybird-search.yml, set the following values in the “production” section:</p></li>
</ol>
</li>
<li><ul>
<li><p>max_segment_size to 200000</p></li>
</ul>
</li>
<li><ul>
<li><p>late_tweet_buffer to 10000</p></li>
</ul>
</li>
<li></li>
<li><ol class="arabic simple" start="2">
<li><p>In KafkaStartup, don’t load the index, replace the .loadIndex call as instructed</p></li>
</ol>
</li>
<li><p>in the file.</p></li>
<li></li>
<li><ol class="arabic simple" start="3">
<li><p>In the aurora configs, set serving_timeslices to a low number (like 5) for staging.</p></li>
</ol>
</li>
</ul>
<p><a href="#id1"><span class="problematic" id="id2">*</span></a>/</p>
</dd>
<dt>public class FreshStartupHandler {</dt><dd><p>private static final Logger LOG = LoggerFactory.getLogger(FreshStartupHandler.class);
private static final NonPagingAssert BUILDING_FEWER_THAN_SPECIFIED_SEGMENTS =</p>
<blockquote>
<div><p>new NonPagingAssert(“building_fewer_than_specified_segments”);</p>
</div></blockquote>
<p>private final Clock clock;
private final TopicPartition tweetTopic;
private final TopicPartition updateTopic;
private final SegmentManager segmentManager;
private final int maxSegmentSize;
private final int lateTweetBuffer;
private final EarlybirdKafkaConsumersFactory earlybirdKafkaConsumersFactory;
private final CriticalExceptionHandler criticalExceptionHandler;</p>
<dl class="simple">
<dt>public FreshStartupHandler(</dt><dd><p>Clock clock,
EarlybirdKafkaConsumersFactory earlybirdKafkaConsumersFactory,
TopicPartition tweetTopic,
TopicPartition updateTopic,
SegmentManager segmentManager,
int maxSegmentSize,
int lateTweetBuffer,
CriticalExceptionHandler criticalExceptionHandler</p>
</dd>
<dt>) {</dt><dd><p>this.clock = clock;
this.earlybirdKafkaConsumersFactory = earlybirdKafkaConsumersFactory;
this.tweetTopic = tweetTopic;
this.updateTopic = updateTopic;
this.segmentManager = segmentManager;
this.maxSegmentSize = maxSegmentSize;
this.criticalExceptionHandler = criticalExceptionHandler;
this.lateTweetBuffer = lateTweetBuffer;</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Don’t index in parallel, just pass some time back that the EarlybirdKafkaConsumer</p></li>
<li><p>can start indexing from.</p></li>
</ul>
<p><a href="#id3"><span class="problematic" id="id4">*</span></a>/</p>
</dd>
<dt>public EarlybirdIndex indexFromScratch() {</dt><dd><dl class="simple">
<dt>long indexTimePeriod = Duration.ofHours(</dt><dd><p>EarlybirdConfig.getInt(“index_from_scratch_hours”, 12)</p>
</dd>
</dl>
<p>).toMillis();</p>
<p>return runIndexFromScratch(indexTimePeriod);</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>public EarlybirdIndex fastIndexFromScratchForDevelopment() {</dt><dd><p>LOG.info(“Running fast index from scratch…”);
return runIndexFromScratch(Duration.ofMinutes(10).toMillis());</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>private EarlybirdIndex runIndexFromScratch(long indexTimePeriodMs) {</dt><dd><dl class="simple">
<dt>KafkaConsumer&lt;Long, ThriftVersionedEvents&gt; consumerForFindingOffsets =</dt><dd><p>earlybirdKafkaConsumersFactory.createKafkaConsumer(“consumer_for_offsets”);</p>
</dd>
</dl>
<p>long timestamp = clock.nowMillis() - indexTimePeriodMs;</p>
<p>Map&lt;TopicPartition, OffsetAndTimestamp&gt; offsets;
try {</p>
<blockquote>
<div><dl class="simple">
<dt>offsets = consumerForFindingOffsets</dt><dd><p>.offsetsForTimes(ImmutableMap.of(tweetTopic, timestamp, updateTopic, timestamp));</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>} catch (ApiException kafkaApiException) {</dt><dd><p>throw new WrappedKafkaApiException(kafkaApiException);</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>return new EarlybirdIndex(</dt><dd><p>Lists.newArrayList(),
offsets.get(tweetTopic).offset(),
offsets.get(updateTopic).offset());</p>
</dd>
</dl>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Index Tweets and updates from scratch, without relying on a serialized index in HDFS.</p></li>
<li></li>
<li><p>This function indexes the segments in parallel, limiting the number of segments that</p></li>
<li><p>are currently indexed, due to memory limitations. That’s followed by another pass to index</p></li>
<li><p>some updates - see the implementation for more details.</p></li>
<li></li>
<li><p>The index this function outputs contains N segments, where the first N-1 are optimized and</p></li>
<li><p>the last one is not.</p></li>
</ul>
<p><a href="#id5"><span class="problematic" id="id6">*</span></a>/</p>
</dd>
<dt>public EarlybirdIndex parallelIndexFromScratch() throws Exception {</dt><dd><p>Stopwatch parallelIndexStopwatch = Stopwatch.createStarted();</p>
<p>LOG.info(“Starting parallel fresh startup.”);
LOG.info(“Max segment size: {}”, maxSegmentSize);
LOG.info(“Late tweet buffer size: {}”, lateTweetBuffer);</p>
<p>// Once we finish fresh startup and proceed to indexing from the streams, we’ll immediately
// start a new segment, since the output of the fresh startup is full segments.
//
// That’s why we index max_segments-1 segments here instead of indexing max_segments segments
// and discarding the first one later.
int numSegments = segmentManager.getMaxEnabledSegments() - 1;
LOG.info(“Number of segments to build: {}”, numSegments);</p>
<p>// Find end offsets.
KafkaOffsetPair tweetsOffsetRange = findOffsetRangeForTweetsKafkaTopic();</p>
<dl class="simple">
<dt>ArrayList&lt;SegmentBuildInfo&gt; segmentBuildInfos = makeSegmentBuildInfos(</dt><dd><p>numSegments, tweetsOffsetRange);</p>
</dd>
</dl>
<p>segmentManager.logState(“Before starting fresh startup”);</p>
<p>// Index tweets and events.
Stopwatch initialIndexStopwatch = Stopwatch.createStarted();</p>
<p>// We index at most <cite>MAX_PARALLEL_INDEXED</cite> (MPI) segments at the same time. If we need to
// produce 20 segments here, we’d need memory for MPI unoptimized and 20-MPI optimized segments.
//
// For back of envelope calculations you can assume optimized segments take ~6GB and unoptimized
// ones ~12GB.
final int MAX_PARALLEL_INDEXED = 8;</p>
<dl class="simple">
<dt>List&lt;SegmentInfo&gt; segmentInfos = ParallelUtil.parmap(</dt><dd><p>“fresh-startup”,
MAX_PARALLEL_INDEXED,
segmentBuildInfo -&gt; indexTweetsAndUpdatesForSegment(segmentBuildInfo, segmentBuildInfos),
segmentBuildInfos</p>
</dd>
</dl>
<p>);</p>
<p>LOG.info(“Finished indexing tweets and updates in {}”, initialIndexStopwatch);</p>
<dl class="simple">
<dt>PostOptimizationUpdatesIndexer postOptimizationUpdatesIndexer =</dt><dd><dl class="simple">
<dt>new PostOptimizationUpdatesIndexer(</dt><dd><p>segmentBuildInfos,
earlybirdKafkaConsumersFactory,
updateTopic);</p>
</dd>
</dl>
</dd>
</dl>
<p>postOptimizationUpdatesIndexer.indexRestOfUpdates();</p>
<p>// Finished indexing tweets and updates.
LOG.info(“Segment build infos after we’re done:”);
for (SegmentBuildInfo segmentBuildInfo : segmentBuildInfos) {</p>
<blockquote>
<div><p>segmentBuildInfo.logState();</p>
</div></blockquote>
<p>}</p>
<p>segmentManager.logState(“After finishing fresh startup”);</p>
<p>LOG.info(“Collected {} segment infos”, segmentInfos.size());
LOG.info(“Segment names:”);
for (SegmentInfo segmentInfo : segmentInfos) {</p>
<blockquote>
<div><p>LOG.info(segmentInfo.getSegmentName());</p>
</div></blockquote>
<p>}</p>
<p>SegmentBuildInfo lastSegmentBuildInfo = segmentBuildInfos.get(segmentBuildInfos.size() - 1);
long finishedUpdatesAtOffset = lastSegmentBuildInfo.getUpdateKafkaOffsetPair().getEndOffset();
long maxIndexedTweetId = lastSegmentBuildInfo.getMaxIndexedTweetId();</p>
<p>LOG.info(“Max indexed tweet id: {}”, maxIndexedTweetId);
LOG.info(“Parallel startup finished in {}”, parallelIndexStopwatch);</p>
<p>// verifyConstructedIndex(segmentBuildInfos);
// Run a GC to free up some memory after the fresh startup.
GCUtil.runGC();
logMemoryStats();</p>
<dl class="simple">
<dt>return new EarlybirdIndex(</dt><dd><p>segmentInfos,
tweetsOffsetRange.getEndOffset() + 1,
finishedUpdatesAtOffset + 1,
maxIndexedTweetId</p>
</dd>
</dl>
<p>);</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>private void logMemoryStats() {</dt><dd><p>double toGB = 1024 * 1024 * 1024;
double totalMemoryGB = Runtime.getRuntime().totalMemory() / toGB;
double freeMemoryGB = Runtime.getRuntime().freeMemory() / toGB;
LOG.info(“Memory stats: Total memory GB: {}, Free memory GB: {}”,</p>
<blockquote>
<div><p>totalMemoryGB, freeMemoryGB);</p>
</div></blockquote>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Prints statistics about the constructed index compared to all tweets in the</p></li>
<li><p>tweets stream.</p></li>
<li></li>
<li><p>Only run this for testing and debugging purposes, never in prod environment.</p></li>
</ul>
<p><a href="#id7"><span class="problematic" id="id8">*</span></a>/</p>
</dd>
<dt>private void verifyConstructedIndex(List&lt;SegmentBuildInfo&gt; segmentBuildInfos)</dt><dd><blockquote>
<div><p>throws IOException {</p>
</div></blockquote>
<p>LOG.info(“Verifying constructed index…”);
// Read every tweet from the offset range that we’re constructing an index for.
KafkaConsumer&lt;Long, ThriftVersionedEvents&gt; tweetsKafkaConsumer =</p>
<blockquote>
<div><p>earlybirdKafkaConsumersFactory.createKafkaConsumer(“tweets_verify”);</p>
</div></blockquote>
<dl class="simple">
<dt>try {</dt><dd><p>tweetsKafkaConsumer.assign(ImmutableList.of(tweetTopic));
tweetsKafkaConsumer.seek(tweetTopic, segmentBuildInfos.get(0).getTweetStartOffset());</p>
</dd>
<dt>} catch (ApiException apiException) {</dt><dd><p>throw new WrappedKafkaApiException(apiException);</p>
</dd>
</dl>
<p>}
long finalTweetOffset = segmentBuildInfos.get(segmentBuildInfos.size() - 1).getTweetEndOffset();
boolean done = false;
Set&lt;Long&gt; uniqueTweetIds = new HashSet&lt;&gt;();
long readTweetsCount = 0;
do {</p>
<blockquote>
<div><dl>
<dt>for (ConsumerRecord&lt;Long, ThriftVersionedEvents&gt; record</dt><dd><blockquote>
<div><p>: tweetsKafkaConsumer.poll(Duration.ofSeconds(1))) {</p>
</div></blockquote>
<dl class="simple">
<dt>if (record.offset() &gt; finalTweetOffset) {</dt><dd><p>done = true;
break;</p>
</dd>
</dl>
<p>}
readTweetsCount++;
uniqueTweetIds.add(record.value().getId());</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>} while (!done);</p>
<p>LOG.info(“Total amount of read tweets: {}”, formatInt(readTweetsCount));
// Might be less, due to duplicates.
LOG.info(“Unique tweet ids : {}”, LogFormatUtil.formatInt(uniqueTweetIds.size()));</p>
<p>int notFoundInIndex = 0;
for (Long tweetId : uniqueTweetIds) {</p>
<blockquote>
<div><p>boolean found = false;
for (SegmentBuildInfo segmentBuildInfo : segmentBuildInfos) {</p>
<blockquote>
<div><dl class="simple">
<dt>if (segmentBuildInfo.getSegmentWriter().hasTweet(tweetId)) {</dt><dd><p>found = true;
break;</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>}
if (!found) {</p>
<blockquote>
<div><p>notFoundInIndex++;</p>
</div></blockquote>
<p>}</p>
</div></blockquote>
<p>}</p>
<p>LOG.info(“Tweets not found in the index: {}”, LogFormatUtil.formatInt(notFoundInIndex));</p>
<p>long totalIndexedTweets = 0;
for (SegmentBuildInfo segmentBuildInfo : segmentBuildInfos) {</p>
<blockquote>
<div><p>SegmentInfo si = segmentBuildInfo.getSegmentWriter().getSegmentInfo();
totalIndexedTweets += si.getIndexStats().getStatusCount();</p>
</div></blockquote>
<p>}</p>
<p>LOG.info(“Total indexed tweets: {}”, formatInt(totalIndexedTweets));</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Find the end offsets for the tweets Kafka topic this partition is reading</p></li>
<li><p>from.</p></li>
</ul>
<p><a href="#id9"><span class="problematic" id="id10">*</span></a>/</p>
</dd>
<dt>private KafkaOffsetPair findOffsetRangeForTweetsKafkaTopic() {</dt><dd><dl class="simple">
<dt>KafkaConsumer&lt;Long, ThriftVersionedEvents&gt; consumerForFindingOffsets =</dt><dd><p>earlybirdKafkaConsumersFactory.createKafkaConsumer(“consumer_for_end_offsets”);</p>
</dd>
</dl>
<p>Map&lt;TopicPartition, Long&gt; endOffsets;
Map&lt;TopicPartition, Long&gt; beginningOffsets;</p>
<dl class="simple">
<dt>try {</dt><dd><p>endOffsets = consumerForFindingOffsets.endOffsets(ImmutableList.of(tweetTopic));
beginningOffsets = consumerForFindingOffsets.beginningOffsets(ImmutableList.of(tweetTopic));</p>
</dd>
<dt>} catch (ApiException kafkaApiException) {</dt><dd><p>throw new WrappedKafkaApiException(kafkaApiException);</p>
</dd>
<dt>} finally {</dt><dd><p>consumerForFindingOffsets.close();</p>
</dd>
</dl>
<p>}</p>
<p>long tweetsBeginningOffset = beginningOffsets.get(tweetTopic);
long tweetsEndOffset = endOffsets.get(tweetTopic);
LOG.info(String.format(“Tweets beginning offset: %,d”, tweetsBeginningOffset));
LOG.info(String.format(“Tweets end offset: %,d”, tweetsEndOffset));
LOG.info(String.format(“Total amount of records in the stream: %,d”,</p>
<blockquote>
<div><p>tweetsEndOffset - tweetsBeginningOffset + 1));</p>
</div></blockquote>
<p>return new KafkaOffsetPair(tweetsBeginningOffset, tweetsEndOffset);</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>For each segment, we know what offset it begins at. This function finds the tweet ids</p></li>
<li><p>for these offsets.</p></li>
</ul>
<p><a href="#id11"><span class="problematic" id="id12">*</span></a>/</p>
</dd>
<dt>private void fillTweetIdsForSegmentStarts(List&lt;SegmentBuildInfo&gt; segmentBuildInfos)</dt><dd><blockquote>
<div><p>throws EarlybirdStartupException {</p>
</div></blockquote>
<dl class="simple">
<dt>KafkaConsumer&lt;Long, ThriftVersionedEvents&gt; consumerForTweetIds =</dt><dd><p>earlybirdKafkaConsumersFactory.createKafkaConsumer(“consumer_for_tweet_ids”, 1);</p>
</dd>
</dl>
<p>consumerForTweetIds.assign(ImmutableList.of(tweetTopic));</p>
<p>// Find first tweet ids for each segment.
for (SegmentBuildInfo buildInfo : segmentBuildInfos) {</p>
<blockquote>
<div><p>long tweetOffset = buildInfo.getTweetStartOffset();
ConsumerRecords&lt;Long, ThriftVersionedEvents&gt; records;
try {</p>
<blockquote>
<div><p>consumerForTweetIds.seek(tweetTopic, tweetOffset);
records = consumerForTweetIds.poll(Duration.ofSeconds(1));</p>
</div></blockquote>
<dl class="simple">
<dt>} catch (ApiException kafkaApiException) {</dt><dd><p>throw new WrappedKafkaApiException(kafkaApiException);</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>if (records.count() &gt; 0) {</dt><dd><p>ConsumerRecord&lt;Long, ThriftVersionedEvents&gt; recordAtOffset = records.iterator().next();
if (recordAtOffset.offset() != tweetOffset) {</p>
<blockquote>
<div><dl class="simple">
<dt>LOG.error(String.format(“We were looking for offset %,d. Found a record at offset %,d”,</dt><dd><p>tweetOffset, recordAtOffset.offset()));</p>
</dd>
</dl>
</div></blockquote>
<p>}</p>
<p>buildInfo.setStartTweetId(recordAtOffset.value().getId());</p>
</dd>
<dt>} else {</dt><dd><p>throw new EarlybirdStartupException(“Didn’t get any tweets back for an offset”);</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>}</p>
<p>// Check that something weird didn’t happen where we end up with segment ids
// which are in non-incresing order.
// Goes from oldest to newest.
for (int i = 1; i &lt; segmentBuildInfos.size(); i++) {</p>
<blockquote>
<div><p>long startTweetId = segmentBuildInfos.get(i).getStartTweetId();
long prevStartTweetId = segmentBuildInfos.get(i - 1).getStartTweetId();
Verify.verify(prevStartTweetId &lt; startTweetId);</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Generate the offsets at which tweets begin and end for each segment that we want</p></li>
<li><p>to create.</p></li>
</ul>
<p><a href="#id13"><span class="problematic" id="id14">*</span></a>/</p>
</dd>
<dt>private ArrayList&lt;SegmentBuildInfo&gt; makeSegmentBuildInfos(</dt><dd><blockquote>
<div><p>int numSegments, KafkaOffsetPair tweetsOffsets) throws EarlybirdStartupException {</p>
</div></blockquote>
<p>ArrayList&lt;SegmentBuildInfo&gt; segmentBuildInfos = new ArrayList&lt;&gt;();</p>
<p>// If we have 3 segments, the starting tweet offsets are:
// end-3N, end-2N, end-N
int segmentSize = maxSegmentSize - lateTweetBuffer;
LOG.info(“Segment size: {}”, segmentSize);</p>
<p>long tweetsInStream = tweetsOffsets.getEndOffset() - tweetsOffsets.getBeginOffset() + 1;
double numBuildableSegments = ((double) tweetsInStream) / segmentSize;</p>
<p>LOG.info(“Number of segments we can build: {}”, numBuildableSegments);</p>
<p>int numSegmentsToBuild = numSegments;
int numBuildableSegmentsInt = (int) numBuildableSegments;</p>
<dl>
<dt>if (numBuildableSegmentsInt &lt; numSegmentsToBuild) {</dt><dd><p>// This can happen if we get a low amount of tweets such that the ~10 days of tweets stored in
// Kafka are not enough to build the specified number of segments.
LOG.warn(“Building {} segments instead of the specified {} segments because there are not “</p>
<blockquote>
<div><ul class="simple">
<li><p>“enough tweets”, numSegmentsToBuild, numSegments);</p></li>
</ul>
</div></blockquote>
<p>BUILDING_FEWER_THAN_SPECIFIED_SEGMENTS.assertFailed();
numSegmentsToBuild = numBuildableSegmentsInt;</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>for (int rewind = numSegmentsToBuild; rewind &gt;= 1; rewind–) {</dt><dd><p>long tweetStartOffset = (tweetsOffsets.getEndOffset() + 1) - (rewind * segmentSize);
long tweetEndOffset = tweetStartOffset + segmentSize - 1;</p>
<p>int index = segmentBuildInfos.size();</p>
<dl class="simple">
<dt>segmentBuildInfos.add(new SegmentBuildInfo(</dt><dd><p>tweetStartOffset,
tweetEndOffset,
index,
rewind == 1</p>
</dd>
</dl>
<p>));</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>Verify.verify(segmentBuildInfos.get(segmentBuildInfos.size() - 1)</dt><dd><p>.getTweetEndOffset() == tweetsOffsets.getEndOffset());</p>
</dd>
</dl>
<p>LOG.info(“Filling start tweet ids …”);
fillTweetIdsForSegmentStarts(segmentBuildInfos);</p>
<p>return segmentBuildInfos;</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>private SegmentInfo indexTweetsAndUpdatesForSegment(</dt><dd><blockquote>
<div><p>SegmentBuildInfo segmentBuildInfo,
ArrayList&lt;SegmentBuildInfo&gt; segmentBuildInfos) throws Exception {</p>
</div></blockquote>
<dl>
<dt>PreOptimizationSegmentIndexer preOptimizationSegmentIndexer =</dt><dd><dl class="simple">
<dt>new PreOptimizationSegmentIndexer(</dt><dd><p>segmentBuildInfo,
segmentBuildInfos,
this.segmentManager,
this.tweetTopic,
this.updateTopic,
this.earlybirdKafkaConsumersFactory,
this.lateTweetBuffer</p>
</dd>
</dl>
<p>);</p>
</dd>
</dl>
<p>return preOptimizationSegmentIndexer.runIndexing();</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../../../../../index2.rst.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../../../../../index2.rst.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../../../../../../_sources/src/java/com/twitter/search/earlybird/partition/freshstartup/FreshStartupHandler.java.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>