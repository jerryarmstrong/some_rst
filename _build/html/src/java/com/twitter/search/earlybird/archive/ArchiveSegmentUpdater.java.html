<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../../../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../../../../../" id="documentation_options" src="../../../../../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>package com.twitter.search.earlybird.archive;</p>
<p>import java.io.IOException;
import java.util.Date;</p>
<p>import com.google.common.base.Preconditions;
import com.google.common.base.Predicate;</p>
<p>import org.apache.commons.lang.time.FastDateFormat;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;</p>
<p>import com.twitter.common.util.Clock;
import com.twitter.search.common.metrics.SearchRateCounter;
import com.twitter.search.common.metrics.SearchStatsReceiver;
import com.twitter.search.common.metrics.SearchStatsReceiverImpl;
import com.twitter.search.common.schema.thriftjava.ThriftIndexingEvent;
import com.twitter.search.common.util.io.recordreader.RecordReader;
import com.twitter.search.common.util.zktrylock.ZooKeeperTryLockFactory;
import com.twitter.search.earlybird.EarlybirdIndexConfig;
import com.twitter.search.earlybird.common.config.EarlybirdConfig;
import com.twitter.search.earlybird.document.DocumentFactory;
import com.twitter.search.earlybird.document.TweetDocument;
import com.twitter.search.earlybird.exception.CriticalExceptionHandler;
import com.twitter.search.earlybird.index.EarlybirdSegmentFactory;
import com.twitter.search.earlybird.partition.SearchIndexingMetricSet;
import com.twitter.search.earlybird.partition.SegmentHdfsFlusher;
import com.twitter.search.earlybird.partition.SegmentInfo;
import com.twitter.search.earlybird.partition.SegmentLoader;
import com.twitter.search.earlybird.partition.SegmentOptimizer;
import com.twitter.search.earlybird.partition.SegmentSyncConfig;
import com.twitter.search.earlybird.partition.SimpleSegmentIndexer;
import com.twitter.search.earlybird.stats.EarlybirdSearcherStats;</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Given a segment, this class checks if the segment has an index built on HDFS:</p></li>
<li><p>if not, use SimpleSegmentIndexer to build an index</p></li>
<li><p>if yes, load the HDFS index, build a new index for the new status data which has dates newer</p></li>
<li><p>than the HDFS index, then append the loaded HDFS index.</p></li>
</ul>
<p><a href="#id1"><span class="problematic" id="id2">*</span></a>/</p>
</dd>
<dt>public class ArchiveSegmentUpdater {</dt><dd><p>private static final Logger LOG = LoggerFactory.getLogger(ArchiveSegmentUpdater.class);</p>
<p>private final SegmentSyncConfig sync;
private final EarlybirdIndexConfig earlybirdIndexConfig;
private final ZooKeeperTryLockFactory zkTryLockFactory;
private final SearchStatsReceiver statsReceiver = new SearchStatsReceiverImpl();
private final SearchIndexingMetricSet searchIndexingMetricSet =</p>
<blockquote>
<div><p>new SearchIndexingMetricSet(statsReceiver);</p>
</div></blockquote>
<dl class="simple">
<dt>private final EarlybirdSearcherStats searcherStats =</dt><dd><p>new EarlybirdSearcherStats(statsReceiver);</p>
</dd>
<dt>private final SearchRateCounter indexNewSegment =</dt><dd><p>new SearchRateCounter(“index_new_segment”);</p>
</dd>
<dt>private final SearchRateCounter updateExistingSegment =</dt><dd><p>new SearchRateCounter(“update_existing_segment”);</p>
</dd>
<dt>private final SearchRateCounter skipExistingSegment =</dt><dd><p>new SearchRateCounter(“skip_existing_segment”);</p>
</dd>
</dl>
<p>private Clock clock;</p>
<dl>
<dt>public ArchiveSegmentUpdater(ZooKeeperTryLockFactory zooKeeperTryLockFactory,</dt><dd><blockquote>
<div><p>SegmentSyncConfig sync,
EarlybirdIndexConfig earlybirdIndexConfig,
Clock clock) {</p>
</div></blockquote>
<p>this.sync = sync;
this.earlybirdIndexConfig = earlybirdIndexConfig;
this.zkTryLockFactory = zooKeeperTryLockFactory;
this.clock = clock;</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>private boolean canUpdateSegment(SegmentInfo segmentInfo) {</dt><dd><dl>
<dt>if (!(segmentInfo.getSegment() instanceof ArchiveSegment)) {</dt><dd><dl class="simple">
<dt>LOG.info(“only ArchiveSegment is available for updating now: “</dt><dd><ul class="simple">
<li><p>segmentInfo);</p></li>
</ul>
</dd>
</dl>
<p>return false;</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>if (!segmentInfo.isEnabled()) {</dt><dd><p>LOG.debug(“Segment is disabled: “ + segmentInfo);
return false;</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>if (segmentInfo.isComplete() || segmentInfo.isIndexing()</dt><dd><blockquote>
<div><p>|| segmentInfo.getSyncInfo().isLoaded()) {</p>
</div></blockquote>
<p>LOG.debug(“Cannot update already indexed segment: “ + segmentInfo);
return false;</p>
</dd>
</dl>
<p>}</p>
<p>return true;</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Given a segment, checks if the segment has an index built on HDFS:</p></li>
<li><p>if not, use SimpleSegmentIndexer to build an index</p></li>
<li><p>if yes, load the HDFS index, build a new index for the new status data which has dates newer</p></li>
<li><p>than the HDFS index, then append the loaded HDFS index.</p></li>
<li></li>
<li><p>Returns whether the segment was successfully updated.</p></li>
</ul>
<p><a href="#id3"><span class="problematic" id="id4">*</span></a>/</p>
</dd>
<dt>public boolean updateSegment(SegmentInfo segmentInfo) {</dt><dd><p>Preconditions.checkArgument(segmentInfo.getSegment() instanceof ArchiveSegment);
if (!canUpdateSegment(segmentInfo)) {</p>
<blockquote>
<div><p>return false;</p>
</div></blockquote>
<p>}</p>
<dl class="simple">
<dt>if (segmentInfo.isIndexing()) {</dt><dd><p>LOG.error(“Segment is already being indexed: “ + segmentInfo);
return false;</p>
</dd>
</dl>
<p>}</p>
<p>final Date hdfsEndDate = ArchiveHDFSUtils.getSegmentEndDateOnHdfs(sync, segmentInfo);
if (hdfsEndDate == null) {</p>
<blockquote>
<div><p>indexNewSegment.increment();
if (!indexSegment(segmentInfo, ArchiveSegment.MATCH_ALL_DATE_PREDICATE)) {</p>
<blockquote>
<div><p>return false;</p>
</div></blockquote>
<p>}</p>
</div></blockquote>
<dl>
<dt>} else {</dt><dd><p>final Date curEndDate = ((ArchiveSegment) segmentInfo.getSegment()).getDataEndDate();
if (!hdfsEndDate.before(curEndDate)) {</p>
<blockquote>
<div><p>skipExistingSegment.increment();
LOG.info(“Segment is up-to-date: “ + segmentInfo.getSegment().getTimeSliceID()</p>
<blockquote>
<div><ul class="simple">
<li><p>“ Found flushed segment on HDFS with end date: “</p></li>
<li><p>FastDateFormat.getInstance(“yyyyMMdd”).format(hdfsEndDate));</p></li>
</ul>
</div></blockquote>
<p>segmentInfo.setComplete(true);
segmentInfo.getSyncInfo().setFlushed(true);
return true;</p>
</div></blockquote>
<p>}</p>
<p>updateExistingSegment.increment();
LOG.info(“Updating segment: “ + segmentInfo.getSegment().getTimeSliceID()</p>
<blockquote>
<div><ul class="simple">
<li><p>“; new endDate will be “ + FastDateFormat.getInstance(“yyyyMMdd”).format(curEndDate));</p></li>
</ul>
</div></blockquote>
<dl class="simple">
<dt>if (!updateSegment(segmentInfo, hdfsEndDate)) {</dt><dd><p>return false;</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<p>boolean success = SegmentOptimizer.optimize(segmentInfo);
if (!success) {</p>
<blockquote>
<div><p>// Clean up the segment dir on local disk
segmentInfo.deleteLocalIndexedSegmentDirectoryImmediately();
LOG.info(“Error optimizing segment: “ + segmentInfo);
return false;</p>
</div></blockquote>
<p>}</p>
<p>// Verify segment before uploading.
success = ArchiveSegmentVerifier.verifySegment(segmentInfo);
if (!success) {</p>
<blockquote>
<div><p>segmentInfo.deleteLocalIndexedSegmentDirectoryImmediately();
LOG.info(“Segment not uploaded to HDFS because it did not pass verification: “ + segmentInfo);
return false;</p>
</div></blockquote>
<p>}</p>
<p>// upload the index to HDFS
success = new SegmentHdfsFlusher(zkTryLockFactory, sync, false)</p>
<blockquote>
<div><p>.flushSegmentToDiskAndHDFS(segmentInfo);</p>
</div></blockquote>
<dl class="simple">
<dt>if (success) {</dt><dd><p>ArchiveHDFSUtils.deleteHdfsSegmentDir(sync, segmentInfo, false, true);</p>
</dd>
<dt>} else {</dt><dd><p>// Clean up the segment dir on hdfs
ArchiveHDFSUtils.deleteHdfsSegmentDir(sync, segmentInfo, true, false);
LOG.info(“Error uploading segment to HDFS: “ + segmentInfo);</p>
</dd>
</dl>
<p>}
segmentInfo.deleteLocalIndexedSegmentDirectoryImmediately();</p>
<p>return success;</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Build index for the given segmentInfo. Only those statuses passing the dateFilter are indexed.</p></li>
</ul>
<p><a href="#id5"><span class="problematic" id="id6">*</span></a>/</p>
</dd>
<dt>private boolean indexSegment(final SegmentInfo segmentInfo, Predicate&lt;Date&gt; dateFilter) {</dt><dd><p>Preconditions.checkArgument(segmentInfo.getSegment() instanceof ArchiveSegment);</p>
<p>RecordReader&lt;TweetDocument&gt; documentReader = null;
try {</p>
<blockquote>
<div><p>ArchiveSegment archiveSegment = (ArchiveSegment) segmentInfo.getSegment();
DocumentFactory&lt;ThriftIndexingEvent&gt; documentFactory =</p>
<blockquote>
<div><p>earlybirdIndexConfig.createDocumentFactory();</p>
</div></blockquote>
<p>documentReader = archiveSegment.getStatusRecordReader(documentFactory, dateFilter);</p>
<p>// Read and index the statuses
boolean success = new SimpleSegmentIndexer(documentReader, searchIndexingMetricSet)</p>
<blockquote>
<div><p>.indexSegment(segmentInfo);</p>
</div></blockquote>
<dl class="simple">
<dt>if (!success) {</dt><dd><p>// Clean up segment dir on local disk
segmentInfo.deleteLocalIndexedSegmentDirectoryImmediately();
LOG.info(“Error indexing segment: “ + segmentInfo);</p>
</dd>
</dl>
<p>}</p>
<p>return success;</p>
</div></blockquote>
<dl>
<dt>} catch (IOException e) {</dt><dd><p>segmentInfo.deleteLocalIndexedSegmentDirectoryImmediately();
LOG.info(“Exception while indexing segment: “ + segmentInfo, e);
return false;</p>
</dd>
<dt>} finally {</dt><dd><dl class="simple">
<dt>if (documentReader != null) {</dt><dd><p>documentReader.stop();</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Load the index built on HDFS for the given segmentInfo, index the new data and append the</p></li>
<li><p>HDFS index to the new indexed segment</p></li>
</ul>
<p><a href="#id7"><span class="problematic" id="id8">*</span></a>/</p>
</dd>
<dt>private boolean updateSegment(final SegmentInfo segmentInfo, final Date hdfsEndDate) {</dt><dd><p>SegmentInfo hdfsSegmentInfo = loadSegmentFromHdfs(segmentInfo, hdfsEndDate);
if (hdfsSegmentInfo == null) {</p>
<blockquote>
<div><p>return indexSegment(segmentInfo, ArchiveSegment.MATCH_ALL_DATE_PREDICATE);</p>
</div></blockquote>
<p>}</p>
<dl class="simple">
<dt>boolean success = indexSegment(segmentInfo, input -&gt; {</dt><dd><p>// we’re updating the segment - only index days after the old end date,
// and we’re sure that the previous days have already been indexed.
return input.after(hdfsEndDate);</p>
</dd>
</dl>
<p>});
if (!success) {</p>
<blockquote>
<div><p>LOG.error(“Error indexing new data: “ + segmentInfo);
return indexSegment(segmentInfo, ArchiveSegment.MATCH_ALL_DATE_PREDICATE);</p>
</div></blockquote>
<p>}</p>
<p>// Now, append the index loaded from hdfs
try {</p>
<blockquote>
<div><p>segmentInfo.getIndexSegment().append(hdfsSegmentInfo.getIndexSegment());
hdfsSegmentInfo.deleteLocalIndexedSegmentDirectoryImmediately();
LOG.info(“Deleted local segment directories with end date “ + hdfsEndDate + “ : “</p>
<blockquote>
<div><ul class="simple">
<li><p>segmentInfo);</p></li>
</ul>
</div></blockquote>
</div></blockquote>
<dl class="simple">
<dt>} catch (IOException e) {</dt><dd><p>LOG.warn(“Caught IOException while appending segment “ + hdfsSegmentInfo.getSegmentName(), e);
hdfsSegmentInfo.deleteLocalIndexedSegmentDirectoryImmediately();
segmentInfo.deleteLocalIndexedSegmentDirectoryImmediately();
return false;</p>
</dd>
</dl>
<p>}</p>
<p>segmentInfo.setComplete(true);
return true;</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Load the index built on HDFS for the given segmentInfo and end date</p></li>
</ul>
<p><a href="#id9"><span class="problematic" id="id10">*</span></a>/</p>
</dd>
<dt>private SegmentInfo loadSegmentFromHdfs(final SegmentInfo segmentInfo, final Date hdfsEndDate) {</dt><dd><p>Preconditions.checkArgument(segmentInfo.getSegment() instanceof ArchiveSegment);</p>
<dl class="simple">
<dt>ArchiveSegment segment = new ArchiveSegment(</dt><dd><p>segmentInfo.getTimeSliceID(),
EarlybirdConfig.getMaxSegmentSize(),
segmentInfo.getNumPartitions(),
segmentInfo.getSegment().getHashPartitionID(),
hdfsEndDate);</p>
</dd>
<dt>EarlybirdSegmentFactory factory = new EarlybirdSegmentFactory(</dt><dd><p>earlybirdIndexConfig,
searchIndexingMetricSet,
searcherStats,
clock);</p>
</dd>
</dl>
<p>SegmentInfo hdfsSegmentInfo;</p>
<dl>
<dt>try {</dt><dd><p>hdfsSegmentInfo = new SegmentInfo(segment,  factory, sync);
CriticalExceptionHandler criticalExceptionHandler =</p>
<blockquote>
<div><p>new CriticalExceptionHandler();</p>
</div></blockquote>
<dl>
<dt>boolean success = new SegmentLoader(sync, criticalExceptionHandler)</dt><dd><p>.load(hdfsSegmentInfo);</p>
</dd>
<dt>if (!success) {</dt><dd><p>// If not successful, segmentLoader has already cleaned up the local dir.
LOG.info(“Error loading hdfs segment “ + hdfsSegmentInfo</p>
<blockquote>
<div><ul class="simple">
<li><p>“, building segment from scratch.”);</p></li>
</ul>
</div></blockquote>
<p>hdfsSegmentInfo = null;</p>
</dd>
</dl>
<p>}</p>
</dd>
<dt>} catch (IOException e) {</dt><dd><p>LOG.error(“Exception while loading segment from hdfs: “ + segmentInfo, e);
hdfsSegmentInfo = null;</p>
</dd>
</dl>
<p>}</p>
<p>return hdfsSegmentInfo;</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../../../../index2.rst.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../../../../index2.rst.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../../../../../_sources/src/java/com/twitter/search/earlybird/archive/ArchiveSegmentUpdater.java.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>