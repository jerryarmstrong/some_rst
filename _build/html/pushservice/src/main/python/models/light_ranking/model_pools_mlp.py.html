<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../../../../" id="documentation_options" src="../../../../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>import warnings</p>
<p>from twml.contrib.layers import ZscoreNormalization</p>
<p>from …libs.customized_full_sparse import FullSparse
from …libs.get_feat_config import FEAT_CONFIG_DEFAULT_VAL as MISSING_VALUE_MARKER
from …libs.model_utils import (</p>
<blockquote>
<div><p>_sparse_feature_fixup,
adaptive_transformation,
filter_nans_and_infs,
get_dense_out,
tensor_dropout,</p>
</div></blockquote>
<p>)</p>
<p>import tensorflow.compat.v1 as tf
# checkstyle: noqa</p>
<dl>
<dt>def light_ranking_mlp_ngbdt(features, is_training, params, label=None):</dt><dd><dl class="simple">
<dt>return deepnorm_light_ranking(</dt><dd><p>features,
is_training,
params,
label=label,
decay=params.momentum,
dense_emb_size=params.dense_embedding_size,
base_activation=tf.keras.layers.LeakyReLU(),
input_dropout_rate=params.dropout,
use_gbdt=False,</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>def deepnorm_light_ranking(</dt><dd><p>features,
is_training,
params,
label=None,
decay=0.99999,
dense_emb_size=128,
base_activation=None,
input_dropout_rate=None,
input_dense_type=”self_atten_dense”,
emb_dense_type=”self_atten_dense”,
mlp_dense_type=”self_atten_dense”,
use_gbdt=False,</p>
</dd>
<dt>):</dt><dd><p># ——————————————————–
#            Initial Parameter Checking
# ——————————————————–
if base_activation is None:</p>
<blockquote>
<div><p>base_activation = tf.keras.layers.LeakyReLU()</p>
</div></blockquote>
<dl>
<dt>if label is not None:</dt><dd><dl class="simple">
<dt>warnings.warn(</dt><dd><p>“Label is unused in deepnorm_gbdt. Stop using this argument.”,
DeprecationWarning,</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>with tf.variable_scope(“helper_layers”):</dt><dd><dl class="simple">
<dt>full_sparse_layer = FullSparse(</dt><dd><p>output_size=params.sparse_embedding_size,
activation=base_activation,
use_sparse_grads=is_training,
use_binary_values=False,
dtype=tf.float32,</p>
</dd>
</dl>
<p>)
input_normalizing_layer = ZscoreNormalization(decay=decay, name=”input_normalizing_layer”)</p>
</dd>
</dl>
<p># ——————————————————–
#            Feature Selection &amp; Embedding
# ——————————————————–
if use_gbdt:</p>
<blockquote>
<div><p>sparse_gbdt_features = _sparse_feature_fixup(features[“gbdt_sparse”], params.input_size_bits)
if input_dropout_rate is not None:</p>
<blockquote>
<div><dl class="simple">
<dt>sparse_gbdt_features = tensor_dropout(</dt><dd><p>sparse_gbdt_features, input_dropout_rate, is_training, sparse_tensor=True</p>
</dd>
</dl>
<p>)</p>
</div></blockquote>
<p>total_embed = full_sparse_layer(sparse_gbdt_features, use_binary_values=True)</p>
<dl class="simple">
<dt>if (input_dropout_rate is not None) and is_training:</dt><dd><p>total_embed = total_embed / (1 - input_dropout_rate)</p>
</dd>
</dl>
</div></blockquote>
<dl>
<dt>else:</dt><dd><dl>
<dt>with tf.variable_scope(“dense_branch”):</dt><dd><p>dense_continuous_features = filter_nans_and_infs(features[“continuous”])</p>
<dl>
<dt>if params.use_missing_sub_branch:</dt><dd><p>is_missing = tf.equal(dense_continuous_features, MISSING_VALUE_MARKER)
continuous_features_filled = tf.where(</p>
<blockquote>
<div><p>is_missing,
tf.zeros_like(dense_continuous_features),
dense_continuous_features,</p>
</div></blockquote>
<p>)
normalized_features = input_normalizing_layer(</p>
<blockquote>
<div><p>continuous_features_filled, is_training, tf.math.logical_not(is_missing)</p>
</div></blockquote>
<p>)</p>
<dl>
<dt>with tf.variable_scope(“missing_sub_branch”):</dt><dd><dl class="simple">
<dt>missing_feature_embed = get_dense_out(</dt><dd><p>tf.cast(is_missing, tf.float32),
dense_emb_size,
activation=base_activation,
dense_type=input_dense_type,</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
</dd>
<dt>else:</dt><dd><p>continuous_features_filled = dense_continuous_features
normalized_features = input_normalizing_layer(continuous_features_filled, is_training)</p>
</dd>
<dt>with tf.variable_scope(“continuous_sub_branch”):</dt><dd><dl class="simple">
<dt>normalized_features = adaptive_transformation(</dt><dd><p>normalized_features, is_training, func_type=”tiny”</p>
</dd>
</dl>
<p>)</p>
<dl>
<dt>if input_dropout_rate is not None:</dt><dd><dl class="simple">
<dt>normalized_features = tensor_dropout(</dt><dd><p>normalized_features,
input_dropout_rate,
is_training,
sparse_tensor=False,</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>filled_feature_embed = get_dense_out(</dt><dd><p>normalized_features,
dense_emb_size,
activation=base_activation,
dense_type=input_dense_type,</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>if params.use_missing_sub_branch:</dt><dd><dl class="simple">
<dt>dense_embed = tf.concat(</dt><dd><p>[filled_feature_embed, missing_feature_embed], axis=1, name=”merge_dense_emb”</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>else:</dt><dd><p>dense_embed = filled_feature_embed</p>
</dd>
</dl>
</dd>
<dt>with tf.variable_scope(“sparse_branch”):</dt><dd><dl class="simple">
<dt>sparse_discrete_features = _sparse_feature_fixup(</dt><dd><p>features[“sparse_no_continuous”], params.input_size_bits</p>
</dd>
</dl>
<p>)
if input_dropout_rate is not None:</p>
<blockquote>
<div><dl class="simple">
<dt>sparse_discrete_features = tensor_dropout(</dt><dd><p>sparse_discrete_features, input_dropout_rate, is_training, sparse_tensor=True</p>
</dd>
</dl>
<p>)</p>
</div></blockquote>
<p>discrete_features_embed = full_sparse_layer(sparse_discrete_features, use_binary_values=True)</p>
<dl class="simple">
<dt>if (input_dropout_rate is not None) and is_training:</dt><dd><p>discrete_features_embed = discrete_features_embed / (1 - input_dropout_rate)</p>
</dd>
</dl>
</dd>
<dt>total_embed = tf.concat(</dt><dd><p>[dense_embed, discrete_features_embed],
axis=1,
name=”total_embed”,</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>total_embed = tf.layers.batch_normalization(</dt><dd><p>total_embed,
training=is_training,
renorm_momentum=decay,
momentum=decay,
renorm=is_training,
trainable=True,</p>
</dd>
</dl>
<p>)</p>
<p># ——————————————————–
#                MLP Layers
# ——————————————————–
with tf.variable_scope(“MLP_branch”):</p>
<blockquote>
<div><p>assert params.num_mlp_layers &gt;= 0
embed_list = [total_embed] + [None for _ in range(params.num_mlp_layers)]
dense_types = [emb_dense_type] + [mlp_dense_type for _ in range(params.num_mlp_layers - 1)]</p>
<dl>
<dt>for xl in range(1, params.num_mlp_layers + 1):</dt><dd><p>neurons = params.mlp_neuron_scale ** (params.num_mlp_layers + 1 - xl)
embed_list[xl] = get_dense_out(</p>
<blockquote>
<div><p>embed_list[xl - 1], neurons, activation=base_activation, dense_type=dense_types[xl - 1]</p>
</div></blockquote>
<p>)</p>
</dd>
<dt>if params.task_name in [“Sent”, “HeavyRankPosition”, “HeavyRankProbability”]:</dt><dd><p>logits = get_dense_out(embed_list[-1], 1, activation=None, dense_type=mlp_dense_type)</p>
</dd>
<dt>else:</dt><dd><p>raise ValueError(“Invalid Task Name !”)</p>
</dd>
</dl>
</div></blockquote>
<p>output_dict = {“output”: logits}
return output_dict</p>
</dd>
</dl>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../../../index.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../../../../_sources/pushservice/src/main/python/models/light_ranking/model_pools_mlp.py.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>