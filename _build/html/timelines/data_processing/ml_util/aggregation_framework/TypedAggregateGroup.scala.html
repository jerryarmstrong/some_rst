<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>package com.twitter.timelines.data_processing.ml_util.aggregation_framework</p>
<p>import com.twitter.ml.api._
import com.twitter.ml.api.constant.SharedFeatures
import com.twitter.ml.api.util.SRichDataRecord
import com.twitter.timelines.data_processing.ml_util.aggregation_framework.metrics.AggregateFeature
import com.twitter.timelines.data_processing.ml_util.aggregation_framework.metrics.AggregationMetric
import com.twitter.timelines.data_processing.ml_util.aggregation_framework.metrics.AggregationMetricCommon
import com.twitter.timelines.data_processing.ml_util.aggregation_framework.metrics.AggregationMetricCommon._
import com.twitter.timelines.data_processing.ml_util.transforms.OneToSomeTransform
import com.twitter.util.Duration
import com.twitter.util.Try
import java.lang.{Boolean =&gt; JBoolean}
import java.lang.{Double =&gt; JDouble}
import java.lang.{Long =&gt; JLong}
import java.util.{Set =&gt; JSet}
import scala.annotation.tailrec
import scala.language.existentials
import scala.collection.JavaConverters._
import scala.util.matching.Regex</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>A case class contained precomputed data useful to quickly</p></li>
<li><p>process operations over an aggregate.</p></li>
<li></li>
<li><p>&#64;param query The underlying feature being aggregated</p></li>
<li><p>&#64;param metric The aggregation metric</p></li>
<li><p>&#64;param outputFeatures The output features that aggregation will produce</p></li>
<li><p>&#64;param outputFeatureIds The precomputed hashes of the above outputFeatures</p></li>
</ul>
<p><a href="#id1"><span class="problematic" id="id2">*</span></a>/</p>
</dd>
<dt>case class PrecomputedAggregateDescriptor[T](</dt><dd><p>query: AggregateFeature[T],
metric: AggregationMetric[T, _],
outputFeatures: List[Feature[_]],
outputFeatureIds: List[JLong])</p>
</dd>
</dl>
<p>object TypedAggregateGroup {</p>
<blockquote>
<div><dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Recursive function that generates all combinations of value</p></li>
<li><p>assignments for a collection of sparse binary features.</p></li>
<li></li>
<li><p>&#64;param sparseBinaryIdValues list of sparse binary feature ids and possible values they can take</p></li>
<li><p>&#64;return A set of maps, where each map represents one possible assignment of values to ids</p></li>
</ul>
<p><a href="#id3"><span class="problematic" id="id4">*</span></a>/</p>
</dd>
<dt>def sparseBinaryPermutations(</dt><dd><p>sparseBinaryIdValues: List[(Long, Set[String])]</p>
</dd>
<dt>): Set[Map[Long, String]] = sparseBinaryIdValues match {</dt><dd><dl>
<dt>case (id, values) +: rest =&gt;</dt><dd><dl class="simple">
<dt>tailRecSparseBinaryPermutations(</dt><dd><p>existingPermutations = values.map(value =&gt; Map(id -&gt; value)),
remainingIdValues = rest</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
<p>case Nil =&gt; Set.empty</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>&#64;tailrec private[this] def tailRecSparseBinaryPermutations(</dt><dd><p>existingPermutations: Set[Map[Long, String]],
remainingIdValues: List[(Long, Set[String])]</p>
</dd>
<dt>): Set[Map[Long, String]] = remainingIdValues match {</dt><dd><p>case Nil =&gt; existingPermutations
case (id, values) +: rest =&gt;</p>
<blockquote>
<div><dl>
<dt>tailRecSparseBinaryPermutations(</dt><dd><dl class="simple">
<dt>existingPermutations.flatMap { existingIdValueMap =&gt;</dt><dd><p>values.map(value =&gt; existingIdValueMap ++ Map(id -&gt; value))</p>
</dd>
</dl>
<p>},
rest</p>
</dd>
</dl>
<p>)</p>
</div></blockquote>
</dd>
</dl>
<p>}</p>
<p>val SparseFeatureSuffix = “.member”
def sparseFeature(sparseBinaryFeature: Feature[_]): Feature[String] =</p>
<blockquote>
<div><dl class="simple">
<dt>new Feature.Text(</dt><dd><p>sparseBinaryFeature.getDenseFeatureName + SparseFeatureSuffix,
AggregationMetricCommon.derivePersonalDataTypes(Some(sparseBinaryFeature)))</p>
</dd>
</dl>
</div></blockquote>
<p>/* Throws exception if obj not an instance of U <a href="#id5"><span class="problematic" id="id6">*</span></a>/
private[this] def validate[U](obj: Any): U = {</p>
<blockquote>
<div><p>require(obj.isInstanceOf[U])
obj.asInstanceOf[U]</p>
</div></blockquote>
<p>}</p>
<dl>
<dt>private[this] def getFeatureOpt[U](dataRecord: DataRecord, feature: Feature[U]): Option[U] =</dt><dd><p>Option(SRichDataRecord(dataRecord).getFeatureValue(feature)).map(validate[U](_))</p>
</dd>
<dt>/**</dt><dd><ul class="simple">
<li><p>Get a mapping from feature ids</p></li>
<li><p>(including individual sparse elements of a sparse feature) to values</p></li>
<li><p>from the given data record, for a given feature type.</p></li>
<li></li>
<li><p>&#64;param dataRecord Data record to get features from</p></li>
<li><p>&#64;param keysToAggregate key features to get id-value mappings for</p></li>
<li><p>&#64;param featureType Feature type to get id-value maps for</p></li>
</ul>
<p><a href="#id7"><span class="problematic" id="id8">*</span></a>/</p>
</dd>
<dt>def getKeyFeatureIdValues[U](</dt><dd><p>dataRecord: DataRecord,
keysToAggregate: Set[Feature[_]],
featureType: FeatureType</p>
</dd>
<dt>): Set[(Long, Option[U])] = {</dt><dd><dl>
<dt>val featuresOfThisType: Set[Feature[U]] = keysToAggregate</dt><dd><p>.filter(_.getFeatureType == featureType)
.map(validate[Feature[U]])</p>
</dd>
<dt>featuresOfThisType</dt><dd><dl class="simple">
<dt>.map { feature: Feature[U] =&gt;</dt><dd><p>val featureId: Long = getDenseFeatureId(feature)
val featureOpt: Option[U] = getFeatureOpt(dataRecord, feature)
(featureId, featureOpt)</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
</dd>
</dl>
<p>}</p>
<p>// TypedAggregateGroup may transform the aggregate keys for internal use. This method generates
// denseFeatureIds for the transformed feature.
def getDenseFeatureId(feature: Feature[_]): Long =</p>
<blockquote>
<div><dl class="simple">
<dt>if (feature.getFeatureType != FeatureType.SPARSE_BINARY) {</dt><dd><p>feature.getDenseFeatureId</p>
</dd>
<dt>} else {</dt><dd><p>sparseFeature(feature).getDenseFeatureId</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Return denseFeatureIds for the input features after applying the custom transformation that</p></li>
<li><p>TypedAggregateGroup applies to its keysToAggregate.</p></li>
<li></li>
<li><p>&#64;param keysToAggregate key features to get id for</p></li>
</ul>
<p><a href="#id9"><span class="problematic" id="id10">*</span></a>/</p>
</dd>
<dt>def getKeyFeatureIds(keysToAggregate: Set[Feature[_]]): Set[Long] =</dt><dd><p>keysToAggregate.map(getDenseFeatureId)</p>
</dd>
<dt>def checkIfAllKeysExist[U](featureIdValueMap: Map[Long, Option[U]]): Boolean =</dt><dd><p>featureIdValueMap.forall { case (_, valueOpt) =&gt; valueOpt.isDefined }</p>
</dd>
<dt>def liftOptions[U](featureIdValueMap: Map[Long, Option[U]]): Map[Long, U] =</dt><dd><dl>
<dt>featureIdValueMap</dt><dd><dl class="simple">
<dt>.flatMap {</dt><dd><dl class="simple">
<dt>case (id, valueOpt) =&gt;</dt><dd><p>valueOpt.map { value =&gt; (id, value) }</p>
</dd>
</dl>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
</dd>
</dl>
<p>val timestampFeature: Feature[JLong] = SharedFeatures.TIMESTAMP</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Builds all valid aggregation keys (for the output store) from</p></li>
<li><p>a datarecord and a spec listing the keys to aggregate. There</p></li>
<li><p>can be multiple aggregation keys generated from a single data</p></li>
<li><p>record when grouping by sparse binary features, for which multiple</p></li>
<li><p>values can be set within the data record.</p></li>
<li></li>
<li><p>&#64;param dataRecord Data record to read values for key features from</p></li>
<li><p>&#64;return A set of AggregationKeys encoding the values of all keys</p></li>
</ul>
<p><a href="#id11"><span class="problematic" id="id12">*</span></a>/</p>
</dd>
<dt>def buildAggregationKeys(</dt><dd><p>dataRecord: DataRecord,
keysToAggregate: Set[Feature[_]]</p>
</dd>
<dt>): Set[AggregationKey] = {</dt><dd><dl class="simple">
<dt>val discreteAggregationKeys = getKeyFeatureIdValues[Long](</dt><dd><p>dataRecord,
keysToAggregate,
FeatureType.DISCRETE</p>
</dd>
</dl>
<p>).toMap</p>
<dl class="simple">
<dt>val textAggregationKeys = getKeyFeatureIdValues[String](</dt><dd><p>dataRecord,
keysToAggregate,
FeatureType.STRING</p>
</dd>
</dl>
<p>).toMap</p>
<dl>
<dt>val sparseBinaryIdValues = getKeyFeatureIdValues[JSet[String]](</dt><dd><p>dataRecord,
keysToAggregate,
FeatureType.SPARSE_BINARY</p>
</dd>
<dt>).map {</dt><dd><dl>
<dt>case (id, values) =&gt;</dt><dd><dl>
<dt>(</dt><dd><p>id,
values</p>
<blockquote>
<div><p>.map(_.asScala.toSet)
.getOrElse(Set.empty[String])</p>
</div></blockquote>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
</dd>
</dl>
<p>}.toList</p>
<dl>
<dt>if (checkIfAllKeysExist(discreteAggregationKeys) &amp;&amp;</dt><dd><p>checkIfAllKeysExist(textAggregationKeys)) {
if (sparseBinaryIdValues.nonEmpty) {</p>
<blockquote>
<div><dl>
<dt>sparseBinaryPermutations(sparseBinaryIdValues).map { sparseBinaryTextKeys =&gt;</dt><dd><dl class="simple">
<dt>AggregationKey(</dt><dd><p>discreteFeaturesById = liftOptions(discreteAggregationKeys),
textFeaturesById = liftOptions(textAggregationKeys) ++ sparseBinaryTextKeys</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<dl>
<dt>} else {</dt><dd><dl>
<dt>Set(</dt><dd><dl class="simple">
<dt>AggregationKey(</dt><dd><p>discreteFeaturesById = liftOptions(discreteAggregationKeys),
textFeaturesById = liftOptions(textAggregationKeys)</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>} else Set.empty[AggregationKey]</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Specifies one or more related aggregate(s) to compute in the summingbird job.</p></li>
<li></li>
<li><p>&#64;param inputSource Source to compute this aggregate over</p></li>
<li><p>&#64;param preTransforms Sequence of [[com.twitter.ml.api.RichITransform]] that transform</p></li>
<li><p>data records pre-aggregation (e.g. discretization, renaming)</p></li>
<li><p>&#64;param samplingTransformOpt Optional [[OneToSomeTransform]] that transform data</p></li>
<li><p>record to optional data record (e.g. for sampling) before aggregation</p></li>
<li><p>&#64;param aggregatePrefix Prefix to use for naming resultant aggregate features</p></li>
<li><p>&#64;param keysToAggregate Features to group by when computing the aggregates</p></li>
<li><p>(e.g. USER_ID, AUTHOR_ID)</p></li>
<li><p>&#64;param featuresToAggregate Features to aggregate (e.g. blender_score or is_photo)</p></li>
<li><p>&#64;param labels Labels to cross the features with to make pair features, if any.</p></li>
<li><p>use Label.All if you don’t want to cross with a label.</p></li>
<li><p>&#64;param metrics Aggregation metrics to compute (e.g. count, mean)</p></li>
<li><p>&#64;param halfLives Half lives to use for the aggregations, to be crossed with the above.</p></li>
<li><p>use Duration.Top for “forever” aggregations over an infinite time window (no decay).</p></li>
<li><p>&#64;param outputStore Store to output this aggregate to</p></li>
<li><p>&#64;param includeAnyFeature Aggregate label counts for any feature value</p></li>
<li><p>&#64;param includeAnyLabel Aggregate feature counts for any label value (e.g. all impressions)</p></li>
<li></li>
<li><p>The overall config for the summingbird job consists of a list of “AggregateGroup”</p></li>
<li><p>case class objects, which get translated into strongly typed “TypedAggregateGroup”</p></li>
<li><p>case class objects. A single TypedAggregateGroup always groups input data records from</p></li>
<li><p>‘’inputSource’’ by a single set of aggregation keys (‘’featuresToAggregate’’).</p></li>
<li><p>Within these groups, we perform a comprehensive cross of:</p></li>
<li></li>
<li><p>‘’featuresToAggregate’’ x ‘’labels’’ x ‘’metrics’’ x ‘’halfLives’’</p></li>
<li></li>
<li><p>All the resultant aggregate features are assigned a human-readable feature name</p></li>
<li><p>beginning with ‘’aggregatePrefix’’, and are written to DataRecords that get</p></li>
<li><p>aggregated and written to the store specified by ‘’outputStore’’.</p></li>
<li></li>
<li><p>Illustrative example. Suppose we define our spec as follows:</p></li>
<li></li>
<li><p>TypedAggregateGroup(</p></li>
<li><p>inputSource         = “timelines_recap_daily”,</p></li>
<li><p>aggregatePrefix     = “user_author_aggregate”,</p></li>
<li><p>keysToAggregate     = Set(USER_ID, AUTHOR_ID),</p></li>
<li><p>featuresToAggregate = Set(RecapFeatures.TEXT_SCORE, RecapFeatures.BLENDER_SCORE),</p></li>
<li><p>labels              = Set(RecapFeatures.IS_FAVORITED, RecapFeatures.IS_REPLIED),</p></li>
<li><p>metrics             = Set(CountMetric, MeanMetric),</p></li>
<li><p>halfLives           = Set(7.Days, 30.Days),</p></li>
<li><p>outputStore         = “user_author_aggregate_store”</p></li>
<li><p>)</p></li>
<li></li>
<li><p>This will process data records from the source named “timelines_recap_daily”</p></li>
<li><p>(see AggregateSource.scala for more details on how to add your own source)</p></li>
<li><p>It will produce a total of 2x2x2x2 = 16 aggregation features, named like:</p></li>
<li></li>
<li><p>user_author_aggregate.pair.recap.engagement.is_favorited.recap.searchfeature.blender_score.count.7days</p></li>
<li><p>user_author_aggregate.pair.recap.engagement.is_favorited.recap.searchfeature.blender_score.count.30days</p></li>
<li><p>user_author_aggregate.pair.recap.engagement.is_favorited.recap.searchfeature.blender_score.mean.7days</p></li>
<li></li>
<li><p>… (and so on)</p></li>
<li></li>
<li><p>and all the result features will be stored in DataRecords, summed up, and written</p></li>
<li><p>to the output store defined by the name “user_author_aggregate_store”.</p></li>
<li><p>(see AggregateStore.scala for details on how to add your own store).</p></li>
<li></li>
<li><p>If you do not want a full cross, split up your config into multiple TypedAggregateGroup</p></li>
<li><p>objects. Splitting is strongly advised to avoid blowing up and creating invalid</p></li>
<li><p>or unnecessary combinations of aggregate features (note that some combinations</p></li>
<li><p>are useless or invalid e.g. computing the mean of a binary feature). Splitting</p></li>
<li><p>also does not cost anything in terms of real-time performance, because all</p></li>
<li><p>Aggregate objects in the master spec that share the same ‘’keysToAggregate’’, the</p></li>
<li><p>same ‘’inputSource’’ and the same ‘’outputStore’’ are grouped by the summingbird</p></li>
<li><p>job logic and stored into a single DataRecord in the output store. Overlapping</p></li>
<li><p>aggregates will also automatically be deduplicated so don’t worry about overlaps.</p></li>
</ul>
<p><a href="#id13"><span class="problematic" id="id14">*</span></a>/</p>
</dd>
<dt>case class TypedAggregateGroup[T](</dt><dd><p>inputSource: AggregateSource,
aggregatePrefix: String,
keysToAggregate: Set[Feature[_]],
featuresToAggregate: Set[Feature[T]],
labels: Set[_ &lt;: Feature[JBoolean]],
metrics: Set[AggregationMetric[T, _]],
halfLives: Set[Duration],
outputStore: AggregateStore,
preTransforms: Seq[OneToSomeTransform] = Seq.empty,
includeAnyFeature: Boolean = true,
includeAnyLabel: Boolean = true,
aggExclusionRegex: Seq[String] = Seq.empty) {
import TypedAggregateGroup._</p>
<p>val compiledRegexes = aggExclusionRegex.map(new Regex(_))</p>
<p>// true if should drop, false if should keep
def filterOutAggregateFeature(</p>
<blockquote>
<div><p>feature: PrecomputedAggregateDescriptor[_],
regexes: Seq[Regex]</p>
</div></blockquote>
<dl>
<dt>): Boolean = {</dt><dd><dl>
<dt>if (regexes.nonEmpty)</dt><dd><dl class="simple">
<dt>feature.outputFeatures.exists { feature =&gt;</dt><dd><p>regexes.exists { re =&gt; re.findFirstMatchIn(feature.getDenseFeatureName).nonEmpty }</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>else false</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>def buildAggregationKeys(</dt><dd><p>dataRecord: DataRecord</p>
</dd>
<dt>): Set[AggregationKey] = {</dt><dd><p>TypedAggregateGroup.buildAggregationKeys(dataRecord, keysToAggregate)</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>This val precomputes descriptors for all individual aggregates in this group</p></li>
<li><p>(of type ‘’AggregateFeature’’). Also precompute hashes of all aggregation</p></li>
<li><p>“output” features generated by these operators for faster</p></li>
<li><p>run-time performance (this turns out to be a primary CPU bottleneck).</p></li>
<li><p>Ex: for the mean operator, “sum” and “count” are output features</p></li>
</ul>
<p><a href="#id15"><span class="problematic" id="id16">*</span></a>/</p>
</dd>
<dt>val individualAggregateDescriptors: Set[PrecomputedAggregateDescriptor[T]] = {</dt><dd><dl>
<dt>/*</dt><dd><ul class="simple">
<li><p>By default, in additional to all feature-label crosses, also</p></li>
<li><p>compute in aggregates over each feature and label without crossing</p></li>
</ul>
<p><a href="#id17"><span class="problematic" id="id18">*</span></a>/</p>
</dd>
<dt>val labelOptions = labels.map(Option(_)) ++</dt><dd><p>(if (includeAnyLabel) Set(None) else Set.empty)</p>
</dd>
<dt>val featureOptions = featuresToAggregate.map(Option(_)) ++</dt><dd><p>(if (includeAnyFeature) Set(None) else Set.empty)</p>
</dd>
<dt>for {</dt><dd><p>feature &lt;- featureOptions
label &lt;- labelOptions
metric &lt;- metrics
halfLife &lt;- halfLives</p>
</dd>
<dt>} yield {</dt><dd><p>val query = AggregateFeature[T](aggregatePrefix, feature, label, halfLife)</p>
<p>val aggregateOutputFeatures = metric.getOutputFeatures(query)
val aggregateOutputFeatureIds = metric.getOutputFeatureIds(query)
PrecomputedAggregateDescriptor(</p>
<blockquote>
<div><p>query,
metric,
aggregateOutputFeatures,
aggregateOutputFeatureIds</p>
</div></blockquote>
<p>)</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}.filterNot(filterOutAggregateFeature(_, compiledRegexes))</p>
<p>/* Precomputes a map from all generated aggregate feature ids to their half lives. <a href="#id19"><span class="problematic" id="id20">*</span></a>/
val continuousFeatureIdsToHalfLives: Map[Long, Duration] =</p>
<blockquote>
<div><dl>
<dt>individualAggregateDescriptors.flatMap { descriptor =&gt;</dt><dd><dl>
<dt>descriptor.outputFeatures</dt><dd><dl>
<dt>.flatMap { feature =&gt;</dt><dd><dl class="simple">
<dt>if (feature.getFeatureType() == FeatureType.CONTINUOUS) {</dt><dd><dl class="simple">
<dt>Try(feature.asInstanceOf[Feature[JDouble]]).toOption</dt><dd><p>.map(feature =&gt; (feature.getFeatureId(), descriptor.query.halfLife))</p>
</dd>
</dl>
</dd>
</dl>
<p>} else None</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
</dd>
</dl>
<p>}.toMap</p>
</div></blockquote>
<dl>
<dt>/*</dt><dd><ul class="simple">
<li><p>Sparse binary keys become individual string keys in the output.</p></li>
<li><p>e.g. group by “words.in.tweet”, output key: “words.in.tweet.member”</p></li>
</ul>
<p><a href="#id21"><span class="problematic" id="id22">*</span></a>/</p>
</dd>
<dt>val allOutputKeys: Set[Feature[_]] = keysToAggregate.map { key =&gt;</dt><dd><p>if (key.getFeatureType == FeatureType.SPARSE_BINARY) sparseFeature(key)
else key</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>val allOutputFeatures: Set[Feature[_]] = individualAggregateDescriptors.flatMap {</dt><dd><dl>
<dt>case PrecomputedAggregateDescriptor(</dt><dd><blockquote>
<div><blockquote>
<div><p>query,
metric,
outputFeatures,
outputFeatureIds</p>
</div></blockquote>
<p>) =&gt;</p>
</div></blockquote>
<p>outputFeatures</p>
</dd>
</dl>
</dd>
</dl>
<p>}</p>
<p>val aggregateContext: FeatureContext = new FeatureContext(allOutputFeatures.toList.asJava)</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Adds all aggregates in this group found in the two input data records</p></li>
<li><p>into a result, mutating the result. Uses a while loop for an</p></li>
<li><p>approximately 10% gain in speed over a for comprehension.</p></li>
<li></li>
<li><p>WARNING: mutates ‘’result’’</p></li>
<li></li>
<li><p>&#64;param result The output data record to mutate</p></li>
<li><p>&#64;param left The left data record to add</p></li>
<li><p>&#64;param right The right data record to add</p></li>
</ul>
<p><a href="#id23"><span class="problematic" id="id24">*</span></a>/</p>
</dd>
<dt>def mutatePlus(result: DataRecord, left: DataRecord, right: DataRecord): Unit = {</dt><dd><p>val featureIterator = individualAggregateDescriptors.iterator
while (featureIterator.hasNext) {</p>
<blockquote>
<div><p>val descriptor = featureIterator.next
descriptor.metric.mutatePlus(</p>
<blockquote>
<div><p>result,
left,
right,
descriptor.query,
Some(descriptor.outputFeatureIds)</p>
</div></blockquote>
<p>)</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Apply preTransforms sequentially. If any transform results in a dropped (None)</p></li>
<li><p>DataRecord, then entire tranform sequence will result in a dropped DataRecord.</p></li>
<li><p>Note that preTransforms are order-dependent.</p></li>
</ul>
<p><a href="#id25"><span class="problematic" id="id26">*</span></a>/</p>
</dd>
<dt>private[this] def sequentiallyTransform(dataRecord: DataRecord): Option[DataRecord] = {</dt><dd><p>val recordOpt = Option(new DataRecord(dataRecord))
preTransforms.foldLeft(recordOpt) {</p>
<blockquote>
<div><dl class="simple">
<dt>case (Some(previousRecord), preTransform) =&gt;</dt><dd><p>preTransform(previousRecord)</p>
</dd>
</dl>
<p>case _ =&gt; Option.empty[DataRecord]</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Given a data record, apply transforms and fetch the incremental contributions to</p></li>
<li><p>each configured aggregate from this data record, and store these in an output data record.</p></li>
<li></li>
<li><p>&#64;param dataRecord Input data record to aggregate.</p></li>
<li><p>&#64;return A set of tuples (AggregationKey, DataRecord) whose first entry is an</p></li>
<li><p>AggregationKey indicating what keys we’re grouping by, and whose second entry</p></li>
<li><p>is an output data record with incremental contributions to the aggregate value(s)</p></li>
</ul>
<p><a href="#id27"><span class="problematic" id="id28">*</span></a>/</p>
</dd>
<dt>def computeAggregateKVPairs(dataRecord: DataRecord): Set[(AggregationKey, DataRecord)] = {</dt><dd><dl>
<dt>sequentiallyTransform(dataRecord)</dt><dd><dl>
<dt>.flatMap { dataRecord =&gt;</dt><dd><p>val aggregationKeys = buildAggregationKeys(dataRecord)
val increment = new DataRecord</p>
<dl>
<dt>val isNonEmptyIncrement = individualAggregateDescriptors</dt><dd><dl>
<dt>.map { descriptor =&gt;</dt><dd><dl class="simple">
<dt>descriptor.metric.setIncrement(</dt><dd><p>output = increment,
input = dataRecord,
query = descriptor.query,
timestampFeature = inputSource.timestampFeature,
aggregateOutputs = Some(descriptor.outputFeatureIds)</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
<p>}
.exists(identity)</p>
</dd>
<dt>if (isNonEmptyIncrement) {</dt><dd><dl class="simple">
<dt>SRichDataRecord(increment).setFeatureValue(</dt><dd><p>timestampFeature,
getTimestamp(dataRecord, inputSource.timestampFeature)</p>
</dd>
</dl>
<p>)
Some(aggregationKeys.map(key =&gt; (key, increment)))</p>
</dd>
<dt>} else {</dt><dd><p>None</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}
.getOrElse(Set.empty[(AggregationKey, DataRecord)])</p>
</dd>
</dl>
</dd>
</dl>
<p>}</p>
<dl>
<dt>def outputFeaturesToRenamedOutputFeatures(prefix: String): Map[Feature[_], Feature[_]] = {</dt><dd><p>require(prefix.nonEmpty)</p>
<dl>
<dt>allOutputFeatures.map { feature =&gt;</dt><dd><dl>
<dt>if (feature.isSetFeatureName) {</dt><dd><p>val renamedFeatureName = prefix + feature.getDenseFeatureName
val personalDataTypes =</p>
<blockquote>
<div><p>if (feature.getPersonalDataTypes.isPresent) feature.getPersonalDataTypes.get()
else null</p>
</div></blockquote>
<dl class="simple">
<dt>val renamedFeature = feature.getFeatureType match {</dt><dd><dl class="simple">
<dt>case FeatureType.BINARY =&gt;</dt><dd><p>new Feature.Binary(renamedFeatureName, personalDataTypes)</p>
</dd>
<dt>case FeatureType.DISCRETE =&gt;</dt><dd><p>new Feature.Discrete(renamedFeatureName, personalDataTypes)</p>
</dd>
<dt>case FeatureType.STRING =&gt;</dt><dd><p>new Feature.Text(renamedFeatureName, personalDataTypes)</p>
</dd>
<dt>case FeatureType.CONTINUOUS =&gt;</dt><dd><p>new Feature.Continuous(renamedFeatureName, personalDataTypes)</p>
</dd>
<dt>case FeatureType.SPARSE_BINARY =&gt;</dt><dd><p>new Feature.SparseBinary(renamedFeatureName, personalDataTypes)</p>
</dd>
<dt>case FeatureType.SPARSE_CONTINUOUS =&gt;</dt><dd><p>new Feature.SparseContinuous(renamedFeatureName, personalDataTypes)</p>
</dd>
</dl>
</dd>
</dl>
<p>}
feature -&gt; renamedFeature</p>
</dd>
<dt>} else {</dt><dd><p>feature -&gt; feature</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}.toMap</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../index.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../../_sources/timelines/data_processing/ml_util/aggregation_framework/TypedAggregateGroup.scala.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>