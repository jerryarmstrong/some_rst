<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../_static/doctools.js?v=888ff710"></script>
    <script src="../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>import os</p>
<p>from toxicity_ml_pipeline.settings.default_settings_tox import LOCAL_DIR, MAX_SEQ_LENGTH
try:</p>
<blockquote>
<div><p>from toxicity_ml_pipeline.optim.losses import MaskedBCE</p>
</div></blockquote>
<dl class="simple">
<dt>except ImportError:</dt><dd><p>print(‘No MaskedBCE loss’)</p>
</dd>
</dl>
<p>from toxicity_ml_pipeline.utils.helpers import execute_command</p>
<p>import tensorflow as tf</p>
<dl class="simple">
<dt>try:</dt><dd><p>from twitter.cuad.representation.models.text_encoder import TextEncoder</p>
</dd>
<dt>except ModuleNotFoundError:</dt><dd><p>print(“No TextEncoder package”)</p>
</dd>
<dt>try:</dt><dd><p>from transformers import TFAutoModelForSequenceClassification</p>
</dd>
<dt>except ModuleNotFoundError:</dt><dd><p>print(“No HuggingFace package”)</p>
</dd>
</dl>
<p>LOCAL_MODEL_DIR = os.path.join(LOCAL_DIR, “models”)</p>
<dl>
<dt>def reload_model_weights(weights_dir, language, <a href="#id1"><span class="problematic" id="id2">**</span></a>kwargs):</dt><dd><p>optimizer = tf.keras.optimizers.Adam(0.01)
model_type = (</p>
<blockquote>
<div><p>“twitter_bert_base_en_uncased_mlm”
if language == “en”
else “twitter_multilingual_bert_base_cased_mlm”</p>
</div></blockquote>
<p>)
model = load(optimizer=optimizer, seed=42, model_type=model_type, <a href="#id3"><span class="problematic" id="id4">**</span></a>kwargs)
model.load_weights(weights_dir)</p>
<p>return model</p>
</dd>
<dt>def _locally_copy_models(model_type):</dt><dd><dl class="simple">
<dt>if model_type == “twitter_multilingual_bert_base_cased_mlm”:</dt><dd><p>preprocessor = “bert_multi_cased_preprocess_3”</p>
</dd>
<dt>elif model_type == “twitter_bert_base_en_uncased_mlm”:</dt><dd><p>preprocessor = “bert_en_uncased_preprocess_3”</p>
</dd>
<dt>else:</dt><dd><p>raise NotImplementedError</p>
</dd>
</dl>
<p>copy_cmd = “””mkdir {local_dir}</p>
</dd>
</dl>
<p>gsutil cp -r …
gsutil cp -r …”””</p>
<blockquote>
<div><dl class="simple">
<dt>execute_command(</dt><dd><p>copy_cmd.format(model_type=model_type, preprocessor=preprocessor, local_dir=LOCAL_MODEL_DIR)</p>
</dd>
</dl>
<p>)</p>
<p>return preprocessor</p>
</div></blockquote>
<dl>
<dt>def load_encoder(model_type, trainable):</dt><dd><dl>
<dt>try:</dt><dd><dl class="simple">
<dt>model = TextEncoder(</dt><dd><p>max_seq_lengths=MAX_SEQ_LENGTH,
model_type=model_type,
cluster=”gcp”,
trainable=trainable,
enable_dynamic_shapes=True,</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>except (OSError, tf.errors.AbortedError) as e:</dt><dd><p>print(e)
preprocessor = _locally_copy_models(model_type)</p>
<dl class="simple">
<dt>model = TextEncoder(</dt><dd><p>max_seq_lengths=MAX_SEQ_LENGTH,
local_model_path=f”models/{model_type}”,
local_preprocessor_path=f”models/{preprocessor}”,
cluster=”gcp”,
trainable=trainable,
enable_dynamic_shapes=True,</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
<p>return model</p>
</dd>
<dt>def get_loss(loss_name, from_logits, <a href="#id5"><span class="problematic" id="id6">**</span></a>kwargs):</dt><dd><p>loss_name = loss_name.lower()
if loss_name == “bce”:</p>
<blockquote>
<div><p>print(“Binary CE loss”)
return tf.keras.losses.BinaryCrossentropy(from_logits=from_logits)</p>
</div></blockquote>
<dl>
<dt>if loss_name == “cce”:</dt><dd><p>print(“Categorical cross-entropy loss”)
return tf.keras.losses.CategoricalCrossentropy(from_logits=from_logits)</p>
</dd>
<dt>if loss_name == “scce”:</dt><dd><p>print(“Sparse categorical cross-entropy loss”)
return tf.keras.losses.SparseCategoricalCrossentropy(from_logits=from_logits)</p>
</dd>
<dt>if loss_name == “focal_bce”:</dt><dd><p>gamma = kwargs.get(“gamma”, 2)
print(“Focal binary CE loss”, gamma)
return tf.keras.losses.BinaryFocalCrossentropy(gamma=gamma, from_logits=from_logits)</p>
</dd>
<dt>if loss_name == ‘masked_bce’:</dt><dd><p>multitask = kwargs.get(“multitask”, False)
if from_logits or multitask:</p>
<blockquote>
<div><p>raise NotImplementedError</p>
</div></blockquote>
<p>print(f’Masked Binary Cross Entropy’)
return MaskedBCE()</p>
</dd>
<dt>if loss_name == “inv_kl_loss”:</dt><dd><p>raise NotImplementedError</p>
</dd>
<dt>raise ValueError(</dt><dd><p>f”This loss name is not valid: {loss_name}. Accepted loss names: BCE, masked BCE, CCE, sCCE, ”
f”Focal_BCE, inv_KL_loss”</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>def _add_additional_embedding_layer(doc_embedding, glorot, seed):</dt><dd><p>doc_embedding = tf.keras.layers.Dense(768, activation=”tanh”, kernel_initializer=glorot)(doc_embedding)
doc_embedding = tf.keras.layers.Dropout(rate=0.1, seed=seed)(doc_embedding)
return doc_embedding</p>
</dd>
<dt>def _get_bias(<a href="#id7"><span class="problematic" id="id8">**</span></a>kwargs):</dt><dd><p>smart_bias_value = kwargs.get(‘smart_bias_value’, 0)
print(‘Smart bias init to ‘, smart_bias_value)
output_bias = tf.keras.initializers.Constant(smart_bias_value)
return output_bias</p>
</dd>
<dt>def load_inhouse_bert(model_type, trainable, seed, <a href="#id9"><span class="problematic" id="id10">**</span></a>kwargs):</dt><dd><p>inputs = tf.keras.layers.Input(shape=(), dtype=tf.string)
encoder = load_encoder(model_type=model_type, trainable=trainable)
doc_embedding = encoder([inputs])[“pooled_output”]
doc_embedding = tf.keras.layers.Dropout(rate=0.1, seed=seed)(doc_embedding)</p>
<p>glorot = tf.keras.initializers.glorot_uniform(seed=seed)
if kwargs.get(“additional_layer”, False):</p>
<blockquote>
<div><p>doc_embedding = _add_additional_embedding_layer(doc_embedding, glorot, seed)</p>
</div></blockquote>
<dl>
<dt>if kwargs.get(‘content_num_classes’, None):</dt><dd><p>probs = get_last_layer(glorot=glorot, last_layer_name=’target_output’, <a href="#id11"><span class="problematic" id="id12">**</span></a>kwargs)(doc_embedding)
second_probs = get_last_layer(num_classes=kwargs[‘content_num_classes’],</p>
<blockquote>
<div><p>last_layer_name=’content_output’,
glorot=glorot)(doc_embedding)</p>
</div></blockquote>
<p>probs = [probs, second_probs]</p>
</dd>
<dt>else:</dt><dd><p>probs = get_last_layer(glorot=glorot, <a href="#id13"><span class="problematic" id="id14">**</span></a>kwargs)(doc_embedding)</p>
</dd>
</dl>
<p>model = tf.keras.models.Model(inputs=inputs, outputs=probs)</p>
<p>return model, False</p>
</dd>
<dt>def get_last_layer(<a href="#id15"><span class="problematic" id="id16">**</span></a>kwargs):</dt><dd><p>output_bias = _get_bias(<a href="#id17"><span class="problematic" id="id18">**</span></a>kwargs)
if ‘glorot’ in kwargs:</p>
<blockquote>
<div><p>glorot = kwargs[‘glorot’]</p>
</div></blockquote>
<dl class="simple">
<dt>else:</dt><dd><p>glorot = tf.keras.initializers.glorot_uniform(seed=kwargs[‘seed’])</p>
</dd>
</dl>
<p>layer_name = kwargs.get(‘last_layer_name’, ‘dense_1’)</p>
<dl>
<dt>if kwargs.get(‘num_classes’, 1) &gt; 1:</dt><dd><dl class="simple">
<dt>last_layer = tf.keras.layers.Dense(</dt><dd><p>kwargs[“num_classes”], activation=”softmax”, kernel_initializer=glorot,
bias_initializer=output_bias, name=layer_name</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>elif kwargs.get(‘num_raters’, 1) &gt; 1:</dt><dd><dl class="simple">
<dt>if kwargs.get(‘multitask’, False):</dt><dd><p>raise NotImplementedError</p>
</dd>
<dt>last_layer = tf.keras.layers.Dense(</dt><dd><p>kwargs[‘num_raters’], activation=”sigmoid”, kernel_initializer=glorot,
bias_initializer=output_bias, name=’probs’)</p>
</dd>
</dl>
</dd>
<dt>else:</dt><dd><dl class="simple">
<dt>last_layer = tf.keras.layers.Dense(</dt><dd><p>1, activation=”sigmoid”, kernel_initializer=glorot,
bias_initializer=output_bias, name=layer_name</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
<p>return last_layer</p>
</dd>
<dt>def load_bertweet(<a href="#id19"><span class="problematic" id="id20">**</span></a>kwargs):</dt><dd><dl class="simple">
<dt>bert = TFAutoModelForSequenceClassification.from_pretrained(</dt><dd><p>os.path.join(LOCAL_MODEL_DIR, “bertweet-base”),
num_labels=1,
classifier_dropout=0.1,
hidden_size=768,</p>
</dd>
</dl>
<p>)
if “num_classes” in kwargs and kwargs[“num_classes”] &gt; 2:</p>
<blockquote>
<div><p>raise NotImplementedError</p>
</div></blockquote>
<p>return bert, True</p>
</dd>
<dt>def load(</dt><dd><p>optimizer,
seed,
model_type=”twitter_multilingual_bert_base_cased_mlm”,
loss_name=”BCE”,
trainable=True,
<a href="#id21"><span class="problematic" id="id22">**</span></a>kwargs,</p>
</dd>
<dt>):</dt><dd><dl class="simple">
<dt>if model_type == “bertweet-base”:</dt><dd><p>model, from_logits = load_bertweet()</p>
</dd>
<dt>else:</dt><dd><p>model, from_logits = load_inhouse_bert(model_type, trainable, seed, <a href="#id23"><span class="problematic" id="id24">**</span></a>kwargs)</p>
</dd>
</dl>
<p>pr_auc = tf.keras.metrics.AUC(curve=”PR”, name=”pr_auc”, from_logits=from_logits)
roc_auc = tf.keras.metrics.AUC(curve=”ROC”, name=”roc_auc”, from_logits=from_logits)</p>
<p>loss = get_loss(loss_name, from_logits, <a href="#id25"><span class="problematic" id="id26">**</span></a>kwargs)
if kwargs.get(‘content_num_classes’, None):</p>
<blockquote>
<div><p>second_loss = get_loss(loss_name=kwargs[‘content_loss_name’], from_logits=from_logits)
loss_weights = {‘content_output’: kwargs[‘content_loss_weight’], ‘target_output’: 1}
model.compile(</p>
<blockquote>
<div><p>optimizer=optimizer,
loss={‘content_output’: second_loss, ‘target_output’: loss},
loss_weights=loss_weights,
metrics=[pr_auc, roc_auc],</p>
</div></blockquote>
<p>)</p>
</div></blockquote>
<dl>
<dt>else:</dt><dd><dl class="simple">
<dt>model.compile(</dt><dd><p>optimizer=optimizer,
loss=loss,
metrics=[pr_auc, roc_auc],</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
<p>print(model.summary(), “logits: “, from_logits)</p>
<p>return model</p>
</dd>
</dl>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index2.rst.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index2.rst.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../_sources/trust_and_safety_models/toxicity/load_model.py.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>