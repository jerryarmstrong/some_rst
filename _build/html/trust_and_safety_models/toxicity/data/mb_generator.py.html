<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>from importlib import import_module
import os</p>
<dl class="simple">
<dt>from toxicity_ml_pipeline.settings.default_settings_tox import (</dt><dd><p>INNER_CV,
LOCAL_DIR,
MAX_SEQ_LENGTH,
NUM_PREFETCH,
NUM_WORKERS,
OUTER_CV,
TARGET_POS_PER_EPOCH,</p>
</dd>
</dl>
<p>)
from toxicity_ml_pipeline.utils.helpers import execute_command</p>
<p>import numpy as np
import pandas
from sklearn.model_selection import StratifiedKFold
import tensorflow as tf</p>
<dl>
<dt>try:</dt><dd><p>from transformers import AutoTokenizer, DataCollatorWithPadding</p>
</dd>
<dt>except ModuleNotFoundError:</dt><dd><p>print(”…”)</p>
</dd>
<dt>else:</dt><dd><p>from datasets import Dataset</p>
</dd>
<dt>class BalancedMiniBatchLoader(object):</dt><dd><dl>
<dt>def __init__(</dt><dd><p>self,
fold,
mb_size,
seed,
perc_training_tox,
scope=”TOX”,
project=…,
dual_head=None,
n_outer_splits=None,
n_inner_splits=None,
sample_weights=None,
huggingface=False,</p>
</dd>
<dt>):</dt><dd><dl class="simple">
<dt>if 0 &gt;= perc_training_tox or perc_training_tox &gt; 0.5:</dt><dd><p>raise ValueError(“Perc_training_tox should be in ]0; 0.5]”)</p>
</dd>
</dl>
<p>self.perc_training_tox = perc_training_tox
if not n_outer_splits:</p>
<blockquote>
<div><p>n_outer_splits = OUTER_CV</p>
</div></blockquote>
<dl>
<dt>if isinstance(n_outer_splits, int):</dt><dd><p>self.n_outer_splits = n_outer_splits
self.get_outer_fold = self._get_outer_cv_fold
if fold &lt; 0 or fold &gt;= self.n_outer_splits or int(fold) != fold:</p>
<blockquote>
<div><p>raise ValueError(f”Number of fold should be an integer in [0 ; {self.n_outer_splits} [.”)</p>
</div></blockquote>
</dd>
<dt>elif n_outer_splits == “time”:</dt><dd><p>self.get_outer_fold = self._get_time_fold
if fold != “time”:</p>
<blockquote>
<div><dl class="simple">
<dt>raise ValueError(</dt><dd><p>“To avoid repeating the same run many times, the external fold”
“should be time when test data is split according to dates.”</p>
</dd>
</dl>
<p>)</p>
</div></blockquote>
<dl class="simple">
<dt>try:</dt><dd><p>setting_file = import_module(f”toxicity_ml_pipeline.settings.{scope.lower()}{project}_settings”)</p>
</dd>
<dt>except ModuleNotFoundError:</dt><dd><p>raise ValueError(f”You need to define a setting file for your project {project}.”)</p>
</dd>
</dl>
<p>self.test_begin_date = setting_file.TEST_BEGIN_DATE
self.test_end_date = setting_file.TEST_END_DATE</p>
</dd>
<dt>else:</dt><dd><dl class="simple">
<dt>raise ValueError(</dt><dd><p>f”Argument n_outer_splits should either an integer or ‘time’. Provided: {n_outer_splits}”</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
<p>self.n_inner_splits = n_inner_splits if n_inner_splits is not None else INNER_CV</p>
<p>self.seed = seed
self.mb_size = mb_size
self.fold = fold</p>
<p>self.sample_weights = sample_weights
self.dual_head = dual_head
self.huggingface = huggingface
if self.huggingface:</p>
<blockquote>
<div><p>self._load_tokenizer()</p>
</div></blockquote>
</dd>
<dt>def _load_tokenizer(self):</dt><dd><p>print(“Making a local copy of Bertweet-base model”)
local_model_dir = os.path.join(LOCAL_DIR, “models”)
cmd = f”mkdir {local_model_dir} ; gsutil -m cp -r gs://… {local_model_dir}”
execute_command(cmd)</p>
<dl class="simple">
<dt>self.tokenizer = AutoTokenizer.from_pretrained(</dt><dd><p>os.path.join(local_model_dir, “bertweet-base”), normalization=True</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>def tokenize_function(self, el):</dt><dd><dl class="simple">
<dt>return self.tokenizer(</dt><dd><p>el[“text”],
max_length=MAX_SEQ_LENGTH,
padding=”max_length”,
truncation=True,
add_special_tokens=True,
return_token_type_ids=False,
return_attention_mask=False,</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>def _get_stratified_kfold(self, n_splits):</dt><dd><p>return StratifiedKFold(shuffle=True, n_splits=n_splits, random_state=self.seed)</p>
</dd>
<dt>def _get_time_fold(self, df):</dt><dd><p>test_begin_date = pandas.to_datetime(self.test_begin_date).date()
test_end_date = pandas.to_datetime(self.test_end_date).date()
print(f”Test is going from {test_begin_date} to {test_end_date}.”)
test_data = df.query(“&#64;test_begin_date &lt;= date &lt;= &#64;test_end_date”)</p>
<p>query = “date &lt; &#64;test_begin_date”
other_set = df.query(query)
return other_set, test_data</p>
</dd>
<dt>def _get_outer_cv_fold(self, df):</dt><dd><p>labels = df.int_label
stratifier = self._get_stratified_kfold(n_splits=self.n_outer_splits)</p>
<p>k = 0
for train_index, test_index in stratifier.split(np.zeros(len(labels)), labels):</p>
<blockquote>
<div><dl class="simple">
<dt>if k == self.fold:</dt><dd><p>break</p>
</dd>
</dl>
<p>k += 1</p>
</div></blockquote>
<p>train_data = df.iloc[train_index].copy()
test_data = df.iloc[test_index].copy()</p>
<p>return train_data, test_data</p>
</dd>
<dt>def get_steps_per_epoch(self, nb_pos_examples):</dt><dd><p>return int(max(TARGET_POS_PER_EPOCH, nb_pos_examples) / self.mb_size / self.perc_training_tox)</p>
</dd>
<dt>def make_huggingface_tensorflow_ds(self, group, mb_size=None, shuffle=True):</dt><dd><p>huggingface_ds = Dataset.from_pandas(group).map(self.tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer, return_tensors=”tf”)
tensorflow_ds = huggingface_ds.to_tf_dataset(</p>
<blockquote>
<div><p>columns=[“input_ids”],
label_cols=[“labels”],
shuffle=shuffle,
batch_size=self.mb_size if mb_size is None else mb_size,
collate_fn=data_collator,</p>
</div></blockquote>
<p>)</p>
<dl class="simple">
<dt>if shuffle:</dt><dd><p>return tensorflow_ds.repeat()</p>
</dd>
</dl>
<p>return tensorflow_ds</p>
</dd>
<dt>def make_pure_tensorflow_ds(self, df, nb_samples):</dt><dd><p>buffer_size = nb_samples * 2</p>
<dl>
<dt>if self.sample_weights is not None:</dt><dd><dl class="simple">
<dt>if self.sample_weights not in df.columns:</dt><dd><p>raise ValueError</p>
</dd>
<dt>ds = tf.data.Dataset.from_tensor_slices(</dt><dd><p>(df.text.values, df.label.values, df[self.sample_weights].values)</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>elif self.dual_head:</dt><dd><p>label_d = {f’{e}_output’: df[f’{e}_label’].values for e in self.dual_head}
label_d[‘content_output’] = tf.keras.utils.to_categorical(label_d[‘content_output’], num_classes=3)
ds = tf.data.Dataset.from_tensor_slices((df.text.values, label_d))</p>
</dd>
<dt>else:</dt><dd><p>ds = tf.data.Dataset.from_tensor_slices((df.text.values, df.label.values))</p>
</dd>
</dl>
<p>ds = ds.shuffle(buffer_size, seed=self.seed, reshuffle_each_iteration=True).repeat()
return ds</p>
</dd>
<dt>def get_balanced_dataset(self, training_data, size_limit=None, return_as_batch=True):</dt><dd><p>training_data = training_data.sample(frac=1, random_state=self.seed)
nb_samples = training_data.shape[0] if not size_limit else size_limit</p>
<p>num_classes = training_data.int_label.nunique()
toxic_class = training_data.int_label.max()
if size_limit:</p>
<blockquote>
<div><p>training_data = training_data[: size_limit * num_classes]</p>
</div></blockquote>
<dl>
<dt>print(</dt><dd><dl class="simple">
<dt>“…. {} examples, incl. {:.2f}% tox in train, {} classes”.format(</dt><dd><p>nb_samples,
100 * training_data[training_data.int_label == toxic_class].shape[0] / nb_samples,
num_classes,</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
<p>)
label_groups = training_data.groupby(“int_label”)
if self.huggingface:</p>
<blockquote>
<div><dl class="simple">
<dt>label_datasets = {</dt><dd><p>label: self.make_huggingface_tensorflow_ds(group) for label, group in label_groups</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<dl>
<dt>else:</dt><dd><dl class="simple">
<dt>label_datasets = {</dt><dd><p>label: self.make_pure_tensorflow_ds(group, nb_samples=nb_samples * 2)
for label, group in label_groups</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>datasets = [label_datasets[0], label_datasets[1]]
weights = [1 - self.perc_training_tox, self.perc_training_tox]
if num_classes == 3:</p>
<blockquote>
<div><p>datasets.append(label_datasets[2])
weights = [1 - self.perc_training_tox, self.perc_training_tox / 2, self.perc_training_tox / 2]</p>
</div></blockquote>
<dl class="simple">
<dt>elif num_classes != 2:</dt><dd><p>raise ValueError(“Currently it should not be possible to get other than 2 or 3 classes”)</p>
</dd>
</dl>
<p>resampled_ds = tf.data.experimental.sample_from_datasets(datasets, weights, seed=self.seed)</p>
<dl>
<dt>if return_as_batch and not self.huggingface:</dt><dd><dl class="simple">
<dt>return resampled_ds.batch(</dt><dd><p>self.mb_size, drop_remainder=True, num_parallel_calls=NUM_WORKERS, deterministic=True</p>
</dd>
</dl>
<p>).prefetch(NUM_PREFETCH)</p>
</dd>
</dl>
<p>return resampled_ds</p>
</dd>
</dl>
<p>&#64;staticmethod
def _compute_int_labels(full_df):</p>
<blockquote>
<div><dl>
<dt>if full_df.label.dtype == int:</dt><dd><p>full_df[“int_label”] = full_df.label</p>
</dd>
<dt>elif “int_label” not in full_df.columns:</dt><dd><dl class="simple">
<dt>if full_df.label.max() &gt; 1:</dt><dd><p>raise ValueError(“Binarizing labels that should not be.”)</p>
</dd>
</dl>
<p>full_df[“int_label”] = np.where(full_df.label &gt;= 0.5, 1, 0)</p>
</dd>
</dl>
<p>return full_df</p>
</div></blockquote>
<dl>
<dt>def __call__(self, full_df, <a href="#id1"><span class="problematic" id="id2">*</span></a>args, <a href="#id3"><span class="problematic" id="id4">**</span></a>kwargs):</dt><dd><p>full_df = self._compute_int_labels(full_df)</p>
<p>train_data, test_data = self.get_outer_fold(df=full_df)</p>
<p>stratifier = self._get_stratified_kfold(n_splits=self.n_inner_splits)
for train_index, val_index in stratifier.split(</p>
<blockquote>
<div><p>np.zeros(train_data.shape[0]), train_data.int_label</p>
</div></blockquote>
<dl>
<dt>):</dt><dd><p>curr_train_data = train_data.iloc[train_index]</p>
<p>mini_batches = self.get_balanced_dataset(curr_train_data)</p>
<dl class="simple">
<dt>steps_per_epoch = self.get_steps_per_epoch(</dt><dd><p>nb_pos_examples=curr_train_data[curr_train_data.int_label != 0].shape[0]</p>
</dd>
</dl>
<p>)</p>
<p>val_data = train_data.iloc[val_index].copy()</p>
<p>yield mini_batches, steps_per_epoch, val_data, test_data</p>
</dd>
</dl>
</dd>
<dt>def simple_cv_load(self, full_df):</dt><dd><p>full_df = self._compute_int_labels(full_df)</p>
<p>train_data, test_data = self.get_outer_fold(df=full_df)
if test_data.shape[0] == 0:</p>
<blockquote>
<div><p>test_data = train_data.iloc[:500]</p>
</div></blockquote>
<p>mini_batches = self.get_balanced_dataset(train_data)
steps_per_epoch = self.get_steps_per_epoch(</p>
<blockquote>
<div><p>nb_pos_examples=train_data[train_data.int_label != 0].shape[0]</p>
</div></blockquote>
<p>)</p>
<p>return mini_batches, test_data, steps_per_epoch</p>
</dd>
<dt>def no_cv_load(self, full_df):</dt><dd><p>full_df = self._compute_int_labels(full_df)</p>
<p>val_test = full_df[full_df.origin == “precision”].copy(deep=True)
val_data, test_data = self.get_outer_fold(df=val_test)</p>
<p>train_data = full_df.drop(full_df[full_df.origin == “precision”].index, axis=0)
if test_data.shape[0] == 0:</p>
<blockquote>
<div><p>test_data = train_data.iloc[:500]</p>
</div></blockquote>
<p>mini_batches = self.get_balanced_dataset(train_data)
if train_data.int_label.nunique() == 1:</p>
<blockquote>
<div><p>raise ValueError(‘Should be at least two labels’)</p>
</div></blockquote>
<p>num_examples = train_data[train_data.int_label == 1].shape[0]
if train_data.int_label.nunique() &gt; 2:</p>
<blockquote>
<div><p>second_most_frequent_label = train_data.loc[train_data.int_label != 0, ‘int_label’].mode().values[0]
num_examples = train_data[train_data.int_label == second_most_frequent_label].shape[0] * 2</p>
</div></blockquote>
<p>steps_per_epoch = self.get_steps_per_epoch(nb_pos_examples=num_examples)</p>
<p>return mini_batches, steps_per_epoch, val_data, test_data</p>
</dd>
</dl>
</dd>
</dl>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../index2.rst.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index2.rst.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../_sources/trust_and_safety_models/toxicity/data/mb_generator.py.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>