<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../../../" id="documentation_options" src="../../../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>import argparse
import logging
import os
import pkgutil
import sys
from urllib.parse import urlsplit</p>
<p>import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
import faiss</p>
<dl>
<dt>def parse_d6w_config(argv=None):</dt><dd><p>“””Parse d6w config.
:param argv: d6w config
:return: dictionary containing d6w config
“””</p>
<dl class="simple">
<dt>parser = argparse.ArgumentParser(</dt><dd><p>description=”See <a class="reference external" href="https://docbird.twitter.biz/d6w/model.html">https://docbird.twitter.biz/d6w/model.html</a> for any parameters inherited from d6w job config”</p>
</dd>
</dl>
<p>)
parser.add_argument(”–job_name”, dest=”job_name”, required=True, help=”d6w attribute”)
parser.add_argument(”–project”, dest=”project”, required=True, help=”d6w attribute”)
parser.add_argument(</p>
<blockquote>
<div><p>“–staging_location”, dest=”staging_location”, required=True, help=”d6w attribute”</p>
</div></blockquote>
<p>)
parser.add_argument(”–temp_location”, dest=”temp_location”, required=True, help=”d6w attribute”)
parser.add_argument(</p>
<blockquote>
<div><p>“–output_location”,
dest=”output_location”,
required=True,
help=”GCS bucket and path where resulting artifacts are uploaded”,</p>
</div></blockquote>
<p>)
parser.add_argument(</p>
<blockquote>
<div><p>“–service_account_email”, dest=”service_account_email”, required=True, help=”d6w attribute”</p>
</div></blockquote>
<p>)
parser.add_argument(</p>
<blockquote>
<div><p>“–factory_string”,
dest=”factory_string”,
required=False,
help=”FAISS factory string describing index to build. See <a class="reference external" href="https://github.com/facebookresearch/faiss/wiki/The-index-factory">https://github.com/facebookresearch/faiss/wiki/The-index-factory</a>”,</p>
</div></blockquote>
<p>)
parser.add_argument(</p>
<blockquote>
<div><p>“–metric”,
dest=”metric”,
required=True,
help=”Metric used to compute distance between embeddings. Valid values are ‘l2’, ‘ip’, ‘l1’, ‘linf’”,</p>
</div></blockquote>
<p>)
parser.add_argument(</p>
<blockquote>
<div><p>“–use_gpu”,
dest=”gpu”,
required=True,
help=”–use_gpu=yes if you want to use GPU during index building”,</p>
</div></blockquote>
<p>)</p>
<p>known_args, unknown_args = parser.parse_known_args(argv)
d6w_config = vars(known_args)
d6w_config[“gpu”] = d6w_config[“gpu”].lower() == “yes”
d6w_config[“metric”] = parse_metric(d6w_config)</p>
<p>“””
WARNING: Currently, d6w (a Twitter tool used to deploy Dataflow jobs to GCP) and
PipelineOptions.for_dataflow_runner (a helper method in twitter.ml.common.apache_beam) do not
play nicely together. The helper method will overwrite some of the config specified in the d6w
file using the defaults in <a class="reference external" href="https://sourcegraph.twitter.biz/git.twitter.biz/source/-/blob/src/python/twitter/ml/common/apache_beam/__init__.py?L24">https://sourcegraph.twitter.biz/git.twitter.biz/source/-/blob/src/python/twitter/ml/common/apache_beam/__init__.py?L24</a>.’
However, the d6w output message will still report that the config specified in the d6w file was used.
“””
logging.warning(</p>
<blockquote>
<div><p>f”The following d6w config parameters will be overwritten by the defaults in ”
f”<a class="reference external" href="https://sourcegraph.twitter.biz/git.twitter.biz/source/-/blob/src/python/twitter/ml/common/apache_beam/__init__.py?L24n">https://sourcegraph.twitter.biz/git.twitter.biz/source/-/blob/src/python/twitter/ml/common/apache_beam/__init__.py?L24n</a>”
f”{str(unknown_args)}”</p>
</div></blockquote>
<p>)
return d6w_config</p>
</dd>
<dt>def get_bq_query():</dt><dd><p>“””
Query is expected to return rows with unique entityId
“””
return pkgutil.get_data(__name__, “bq.sql”).decode(“utf-8”)</p>
</dd>
<dt>def parse_metric(config):</dt><dd><p>metric_str = config[“metric”].lower()
if metric_str == “l2”:</p>
<blockquote>
<div><p>return faiss.METRIC_L2</p>
</div></blockquote>
<dl class="simple">
<dt>elif metric_str == “ip”:</dt><dd><p>return faiss.METRIC_INNER_PRODUCT</p>
</dd>
<dt>elif metric_str == “l1”:</dt><dd><p>return faiss.METRIC_L1</p>
</dd>
<dt>elif metric_str == “linf”:</dt><dd><p>return faiss.METRIC_Linf</p>
</dd>
<dt>else:</dt><dd><p>raise Exception(f”Unknown metric: {metric_str}”)</p>
</dd>
</dl>
</dd>
<dt>def run_pipeline(argv=[]):</dt><dd><p>config = parse_d6w_config(argv)
argv_with_extras = argv
if config[“gpu”]:</p>
<blockquote>
<div><p>argv_with_extras.extend([”–experiments”, “use_runner_v2”])
argv_with_extras.extend(</p>
<blockquote>
<div><p>[”–experiments”, “worker_accelerator=type:nvidia-tesla-t4;count:1;install-nvidia-driver”]</p>
</div></blockquote>
<p>)
argv_with_extras.extend(</p>
<blockquote>
<div><dl class="simple">
<dt>[</dt><dd><p>“–worker_harness_container_image”,
“gcr.io/twttr-recos-ml-prod/dataflow-gpu/beam2_39_0_py3_7”,</p>
</dd>
</dl>
<p>]</p>
</div></blockquote>
<p>)</p>
</div></blockquote>
<p>options = PipelineOptions(argv_with_extras)
output_bucket_name = urlsplit(config[“output_location”]).netloc</p>
<dl>
<dt>with beam.Pipeline(options=options) as p:</dt><dd><dl class="simple">
<dt>input_data = p | “Read from BigQuery” &gt;&gt; beam.io.ReadFromBigQuery(</dt><dd><p>method=beam.io.ReadFromBigQuery.Method.DIRECT_READ,
query=get_bq_query(),
use_standard_sql=True,</p>
</dd>
</dl>
<p>)</p>
<dl>
<dt>index_built = input_data | “Build and upload index” &gt;&gt; beam.CombineGlobally(</dt><dd><dl class="simple">
<dt>MergeAndBuildIndex(</dt><dd><p>output_bucket_name,
config[“output_location”],
config[“factory_string”],
config[“metric”],
config[“gpu”],</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
<p>)</p>
<p># Make linter happy
index_built</p>
</dd>
</dl>
</dd>
<dt>class MergeAndBuildIndex(beam.CombineFn):</dt><dd><dl>
<dt>def __init__(self, bucket_name, gcs_output_path, factory_string, metric, gpu):</dt><dd><p>self.bucket_name = bucket_name
self.gcs_output_path = gcs_output_path
self.factory_string = factory_string
self.metric = metric
self.gpu = gpu</p>
</dd>
<dt>def create_accumulator(self):</dt><dd><p>return []</p>
</dd>
<dt>def add_input(self, accumulator, element):</dt><dd><p>accumulator.append(element)
return accumulator</p>
</dd>
<dt>def merge_accumulators(self, accumulators):</dt><dd><p>merged = []
for accum in accumulators:</p>
<blockquote>
<div><p>merged.extend(accum)</p>
</div></blockquote>
<p>return merged</p>
</dd>
<dt>def extract_output(self, rows):</dt><dd><p># Reimports are needed on workers
import glob
import subprocess</p>
<p>import faiss
from google.cloud import storage
import numpy as np</p>
<p>client = storage.Client()
bucket = client.get_bucket(self.bucket_name)</p>
<p>logging.info(“Building FAISS index”)
logging.info(f”There are {len(rows)} rows”)</p>
<p>ids = np.array([x[“entityId”] for x in rows]).astype(“long”)
embeds = np.array([x[“embedding”] for x in rows]).astype(“float32”)
dimensions = len(embeds[0])
N = ids.shape[0]
logging.info(f”There are {dimensions} dimensions”)</p>
<dl>
<dt>if self.factory_string is None:</dt><dd><p>M = 48</p>
<p>divideable_dimensions = (dimensions // M) * M
if divideable_dimensions != dimensions:</p>
<blockquote>
<div><p>opq_prefix = f”OPQ{M}_{divideable_dimensions}”</p>
</div></blockquote>
<dl class="simple">
<dt>else:</dt><dd><p>opq_prefix = f”OPQ{M}”</p>
</dd>
</dl>
<p>clusters = N // 20
self.factory_string = f”{opq_prefix},IVF{clusters},PQ{M}”</p>
</dd>
</dl>
<p>logging.info(f”Factory string is {self.factory_string}, metric={self.metric}”)</p>
<dl>
<dt>if self.gpu:</dt><dd><p>logging.info(“Using GPU”)</p>
<p>res = faiss.StandardGpuResources()
cpu_index = faiss.index_factory(dimensions, self.factory_string, self.metric)
cpu_index = faiss.IndexIDMap(cpu_index)
gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)
gpu_index.train(embeds)
gpu_index.add_with_ids(embeds, ids)
cpu_index = faiss.index_gpu_to_cpu(gpu_index)</p>
</dd>
<dt>else:</dt><dd><p>logging.info(“Using CPU”)</p>
<p>cpu_index = faiss.index_factory(dimensions, self.factory_string, self.metric)
cpu_index = faiss.IndexIDMap(cpu_index)
cpu_index.train(embeds)
cpu_index.add_with_ids(embeds, ids)</p>
</dd>
</dl>
<p>logging.info(“Built faiss index”)</p>
<p>local_path = “/indices”
logging.info(f”Writing indices to local {local_path}”)
subprocess.run(f”mkdir -p {local_path}”.strip().split())
local_index_path = os.path.join(local_path, “result.index”)</p>
<p>faiss.write_index(cpu_index, local_index_path)
logging.info(f”Done writing indices to local {local_path}”)</p>
<p>logging.info(f”Uploading to GCS with path {self.gcs_output_path}”)
assert os.path.isdir(local_path)
for local_file in glob.glob(local_path + “/<a href="#id1"><span class="problematic" id="id2">*</span></a>”):</p>
<blockquote>
<div><dl class="simple">
<dt>remote_path = os.path.join(</dt><dd><p>self.gcs_output_path.split(“/”)[-1], local_file[1 + len(local_path) :]</p>
</dd>
</dl>
<p>)
blob = bucket.blob(remote_path)
blob.upload_from_filename(local_file)</p>
</div></blockquote>
</dd>
</dl>
</dd>
<dt>if __name__ == “__main__”:</dt><dd><p>logging.getLogger().setLevel(logging.INFO)
run_pipeline(sys.argv)</p>
</dd>
</dl>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../../index2.rst.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../../index2.rst.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../../../_sources/ann/src/main/python/dataflow/faiss_index_bq_dataset.py.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>