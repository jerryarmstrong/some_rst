<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../_static/doctools.js?v=888ff710"></script>
    <script src="../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>import os
import time
from typing import Any, Dict, List, Optional</p>
<p>from tml.ml_logging.torch_logging import logging
from tml.common.filesystem import infer_fs, is_gcs_fs</p>
<p>import torchsnapshot</p>
<p>DONE_EVAL_SUBDIR = “evaled_by”
GCS_PREFIX = “gs://”</p>
<dl>
<dt>class Snapshot:</dt><dd><p>“””Checkpoints using torchsnapshot.</p>
<p>Also saves step to be updated by the training loop.</p>
<p>“””</p>
<dl class="simple">
<dt>def __init__(self, save_dir: str, state: Dict[str, Any]) -&gt; None:</dt><dd><p>self.save_dir = save_dir
self.state = state
self.state[“extra_state”] = torchsnapshot.StateDict(step=0, walltime=0.0)</p>
</dd>
</dl>
<p>&#64;property
def step(self):</p>
<blockquote>
<div><p>return self.state[“extra_state”][“step”]</p>
</div></blockquote>
<p>&#64;step.setter
def step(self, step: int) -&gt; None:</p>
<blockquote>
<div><p>self.state[“extra_state”][“step”] = step</p>
</div></blockquote>
<p>&#64;property
def walltime(self):</p>
<blockquote>
<div><p>return self.state[“extra_state”][“walltime”]</p>
</div></blockquote>
<p>&#64;walltime.setter
def walltime(self, walltime: float) -&gt; None:</p>
<blockquote>
<div><p>self.state[“extra_state”][“walltime”] = walltime</p>
</div></blockquote>
<dl>
<dt>def save(self, global_step: int) -&gt; “PendingSnapshot”:</dt><dd><p>“””Saves checkpoint with given global_step.”””
path = os.path.join(self.save_dir, str(global_step))
logging.info(f”Saving snapshot global_step {global_step} to {path}.”)
start_time = time.time()
# Take a snapshot in async manner, the snapshot is consistent that state changes after this method returns have no effect on the snapshot. It performs storage I/O in the background.
snapshot = torchsnapshot.Snapshot.async_take(</p>
<blockquote>
<div><p>app_state=self.state,
path=path,
# commented out because DistributedModelParallel model saving
# errors with this on multi-GPU. With it removed, CPU, single
# GPU, and multi-GPU training all successfully checkpoint.
# replicated=[“**”],</p>
</div></blockquote>
<p>)
logging.info(f”Snapshot saved to {snapshot.path} ({time.time() - start_time:.05}s”)
return snapshot</p>
</dd>
<dt>def restore(self, checkpoint: str) -&gt; None:</dt><dd><p>“””Restores a given checkpoint.”””
snapshot = torchsnapshot.Snapshot(path=checkpoint)
logging.info(f”Restoring snapshot from {snapshot.path}.”)
start_time = time.time()
# We can remove the try-except when we are confident that we no longer need to restore from
# checkpoints from before walltime was added
try:</p>
<blockquote>
<div><p># checkpoints that do not have extra_state[walltime] will fail here
snapshot.restore(self.state)</p>
</div></blockquote>
<dl class="simple">
<dt>except RuntimeError:</dt><dd><p># extra_state[walltime] does not exist in the checkpoint, but step should be there so restore it
self.state[“extra_state”] = torchsnapshot.StateDict(step=0)
snapshot.restore(self.state)
# we still need to ensure that extra_state has walltime in it
self.state[“extra_state”] = torchsnapshot.StateDict(step=self.step, walltime=0.0)</p>
</dd>
</dl>
<p>logging.info(f”Restored snapshot from {snapshot.path}. ({time.time() - start_time:.05}s”)</p>
</dd>
</dl>
<p>&#64;classmethod
def get_torch_snapshot(</p>
<blockquote>
<div><p>cls,
snapshot_path: str,
global_step: Optional[int] = None,
missing_ok: bool = False,</p>
</div></blockquote>
<dl>
<dt>) -&gt; torchsnapshot.Snapshot:</dt><dd><p>“””Get torch stateless snapshot, without actually loading it.
Args:</p>
<blockquote>
<div><p>snapshot_path: path to the model snapshot
global_step: restores from this checkpoint if specified.
missing_ok: if True and checkpoints do not exist, returns without restoration.</p>
</div></blockquote>
<p>“””
path = get_checkpoint(snapshot_path, global_step, missing_ok)
logging.info(f”Loading snapshot from {path}.”)
return torchsnapshot.Snapshot(path=path)</p>
</dd>
</dl>
<p>&#64;classmethod
def load_snapshot_to_weight(</p>
<blockquote>
<div><p>cls,
embedding_snapshot: torchsnapshot.Snapshot,
snapshot_emb_name: str,
weight_tensor,</p>
</div></blockquote>
<dl>
<dt>) -&gt; None:</dt><dd><dl class="simple">
<dt>“””Loads pretrained embedding from the snapshot to the model.</dt><dd><p>Utilise partial lodaing meachanism from torchsnapshot.</p>
</dd>
<dt>Args:</dt><dd><p>embedding_snapshot: Path to the snapshot containing pretrained embeddings (EBC).
snapshot_emb_name: Name of the layer in the <em>snapshot</em> model, containing the EBC.
weight_tensor: embeddings tensor of <em>current</em> model, where the embeddings will be loaded.</p>
</dd>
</dl>
<p>“””
start_time = time.time()
manifest = embedding_snapshot.get_manifest()
for path in manifest.keys():</p>
<blockquote>
<div><dl class="simple">
<dt>if path.startswith(“0”) and snapshot_emb_name in path:</dt><dd><p>snapshot_path_to_load = path</p>
</dd>
</dl>
</div></blockquote>
<p>embedding_snapshot.read_object(snapshot_path_to_load, weight_tensor)
logging.info(</p>
<blockquote>
<div><p>f”Loaded embedding snapshot from {snapshot_path_to_load}: {time.time() - start_time:.05}s”,
rank=-1,</p>
</div></blockquote>
<p>)
logging.info(f”Snapshot loaded to {weight_tensor.metadata()}”, rank=-1)</p>
</dd>
</dl>
</dd>
<dt>def _eval_subdir(checkpoint_path: str) -&gt; str:</dt><dd><p>return os.path.join(checkpoint_path, DONE_EVAL_SUBDIR)</p>
</dd>
<dt>def _eval_done_path(checkpoint_path: str, eval_partition: str) -&gt; str:</dt><dd><p>return os.path.join(_eval_subdir(checkpoint_path), f”{eval_partition}_DONE”)</p>
</dd>
<dt>def is_done_eval(checkpoint_path: str, eval_partition: str):</dt><dd><p>return get_checkpoint(checkpoint_path).exists(_eval_done_path(checkpoint_path, eval_partition))</p>
</dd>
<dt>def mark_done_eval(checkpoint_path: str, eval_partition: str):</dt><dd><p>infer_fs(checkpoint_path).touch(_eval_done_path(checkpoint_path, eval_partition))</p>
</dd>
<dt>def step_from_checkpoint(checkpoint: str) -&gt; int:</dt><dd><p>return int(os.path.basename(checkpoint))</p>
</dd>
<dt>def checkpoints_iterator(save_dir: str, seconds_to_sleep: int = 30, timeout: int = 1800):</dt><dd><p>“””Simplified equivalent of tf.train.checkpoints_iterator.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>seconds_to_sleep: time between polling calls.
timeout: how long to wait for a new checkpoint.</p>
</dd>
</dl>
<p>“””</p>
<dl>
<dt>def _poll(last_checkpoint: Optional[str] = None):</dt><dd><p>stop_time = time.time() + timeout
while True:</p>
<blockquote>
<div><p>_checkpoint_path = get_checkpoint(save_dir, missing_ok=True)
if not _checkpoint_path or _checkpoint_path == last_checkpoint:</p>
<blockquote>
<div><dl>
<dt>if time.time() + seconds_to_sleep &gt; stop_time:</dt><dd><dl class="simple">
<dt>logging.info(</dt><dd><p>f”Timed out waiting for next available checkpoint from {save_dir} for {timeout}s.”</p>
</dd>
</dl>
<p>)
return None</p>
</dd>
</dl>
<p>logging.info(f”Waiting for next available checkpoint from {save_dir}.”)
time.sleep(seconds_to_sleep)</p>
</div></blockquote>
<dl class="simple">
<dt>else:</dt><dd><p>logging.info(f”Found latest checkpoint {_checkpoint_path}.”)
return _checkpoint_path</p>
</dd>
</dl>
</div></blockquote>
</dd>
</dl>
<p>checkpoint_path = None
while True:</p>
<blockquote>
<div><p>new_checkpoint = _poll(checkpoint_path)
if not new_checkpoint:</p>
<blockquote>
<div><p>return</p>
</div></blockquote>
<p>checkpoint_path = new_checkpoint
yield checkpoint_path</p>
</div></blockquote>
</dd>
<dt>def get_checkpoint(</dt><dd><p>save_dir: str,
global_step: Optional[int] = None,
missing_ok: bool = False,</p>
</dd>
<dt>) -&gt; str:</dt><dd><p>“””Gets latest checkpoint or checkpoint at specified global_step.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>global_step: Finds this checkpoint if specified.
missing_ok: if True and checkpoints do not exist, returns without restoration.</p>
</dd>
</dl>
<p>“””
checkpoints = get_checkpoints(save_dir)
if not checkpoints:</p>
<blockquote>
<div><dl class="simple">
<dt>if not missing_ok:</dt><dd><p>raise Exception(f”No checkpoints found at {save_dir}”)</p>
</dd>
<dt>else:</dt><dd><p>logging.info(f”No checkpoints found for restoration at {save_dir}.”)
return “”</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>if global_step is None:</dt><dd><p>return checkpoints[-1]</p>
</dd>
</dl>
<p>logging.info(f”Found checkpoints: {checkpoints}”)
for checkpoint in checkpoints:</p>
<blockquote>
<div><p>step = step_from_checkpoint(checkpoint)
if global_step == step:</p>
<blockquote>
<div><p>chosen_checkpoint = checkpoint
break</p>
</div></blockquote>
</div></blockquote>
<dl class="simple">
<dt>else:</dt><dd><p>raise Exception(f”Desired checkpoint at {global_step} not found in {save_dir}”)</p>
</dd>
</dl>
<p>return chosen_checkpoint</p>
</dd>
<dt>def get_checkpoints(save_dir: str) -&gt; List[str]:</dt><dd><p>“””Gets all checkpoints that have been fully written.”””
checkpoints = []
fs = infer_fs(save_dir)
if fs.exists(save_dir):</p>
<blockquote>
<div><p>prefix = GCS_PREFIX if is_gcs_fs(fs) else “”
checkpoints = list(f”{prefix}{elem}” for elem in fs.ls(save_dir, detail=False))
# Only take checkpoints that were fully written.
checkpoints = list(</p>
<blockquote>
<div><dl class="simple">
<dt>filter(</dt><dd><p>lambda path: fs.exists(f”{path}/{torchsnapshot.snapshot.SNAPSHOT_METADATA_FNAME}”),
checkpoints,</p>
</dd>
</dl>
<p>)</p>
</div></blockquote>
<p>)
checkpoints = sorted(checkpoints, key=lambda path: int(os.path.basename(path)))</p>
</div></blockquote>
<p>return checkpoints</p>
</dd>
<dt>def wait_for_evaluators(</dt><dd><p>save_dir: str,
partition_names: List[str],
global_step: int,
timeout: int,</p>
</dd>
<dt>) -&gt; None:</dt><dd><p>logging.info(“Waiting for all evaluators to finish.”)
start_time = time.time()</p>
<dl>
<dt>for checkpoint in checkpoints_iterator(save_dir):</dt><dd><p>step = step_from_checkpoint(checkpoint)
logging.info(f”Considering checkpoint {checkpoint} for global step {global_step}.”)
if step == global_step:</p>
<blockquote>
<div><dl>
<dt>while partition_names:</dt><dd><dl>
<dt>if is_done_eval(checkpoint, partition_names[-1]):</dt><dd><dl class="simple">
<dt>logging.info(</dt><dd><p>f”Checkpoint {checkpoint} marked as finished eval for partition {partition_names[-1]} at step {step}, still waiting for {partition_names}.”</p>
</dd>
</dl>
<p>)
partition_names.pop()</p>
</dd>
<dt>if time.time() - start_time &gt;= timeout:</dt><dd><dl class="simple">
<dt>logging.warning(</dt><dd><p>f”Not all evaluators finished after waiting for {time.time() - start_time}”</p>
</dd>
</dl>
<p>)
return</p>
</dd>
</dl>
<p>time.sleep(10)</p>
</dd>
</dl>
<p>logging.info(“All evaluators finished.”)
return</p>
</div></blockquote>
<dl class="simple">
<dt>if time.time() - start_time &gt;= timeout:</dt><dd><p>logging.warning(f”Not all evaluators finished after waiting for {time.time() - start_time}”)
return</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index2.rst.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index2.rst.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../_sources/common/checkpointing/snapshot.py.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>