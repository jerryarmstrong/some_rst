<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p># pylint: disable=no-member, attribute-defined-outside-init, too-many-instance-attributes
“””
Implementing PercentileDiscretizer Layer
“””</p>
<p>import libtwml
import numpy as np
import tensorflow.compat.v1 as tf
import twml
from twml.layers import Layer</p>
<dl>
<dt>class PercentileDiscretizer(Layer):</dt><dd><p>“””
PercentileDiscretizer layer is constructed by PercentileDiscretizerCalibrator after
accumulating data and performing percentile bucket calibration.</p>
<p>PercentileDiscretizer takes sparse continuous features and converts then to sparse
binary features. Each binary output feature is associated to an PercentileDiscretizer bin.
Each PercentileDiscretizer input feature is converted to n_bin bins.
Each PercentileDiscretizer calibration tries to find bin delimiters such
that the number of features values per bin is roughly equal (for
each given PercentileDiscretizer feature). In other words, bins are calibrated to be approx.
equiprobable, according to the given calibration data.
Note that if an input feature is rarely used, so will its associated output bin/features.
“””</p>
<dl>
<dt>def __init__(</dt><dd><blockquote>
<div><p>self,
n_feature, n_bin, out_bits,
bin_values=None, hash_keys=None, hash_values=None,
bin_ids=None, feature_offsets=None, num_parts=1, cost_per_unit=100, <a href="#id1"><span class="problematic" id="id2">**</span></a>kwargs):</p>
</div></blockquote>
<p>“””
Creates a non-initialized <cite>PercentileDiscretizer</cite> object.
Before using the table you will have to initialize it. After initialization
the table will be immutable.</p>
<p>If there are no calibrated features, then the discretizer will only apply
twml.util.limit_bits to the the feature keys (aka “feature_ids”). Essentially,
the discretizer will be a “no-operation”, other than obeying <cite>out_bits</cite></p>
<dl>
<dt>Parent class args:</dt><dd><p>see [tf.layers.Layer](<a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/layers/Layer">https://www.tensorflow.org/api_docs/python/tf/layers/Layer</a>)
for documentation of parent class arguments.</p>
</dd>
<dt>Required args:</dt><dd><dl class="simple">
<dt>n_feature:</dt><dd><p>number of unique features accumulated during PercentileDiscretizer calibration.
This is the number of features in the hash map.
Used to initialize bin_values, hash_keys, hash_values,
bin_ids, bin_values and feature_offsets.</p>
</dd>
<dt>n_bin:</dt><dd><p>number of PercentileDiscretizer bins used for PercentileDiscretizer calibration.
Used to initialize bin_values, hash_keys, hash_values,
bin_ids, bin_values and feature_offsets.</p>
</dd>
<dt>out_bits:</dt><dd><p>Determines the maximum value for output feature IDs.
The dense_shape of the SparseTensor returned by lookup(x)
will be [x.shape[0], 1 &lt;&lt; output_bits].</p>
</dd>
</dl>
</dd>
<dt>Optional args:</dt><dd><dl>
<dt>hash_keys:</dt><dd><p>contains the features ID that PercentileDiscretizer discretizes and knows about.
The hash map (hash_keys-&gt;hash_values) is used for two reasons:</p>
<blockquote>
<div><p>1. divide inputs into two feature spaces:
PercentileDiscretizer vs non-PercentileDiscretizer
2. transate the PercentileDiscretizer features into a hash_feature ID that
PercentileDiscretizer understands.</p>
</div></blockquote>
<p>The hash_map is expected to contain n_feature items.</p>
</dd>
<dt>hash_values:</dt><dd><p>translates the feature IDs into hash_feature IDs for PercentileDiscretizer.</p>
</dd>
<dt>bin_ids:</dt><dd><p>a 1D Tensor of size n_feature * n_bin + 1 which contains
unique IDs to which the PercentileDiscretizer features will be translated to.
For example, tf.Tensor(np.arange(n_feature * n_bin)) would produce
the most efficient output space.</p>
</dd>
<dt>bin_values:</dt><dd><p>a 1D Tensor aligned with bin_ids.
For a given hash_feature ID j, it’s value bin’s are indexed between
<cite>j*n_bin</cite> and <cite>j*n_bin + n_bin-1</cite>.
As such, bin_ids[j*n_bin+i] is translated from a hash_feature ID of j
and a inputs value between
<cite>bin_values[j*n_bin + i]</cite> and <cite>bin_values[j*n_bin+i+1]</cite>.</p>
</dd>
<dt>feature_offsets:</dt><dd><p>a 1D Tensor specifying the starting location of bins for a given feature id.
For example, tf.Tensor(np.arange(0, bin_values.size, n_bin, dtype=’int64’)).</p>
</dd>
</dl>
</dd>
</dl>
<p>“””</p>
<p>super(PercentileDiscretizer, self).__init__(<a href="#id3"><span class="problematic" id="id4">**</span></a>kwargs)</p>
<dl class="simple">
<dt>if not self.built:</dt><dd><p>self.build(input_shape=None)</p>
</dd>
</dl>
<p>max_discretizer_feature = n_feature * (n_bin + 1)
self._n_feature = n_feature
self._n_bin = n_bin</p>
<p># build variables
self._out_bits = out_bits
self._output_size = tf.convert_to_tensor(1 &lt;&lt; out_bits, tf.int64)
self._hash_keys = (hash_keys if hash_keys is not None else</p>
<blockquote>
<div><p>np.empty(n_feature, dtype=np.int64))</p>
</div></blockquote>
<dl class="simple">
<dt>self._hash_values = (hash_values if hash_values is not None else</dt><dd><p>np.empty(n_feature, dtype=np.int64))</p>
</dd>
<dt>self._bin_ids = (bin_ids if bin_ids is not None else</dt><dd><p>np.empty(max_discretizer_feature, dtype=np.int64))</p>
</dd>
<dt>self._bin_values = (bin_values if bin_values is not None else</dt><dd><p>np.empty(max_discretizer_feature, dtype=np.float32))</p>
</dd>
<dt>self._feature_offsets = (feature_offsets if feature_offsets is not None else</dt><dd><p>np.empty(n_feature, dtype=np.int64))</p>
</dd>
</dl>
<p>self.num_parts = num_parts
self.cost_per_unit = cost_per_unit</p>
</dd>
<dt>def build(self, input_shape):  # pylint: disable=unused-argument</dt><dd><p>“””
Creates the variables of the layer
“””
self.built = True</p>
</dd>
<dt>def call(self, inputs, keep_inputs=False, <a href="#id5"><span class="problematic" id="id6">**</span></a>kwargs):</dt><dd><p>“””Looks up <cite>keys</cite> in a table, outputs the corresponding values.</p>
<p>Implements PercentileDiscretizer inference where inputs are intersected with a hash_map.
Input features that were not calibrated have their feature IDs truncated, so as
to be less than 1&lt;&lt;output_bits, but their values remain untouched (not discretized)</p>
<p>If there are no calibrated features, then the discretizer will only apply
twml.util.limit_bits to the the feature keys (aka “feature_ids”). Essentially,
the discretizer will be a “no-operation”, other than obeying <cite>out_bits</cite></p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>inputs: A 2D SparseTensor that is input to PercentileDiscretizer for discretization.</dt><dd><p>It has a dense_shape of [batch_size, input_size]</p>
</dd>
<dt>keep_inputs:</dt><dd><p>Include the original inputs in the output.
Note - if True, undiscretized features will be passed through, but will have
their values doubled (unless there are no calibrated features to discretize).</p>
</dd>
</dl>
<p>name: A name for the operation (optional).</p>
</dd>
<dt>Returns:</dt><dd><p>A <cite>SparseTensor</cite> of the same type as <cite>inputs</cite>.
Its dense_shape is [shape_input.dense_shape[0], 1 &lt;&lt; output_bits].</p>
</dd>
</dl>
<p>“””</p>
<dl class="simple">
<dt>if isinstance(inputs, tf.SparseTensor):</dt><dd><p>inputs = twml.SparseTensor.from_tf(inputs)</p>
</dd>
</dl>
<p>assert(isinstance(inputs, twml.SparseTensor))</p>
<p># sparse column indices
ids = inputs.ids
# sparse row indices
keys = inputs.indices
# sparse values
vals = inputs.values</p>
<dl>
<dt>if self._n_feature &gt; 0:</dt><dd><dl class="simple">
<dt>discretizer_keys, discretizer_vals = libtwml.ops.percentile_discretizer_v2(</dt><dd><p>input_ids=keys,  # inc key assigned to feature_id, or -1
input_vals=vals,  # the observed feature values
bin_ids=self._bin_ids,  # n_feat X (n_bin+1) 2D arange
bin_vals=self._bin_values,  # bin boundaries
feature_offsets=self._feature_offsets,  # 0 : nbin_1 : max_feat
output_bits=self._out_bits,
feature_ids=tf.make_tensor_proto(self._hash_keys),  # feature ids to build internal hash map
feature_indices=tf.make_tensor_proto(self._hash_values),  # keys associated w/ feat. indices
start_compute=tf.constant(0, shape=[], dtype=tf.int64),
end_compute=tf.constant(-1, shape=[], dtype=tf.int64),
cost_per_unit=self.cost_per_unit</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>else:</dt><dd><p>discretizer_keys = twml.util.limit_bits(keys, self._out_bits)
discretizer_vals = vals
# don’t 2x the input.
keep_inputs = False</p>
</dd>
</dl>
<p>batch_size = tf.to_int64(inputs.dense_shape[0])
output_shape = [batch_size, self._output_size]</p>
<p>output = twml.SparseTensor(ids, discretizer_keys, discretizer_vals, output_shape).to_tf()</p>
<dl>
<dt>if keep_inputs:</dt><dd><p># Note the non-discretized features will end up doubled,
#   since these are already in <cite>output</cite>
# handle output ID conflicts
mdl_size = self._n_feature * (self._n_bin + 1)
non_mdl_size = tf.subtract(self._output_size, mdl_size)
input_keys = tf.add(tf.floormod(keys, non_mdl_size), mdl_size)</p>
<dl class="simple">
<dt>new_input = twml.SparseTensor(</dt><dd><p>ids=ids, indices=input_keys, values=vals, dense_shape=output_shape).to_tf()</p>
</dd>
</dl>
<p># concatenate discretizer output with original input
sparse_add = tf.sparse_add(new_input, output)
output = tf.SparseTensor(sparse_add.indices, sparse_add.values, output_shape)</p>
</dd>
</dl>
<p>return output</p>
</dd>
<dt>def compute_output_shape(self, input_shape):</dt><dd><p>“””Computes the output shape of the layer given the input shape.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>input_shape: A (possibly nested tuple of) <cite>TensorShape</cite>.  It need not</dt><dd><p>be fully defined (e.g. the batch size may be unknown).</p>
</dd>
</dl>
</dd>
</dl>
<p>Raises NotImplementedError.</p>
<p>“””
raise NotImplementedError</p>
</dd>
</dl>
</dd>
</dl>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../index.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../_sources/twml/twml/layers/percentile_discretizer.py.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>