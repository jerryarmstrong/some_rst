<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../_static/doctools.js?v=888ff710"></script>
    <script src="../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p># pylint: disable=too-many-branches
“”” This module includes functions for managing learning rate decay “””
import tensorflow.compat.v1 as tf</p>
<dl>
<dt>def get_learning_rate_decay_fn(params):</dt><dd><p>“””
Returns a learning rate decay function that takes the initial
learning_rate and global_step
as arguments and returns the current learning rate.</p>
<p>Currently supports params.learning_rate_decay values of:
exponential | polynomial | piecewise_constant | cosine | cosine restarts.
See <a class="reference external" href="https://www.tensorflow.org/api_guides/python/train#Decaying_the_learning_rate">Decaying the Leanring Rate</a> for details.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>params:</dt><dd><p>a tensorflow.contrib.train.HParams object containing the relevant hyperparameters.</p>
</dd>
</dl>
</dd>
</dl>
<p>“””
paramsv = params.values()
if ‘learning_rate_decay’ not in paramsv or params.learning_rate_decay == ‘no_learning_rate_decay’:</p>
<blockquote>
<div><p>return None</p>
</div></blockquote>
<dl>
<dt>elif params.learning_rate_decay == ‘exponential_learning_rate_decay’:</dt><dd><dl>
<dt>if ‘decay_steps’ not in paramsv:</dt><dd><dl class="simple">
<dt>raise ValueError(“Expecting params.decay_steps for “</dt><dd><p>“params.learning_rate_decay == ‘exponential’”)</p>
</dd>
</dl>
</dd>
<dt>if ‘exponential_decay_rate’ not in paramsv:</dt><dd><dl class="simple">
<dt>raise ValueError(“Expecting params.exponential_decay_rate for “</dt><dd><p>“params.learning_rate_decay == ‘exponential’”)</p>
</dd>
</dl>
</dd>
<dt>def exponential_decay_fn(learning_rate, global_step):</dt><dd><p>“”” exponential decay function to be passed to optimize_loss “””
return tf.train.exponential_decay(</p>
<blockquote>
<div><p>learning_rate=learning_rate,
global_step=global_step,
decay_steps=params.decay_steps,
decay_rate=params.exponential_decay_rate</p>
</div></blockquote>
<p>)</p>
</dd>
</dl>
<p>return exponential_decay_fn</p>
</dd>
<dt>elif params.learning_rate_decay == ‘piecewise_constant_learning_rate_decay’:</dt><dd><dl class="simple">
<dt>if ‘piecewise_constant_boundaries’ not in paramsv:</dt><dd><dl class="simple">
<dt>raise ValueError(“Expecting params.piecewise_constant_boundaries for “</dt><dd><p>“params.learning_rate_decay == ‘piecewise_constant’”)</p>
</dd>
</dl>
</dd>
<dt>if ‘piecewise_constant_values’ not in paramsv:</dt><dd><dl class="simple">
<dt>raise ValueError(“Expecting params.piecewise_constant_values for “</dt><dd><p>“params.learning_rate_decay == ‘piecewise_constant’”)</p>
</dd>
</dl>
</dd>
</dl>
<p># pylint: disable=unused-argument</p>
<dl>
<dt>def piecewise_constant_fn(learning_rate, global_step):</dt><dd><p>“”” piecewise_constant decay function to be passed to optimize_loss “””
return tf.train.piecewise_constant(</p>
<blockquote>
<div><p>x=global_step,
boundaries=params.piecewise_constant_boundaries,
values=params.piecewise_constant_values</p>
</div></blockquote>
<p>)</p>
</dd>
</dl>
<p>return piecewise_constant_fn</p>
</dd>
<dt>elif params.learning_rate_decay == ‘polynomial_learning_rate_decay’:</dt><dd><dl>
<dt>if ‘decay_steps’ not in paramsv:</dt><dd><dl class="simple">
<dt>raise ValueError(“Expecting params.decay_steps for “</dt><dd><p>“params.learning_rate_decay == ‘polynomial’”)</p>
</dd>
</dl>
</dd>
<dt>if ‘end_learning_rate’ not in paramsv:</dt><dd><dl class="simple">
<dt>raise ValueError(“Expecting params.end_learning_rate for “</dt><dd><p>“params.learning_rate_decay == ‘polynomial’”)</p>
</dd>
</dl>
</dd>
<dt>def polynomial_decay_fn(learning_rate, global_step):</dt><dd><p>“”” polynomial decay function to be passed to optimize_loss “””
return tf.train.polynomial_decay(</p>
<blockquote>
<div><p>learning_rate=learning_rate,
global_step=global_step,
decay_steps=params.decay_steps,
end_learning_rate=params.end_learning_rate,
power=params.polynomial_power if ‘polynomial_power’ in paramsv else 1.0,</p>
</div></blockquote>
<p>)</p>
</dd>
</dl>
<p>return polynomial_decay_fn</p>
</dd>
<dt>elif params.learning_rate_decay == ‘inverse_learning_rate_decay’:</dt><dd><dl>
<dt>if ‘min_learning_rate’ not in paramsv:</dt><dd><dl class="simple">
<dt>raise ValueError(“Expecting params.min_learning_rate for “</dt><dd><p>“params.learning_rate_decay == ‘inverse’”)</p>
</dd>
</dl>
</dd>
<dt>if ‘decay_rate’ not in paramsv:</dt><dd><dl class="simple">
<dt>raise ValueError(“Expecting params.decay_rate for “</dt><dd><p>“params.learning_rate_decay == ‘inverse’”)</p>
</dd>
</dl>
</dd>
<dt>if ‘decay_steps’ not in paramsv:</dt><dd><dl class="simple">
<dt>raise ValueError(“Expecting params.decay_steps for “</dt><dd><p>“params.learning_rate_decay == ‘inverse’”)</p>
</dd>
</dl>
</dd>
<dt>def bounded_inverse_time_decay_fn(learning_rate, global_step):</dt><dd><p>‘’’
Returns the decayed learning_rate by applying the function:
decayed_lr = max(lr /(1 + decay_rate * floor(global_step /decay_step)),</p>
<blockquote>
<div><p>min_learning_rate)</p>
</div></blockquote>
<dl>
<dt>Arguments:</dt><dd><dl class="simple">
<dt>learning_rate:</dt><dd><p>A scalar <cite>float32</cite> or <cite>float64</cite> <cite>Tensor</cite> or a Python number.
The initial learning rate.</p>
</dd>
<dt>global_step:</dt><dd><p>A scalar <cite>int32</cite> or <cite>int64</cite> <cite>Tensor</cite> or a Python number.
Global step to use for the decay computation.  Must not be negative.</p>
</dd>
<dt>min_learning_rate:</dt><dd><p>A scalar <cite>int32</cite> or <cite>int64</cite> <cite>Tensor</cite> or a Python number.
Minimum possible learning_rate. The decayed learning_rate will not be
smaller than the min_learning_rate</p>
</dd>
<dt>decay_steps:</dt><dd><p>How often to apply decay. In dbv1, this should be 1.</p>
</dd>
<dt>decay_rate:</dt><dd><p>A scalar <cite>int32</cite> or <cite>int64</cite> <cite>Tensor</cite> or a Python number.
Rate in which we decay the learning rate.</p>
</dd>
</dl>
<p>Returns:
A scalar <cite>Tensor</cite> of the same type as <cite>learning_rate</cite>.  The decayed
learning rate.</p>
</dd>
</dl>
<p>‘’’
decayed_rate = tf.train.inverse_time_decay(</p>
<blockquote>
<div><p>learning_rate=learning_rate,
global_step=global_step,
decay_steps=params.decay_steps,
decay_rate=params.decay_rate)</p>
</div></blockquote>
<p># Getting dtype of returned Tensor
dtype = decayed_rate.dtype
# Casting the min_learning rate the same dtype as decayes rate
min_learning_rate = tf.cast(params.min_learning_rate, dtype)
# Returning the maximum between the two
return tf.maximum(decayed_rate, min_learning_rate)</p>
</dd>
</dl>
<p>return bounded_inverse_time_decay_fn</p>
</dd>
<dt>elif params.learning_rate_decay == ‘cosine_learning_rate_decay’:</dt><dd><dl>
<dt>if ‘decay_steps’ not in paramsv:</dt><dd><dl class="simple">
<dt>raise ValueError(“Expecting params.decay_steps for “</dt><dd><p>“params.learning_rate_decay == ‘cosine_decay’”)</p>
</dd>
</dl>
</dd>
<dt>if “alpha” not in paramsv:</dt><dd><dl class="simple">
<dt>raise ValueError(“Expecting params.alpha for “</dt><dd><p>“params.learning_rate_decay == ‘cosine_decay’”)</p>
</dd>
</dl>
</dd>
<dt>def cosine_decay_fn(learning_rate, global_step):</dt><dd><p>“”” cosine decay function to be passed to optimize_loss “””
return tf.train.cosine_decay(</p>
<blockquote>
<div><p>learning_rate=learning_rate,
global_step=global_step,
decay_steps=params.decay_steps,
alpha=params.alpha</p>
</div></blockquote>
<p>)</p>
</dd>
</dl>
<p>return cosine_decay_fn</p>
</dd>
<dt>elif params.learning_rate_decay == ‘cosine_restarts_learning_rate_decay’:</dt><dd><dl>
<dt>if ‘first_decay_steps’ not in paramsv:</dt><dd><dl class="simple">
<dt>raise ValueError(“Expecting params.first_decay_steps for “</dt><dd><p>“params.learning_rate_decay == ‘cosine_restarts_decay’”)</p>
</dd>
</dl>
</dd>
<dt>if ‘t_mul’ not in paramsv:</dt><dd><dl class="simple">
<dt>raise ValueError(“Expecting params.t_mul for “</dt><dd><p>“params.learning_rate_decay == ‘cosine_restarts_decay’”)</p>
</dd>
</dl>
</dd>
<dt>if ‘m_mul’ not in paramsv:</dt><dd><dl class="simple">
<dt>raise ValueError(“Expecting params.m_mul for “</dt><dd><p>“params.learning_rate_decay == ‘cosine_restarts_decay’”)</p>
</dd>
</dl>
</dd>
<dt>if “alpha” not in paramsv:</dt><dd><dl class="simple">
<dt>raise ValueError(“Expecting params.alpha for “</dt><dd><p>“params.learning_rate_decay == ‘cosine_restarts_decay’”)</p>
</dd>
</dl>
</dd>
<dt>def cosine_restart_decay_fn(learning_rate, global_step):</dt><dd><p>“”” cosine decay function to be passed to optimize_loss “””
return tf.train.cosine_decay_restarts(</p>
<blockquote>
<div><p>learning_rate=learning_rate,
global_step=global_step,
first_decay_steps=params.first_decay_steps,
t_mul=params.t_mul,
m_mul=params.m_mul,
alpha=params.alpha</p>
</div></blockquote>
<p>)</p>
</dd>
</dl>
<p>return cosine_restart_decay_fn</p>
</dd>
</dl>
<p>raise ValueError(“Unsupported params.learning_rate_decay: %s” % params.learning_rate_decay)</p>
</dd>
</dl>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../index2.rst.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../index2.rst.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../_sources/twml/twml/learning_rate_decay.py.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>