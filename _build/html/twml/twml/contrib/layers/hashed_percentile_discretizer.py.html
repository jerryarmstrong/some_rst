<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p># pylint: disable=no-member, attribute-defined-outside-init, too-many-instance-attributes
“””
Implementing HashedPercentileDiscretizer Layer
“””</p>
<dl class="simple">
<dt>from twitter.deepbird.util.hashing import (</dt><dd><p>integer_multiplicative_hashing_uniform,
integer_multiplicative_hashing,</p>
</dd>
</dl>
<p>)  # noqa: F401</p>
<p>from libtwml import percentile_discretizer_bin_indices
import numpy as np
import tensorflow.compat.v1 as tf
import twml
from twml.layers.layer import Layer
from twml.layers.partition import Partition
from twml.layers.stitch import Stitch</p>
<dl>
<dt>class HashedPercentileDiscretizer(Layer):</dt><dd><p>“””
HashedPercentileDiscretizer layer is constructed by PercentileDiscretizerCalibrator
after accumulating data
and performing minimum description length (PercentileDiscretizer) calibration.</p>
<p>HashedPercentileDiscretizer takes sparse continuous features and converts then to sparse
binary features. Each binary output feature is associated to an HashedPercentileDiscretizer
bin.
Each HashedPercentileDiscretizer input feature is converted to n_bin bins.
Each HashedPercentileDiscretizer calibration tries to find bin delimiters such
that the number of features values
per bin is roughly equal (for each given HashedPercentileDiscretizer feature).
Note that if an input feature is rarely used, so will its associated output bin/features.
The difference between this layer and PercentileDiscretizer is that the
DeterministicPercentileDiscretize always assigns the same output id in the SparseTensor to the
same input feature id + bin. This is useful if you want to user transfer learning on pre-trained
sparse to dense embedding layers, but re-calibrate your discretizer on newer data.
“””</p>
<dl>
<dt>def __init__(self, n_feature, n_bin, out_bits,</dt><dd><blockquote>
<div><p>bin_values=None, hash_keys=None, hash_values=None,
bin_ids=None, feature_offsets=None,
hash_fn=integer_multiplicative_hashing_uniform, <a href="#id1"><span class="problematic" id="id2">**</span></a>kwargs):</p>
</div></blockquote>
<p>“””
Creates a non-initialized <cite>HashedPercentileDiscretizer</cite> object.
Before using the table you will have to initialize it. After initialization
the table will be immutable.</p>
<dl>
<dt>Parent class args:</dt><dd><p>see [tf.layers.Layer](<a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/layers/Layer">https://www.tensorflow.org/api_docs/python/tf/layers/Layer</a>)
for documentation of parent class arguments.</p>
</dd>
<dt>Required args:</dt><dd><dl class="simple">
<dt>n_feature:</dt><dd><p>number of unique features accumulated during HashedPercentileDiscretizer calibration.
This is the number of features in the hash map.
Used to initialize bin_values, hash_keys, hash_values,
bin_ids, bin_values and feature_offsets.</p>
</dd>
<dt>n_bin:</dt><dd><p>number of HashedPercentileDiscretizer bins used for
HashedPercentileDiscretizer calibration. Used to initialize bin_values, hash_keys,
hash_values, bin_ids, bin_values and feature_offsets.</p>
</dd>
<dt>out_bits:</dt><dd><p>Determines the maximum value for output feature IDs.
The dense_shape of the SparseTensor returned by lookup(x)
will be [x.shape[0], 1 &lt;&lt; output_bits].</p>
</dd>
</dl>
</dd>
<dt>Optional args:</dt><dd><dl>
<dt>hash_keys:</dt><dd><p>contains the features ID that HashedPercentileDiscretizer discretizes and knows
about. The hash map (hash_keys-&gt;hash_values) is used for two reasons:</p>
<blockquote>
<div><p>1. divide inputs into two feature spaces:
HashedPercentileDiscretizer vs non-HashedPercentileDiscretizer
2. transate the HashedPercentileDiscretizer features into a hash_feature ID that
HashedPercentileDiscretizer understands.</p>
</div></blockquote>
<p>The hash_map is expected to contain n_feature items.</p>
</dd>
<dt>hash_values:</dt><dd><p>translates the feature IDs into hash_feature IDs for HashedPercentileDiscretizer.</p>
</dd>
<dt>bin_ids:</dt><dd><p>a 1D Tensor of size n_feature * n_bin + 1 which contains
unique IDs to which the HashedPercentileDiscretizer features will be translated to.
For example, tf.Tensor(np.arange(n_feature * n_bin)) would produce
the most efficient output space.</p>
</dd>
<dt>bin_values:</dt><dd><p>a 1D Tensor aligned with bin_ids.
For a given hash_feature ID j, it’s value bin’s are indexed between
<cite>j*n_bin</cite> and <cite>j*n_bin + n_bin-1</cite>.
As such, bin_ids[j*n_bin+i] is translated from a hash_feature ID of j
and a inputs value between
<cite>bin_values[j*n_bin + i]</cite> and <cite>bin_values[j*n_bin+i+1]</cite>.</p>
</dd>
<dt>feature_offsets:</dt><dd><p>a 1D Tensor specifying the starting location of bins for a given feature id.
For example, tf.Tensor(np.arange(0, bin_values.size, n_bin, dtype=’int64’)).</p>
</dd>
<dt>hash_fn:</dt><dd><p>a function that takes in <cite>feature_ids</cite>, <cite>bucket_indices</cite> and <cite>output_size</cite> and
hashes the bucketed features into the <cite>output_size</cite> buckets. The default uses knuth’s
multiplicative hashing</p>
</dd>
</dl>
</dd>
</dl>
<p>“””
super(HashedPercentileDiscretizer, self).__init__(<a href="#id3"><span class="problematic" id="id4">**</span></a>kwargs)</p>
<p>max_discretizer_feature = n_feature * (n_bin + 1)
self._n_feature = n_feature
self._n_bin = n_bin</p>
<dl class="simple">
<dt>if not self.built:</dt><dd><p>self.build(input_shape=None)</p>
</dd>
</dl>
<p># build variables
self.output_size = tf.convert_to_tensor(1 &lt;&lt; out_bits, tf.int64)
self._out_bits = out_bits</p>
<p>hash_keys = hash_keys
if hash_keys is None:</p>
<blockquote>
<div><p>hash_keys = np.empty(n_feature, dtype=np.int64)</p>
</div></blockquote>
<p>hash_values = hash_values
if hash_values is None:</p>
<blockquote>
<div><p>hash_values = np.empty(n_feature, dtype=np.int64)</p>
</div></blockquote>
<p>initializer = tf.lookup.KeyValueTensorInitializer(hash_keys, hash_values)
self.hash_map = tf.lookup.StaticHashTable(initializer, -1)
self.bin_ids = bin_ids
if bin_ids is None:</p>
<blockquote>
<div><p>bin_ids = np.empty(max_discretizer_feature, dtype=np.int64)</p>
</div></blockquote>
<p>self.bin_values = bin_values
if bin_values is None:</p>
<blockquote>
<div><p>bin_values = np.empty(max_discretizer_feature, dtype=np.float32)</p>
</div></blockquote>
<p>self.feature_offsets = feature_offsets
if feature_offsets is None:</p>
<blockquote>
<div><p>feature_offsets = np.empty(n_feature, dtype=np.int64)</p>
</div></blockquote>
<p>self.hash_fn = hash_fn</p>
</dd>
<dt>def build(self, input_shape):  # pylint: disable=unused-argument</dt><dd><p>“””
Creates the variables of the layer:
hash_keys, hash_values, bin_ids, bin_values, feature_offsets and self.output_size.
“””
# build layers
self.partition = Partition()
self.stitch = Stitch()
# make sure this is last
self.built = True</p>
</dd>
<dt>def call(self, inputs, <a href="#id5"><span class="problematic" id="id6">**</span></a>kwargs):</dt><dd><p>“””Looks up <cite>keys</cite> in a table, outputs the corresponding values.</p>
<p>Implements HashedPercentileDiscretizer inference where inputs are intersected with a
hash_map.
Part of the inputs are discretized using twml.discretizer
to produce a discretizer_output SparseTensor.
This SparseTensor is then joined with the original inputs SparseTensor,
but only for the inputs keys that did not get discretized.</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>inputs: A 2D SparseTensor that is input to HashedPercentileDiscretizer for</dt><dd><p>discretization. It has a dense_shape of [batch_size, input_size]</p>
</dd>
</dl>
<p>name: A name for the operation (optional).</p>
</dd>
<dt>Returns:</dt><dd><p>A <cite>SparseTensor</cite> of the same type as <cite>inputs</cite>.
Its dense_shape is [shape_input.dense_shape[0], 1 &lt;&lt; output_bits].</p>
</dd>
</dl>
<p>“””
if isinstance(inputs, tf.SparseTensor):</p>
<blockquote>
<div><p>inputs = twml.SparseTensor.from_tf(inputs)</p>
</div></blockquote>
<p>assert(isinstance(inputs, twml.SparseTensor))</p>
<p># sparse column indices
ids = inputs.ids
# sparse row indices
keys = inputs.indices
# sparse values
vals = inputs.values</p>
<p>hashed_keys = self.hash_map.lookup(keys)
hashed_keys = tf.cast(hashed_keys, tf.int64)</p>
<p>found = tf.not_equal(hashed_keys, tf.constant(-1, tf.int64))
partition_ids = tf.cast(found, tf.int32)</p>
<p>found = tf.reshape(found, [-1])
continuous_feature_ids = tf.boolean_mask(keys, found)</p>
<p>vals, key, indices = self.partition(partition_ids, vals, tf.where(found, hashed_keys, keys))
non_discretizer_keys, discretizer_in_keys = key
non_discretizer_vals, discretizer_in_vals = vals</p>
<p>non_discretizer_keys = twml.util.limit_bits(non_discretizer_keys, self._out_bits)
self.non_discretizer_keys = non_discretizer_keys</p>
<p># run HashedPercentileDiscretizer on the keys/values it knows about
output = percentile_discretizer_bin_indices(discretizer_in_keys,</p>
<blockquote>
<div><p>discretizer_in_vals,
self.bin_ids,
self.bin_values,
self.feature_offsets)</p>
</div></blockquote>
<p>discretizer_bucket_idxs, discretizer_vals = output
new_discretizer_keys = self.hash_fn(continuous_feature_ids, discretizer_bucket_idxs,</p>
<blockquote>
<div><p>self.output_size)</p>
</div></blockquote>
<p># Stitch the keys and values from discretizer and non discretizer indices back, with help
# of the Stitch Layer
self.discretizer_out_keys = new_discretizer_keys</p>
<dl class="simple">
<dt>concat_data = self.stitch([non_discretizer_vals, discretizer_vals],</dt><dd><p>[non_discretizer_keys, new_discretizer_keys],
indices)</p>
</dd>
</dl>
<p>concat_vals, concat_keys = concat_data</p>
<p># Generate output shape using _compute_output_shape</p>
<p>batch_size = tf.to_int64(inputs.dense_shape[0])
output_shape = [batch_size, self.output_size]
return twml.SparseTensor(ids, concat_keys, concat_vals, output_shape).to_tf()</p>
</dd>
</dl>
</dd>
</dl>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../index2.rst.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../index2.rst.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../../_sources/twml/twml/contrib/layers/hashed_percentile_discretizer.py.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>