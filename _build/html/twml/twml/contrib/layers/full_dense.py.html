<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p># pylint: disable=no-member,arguments-differ, attribute-defined-outside-init
“””
Implementing Full Dense Layer
“””
from twml.layers import Layer</p>
<p>import tensorflow.compat.v1 as tf
from tensorflow.python.layers import core</p>
<dl>
<dt>class FullDense(Layer):</dt><dd><p>“””
Full-connected, Dense input layer class.
This layer implements the operation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">weight</span> <span class="o">+</span> <span class="n">bias</span><span class="p">)</span>
</pre></div>
</div>
<p>Where <code class="docutils literal notranslate"><span class="pre">activation</span></code> is the activation function passed as the <code class="docutils literal notranslate"><span class="pre">activation</span></code>
argument (if not <code class="docutils literal notranslate"><span class="pre">None</span></code>), <code class="docutils literal notranslate"><span class="pre">weight</span></code> is a weights matrix created by the layer,
and <code class="docutils literal notranslate"><span class="pre">bias</span></code> is a bias vector created by the layer.</p>
<p>However, this layer breaks up <code class="docutils literal notranslate"><span class="pre">weight</span></code> into <code class="docutils literal notranslate"><span class="pre">num_partitions</span></code> parts,
for the purpose of even disribution of weights across parameter servers
for distributed training.</p>
<p>Note - This layer is created to allow distributed training optimizations,
but can also be used for single node training (e.g. hogwild) without
code modification</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>output_size:</dt><dd><p>Integer or Long, dimensionality of the output space.</p>
</dd>
<dt>weight_initializer:</dt><dd><p>Initializer function for the weight matrix.</p>
</dd>
<dt>weight_regularizer:</dt><dd><p>Regularizer function for the weight matrix.
Ensure to add tf.losses.get_regularization_loss() to your loss for this to take effect.</p>
</dd>
<dt>weight_constraint:</dt><dd><p>An optional projection function to be applied to the
weight after being updated by an <cite>Optimizer</cite> (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training.</p>
</dd>
<dt>bias_constraint:</dt><dd><p>An optional projection function to be applied to the
bias after being updated by an <cite>Optimizer</cite>.</p>
</dd>
<dt>num_partitions:</dt><dd><p>Number of pieces to partition the weights into. This layer does
column partitioning of the weights, which is equivalent to
processing the input tensor with multiple fully connected layers
of smaller output size, and then concatenating these outputs</p>
</dd>
<dt>activation:</dt><dd><p>Activation function (callable). Set it to None to maintain a linear activation.</p>
</dd>
<dt>use_bias:</dt><dd><p>Boolean whether to include a bias parameter in the layer</p>
</dd>
<dt>bias_initializer:</dt><dd><p>Initializer function for the bias.</p>
</dd>
<dt>bias_regularizer:</dt><dd><p>Regularizer function for the bias.
Ensure to add tf.losses.get_regularization_loss() to your loss for this to take effect.</p>
</dd>
<dt>activity_regularizer:</dt><dd><p>Regularizer function for the output.</p>
</dd>
<dt>trainable:</dt><dd><p>Boolean, if <cite>True</cite> also add variables to the graph collection
<code class="docutils literal notranslate"><span class="pre">GraphKeys.TRAINABLE_VARIABLES</span></code> (see <a class="reference external" href="https://www.tensorflow.org/versions/master/api_docs/python/tf/Variable">tf.Variable</a>).</p>
</dd>
<dt>name:</dt><dd><p>String, the name of the layer. Layers with the same name will
share weights, but to avoid mistakes we require <code class="docutils literal notranslate"><span class="pre">reuse=True</span></code> in such cases.</p>
</dd>
</dl>
</dd>
<dt>Properties:</dt><dd><dl class="simple">
<dt>output_size:</dt><dd><p>Python integer, dimensionality of the output space.</p>
</dd>
<dt>activation:</dt><dd><p>Activation function (callable).</p>
</dd>
<dt>weight_initializer:</dt><dd><p>Initializer instance (or name) for the weight matrix.</p>
</dd>
<dt>bias_initializer:</dt><dd><p>Initializer instance (or name) for the bias.</p>
</dd>
<dt>weights:</dt><dd><p>list of underlying weight and bias matrix components. no guarantee on order of elements</p>
</dd>
<dt>weight_regularizer:</dt><dd><p>Regularizer instance for the weight matrix (callable)</p>
</dd>
<dt>bias_regularizer:</dt><dd><p>Regularizer instance for the bias (callable).</p>
</dd>
<dt>activity_regularizer:</dt><dd><p>Regularizer instance for the output (callable)</p>
</dd>
<dt>weight_constraint:</dt><dd><p>Constraint function for the weight matrix.</p>
</dd>
<dt>bias_constraint:</dt><dd><p>Constraint function for the bias.</p>
</dd>
</dl>
</dd>
</dl>
<p>“””</p>
<dl>
<dt>def __init__(self, output_size,</dt><dd><blockquote>
<div><p>weight_initializer=None,
weight_regularizer=None,
weight_constraint=None,
bias_constraint=None,
num_partitions=3,
activation=None,
use_bias=True,
bias_initializer=tf.zeros_initializer(),
bias_regularizer=None,
activity_regularizer=None,
trainable=True,
name=None,
<a href="#id1"><span class="problematic" id="id2">**</span></a>kwargs):</p>
</div></blockquote>
<p>super(FullDense, self).__init__(trainable=trainable, name=name, <a href="#id3"><span class="problematic" id="id4">**</span></a>kwargs)
self._output_sizes = self._get_output_partition_sizes(output_size, num_partitions)
self._units = output_size
self._activation = activation
self._weight_initializer = weight_initializer
self._bias_initializer = bias_initializer
self._weight_regularizer = weight_regularizer
self._bias_regularizer = bias_regularizer
self._weight_constraint = weight_constraint
self._bias_constraint = bias_constraint
self._use_bias = use_bias
# NOTE - many initializers depend on fan_in and fan_out
#      - as such, initialization here may be different than
#      - for a non-partitioned FullDense
self._parts = [core.Dense(units=out_size,</p>
<blockquote>
<div><p>activation=activation,
use_bias=use_bias,
kernel_initializer=weight_initializer,
bias_initializer=bias_initializer,
kernel_regularizer=weight_regularizer,
bias_regularizer=bias_regularizer,
activity_regularizer=activity_regularizer,
kernel_constraint=weight_constraint,
bias_constraint=bias_constraint,
trainable=trainable,
name=name,
<a href="#id5"><span class="problematic" id="id6">**</span></a>kwargs) for out_size in self._output_sizes]</p>
</div></blockquote>
</dd>
</dl>
<p>&#64;staticmethod
def _get_output_partition_sizes(out_size, num_parts):</p>
<blockquote>
<div><p>“”” Returns the appropriate output sizes of the partitions “””
boundaries = [out_size * n // num_parts for n in range(num_parts + 1)]
return [k - j for j, k in zip(boundaries[:], boundaries[1:])]</p>
</div></blockquote>
<dl>
<dt>def build(self, input_shapes):</dt><dd><p>“”” Create the appropriately sized weights and biases in each layer partition “””
if isinstance(input_shapes, (list, tuple)):</p>
<blockquote>
<div><p>input_shape = input_shapes[0]
is_compatible = True
for other_shape in input_shapes[1:]:</p>
<blockquote>
<div><p>is_compatible &amp;= input_shape.is_compatible_with(other_shape)</p>
</div></blockquote>
<dl class="simple">
<dt>if not is_compatible:</dt><dd><p>raise ValueError(“Input shapes %s are not compatible.” % input_shapes)</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>else:</dt><dd><p>input_shape = input_shapes</p>
</dd>
<dt>for part in self._parts:</dt><dd><p>part.build(input_shape)</p>
</dd>
</dl>
<p>self.built = True</p>
</dd>
</dl>
<p>&#64;property
def units(self):</p>
<blockquote>
<div><p>“”” Returns the number of output units of the layer “””
return self._units</p>
</div></blockquote>
<p>&#64;property
def output_size(self):</p>
<blockquote>
<div><p>“”” Returns the number of output units of the layer “””
return self._units</p>
</div></blockquote>
<p>&#64;property
def activation(self):</p>
<blockquote>
<div><p>“”” Returns the activation function “””
return self._activation</p>
</div></blockquote>
<p>&#64;property
def weight_initializer(self):</p>
<blockquote>
<div><p>“”” Returns the weight_initializer “””
return self._weight_initializer</p>
</div></blockquote>
<p>&#64;property
def weight_regularizer(self):</p>
<blockquote>
<div><p>“”” Returns the weight_regularizer “””
return self._weight_regularizer</p>
</div></blockquote>
<p>&#64;property
def weight_constraint(self):</p>
<blockquote>
<div><p>“”” Returns the weight_constraint “””
return self._weight_constraint</p>
</div></blockquote>
<p>&#64;property
def bias_initializer(self):</p>
<blockquote>
<div><p>“”” Returns the bias_initializer “””
return self._bias_initializer</p>
</div></blockquote>
<p>&#64;property
def bias_regularizer(self):</p>
<blockquote>
<div><p>“”” Returns the bias_regularizer “””
return self._bias_regularizer</p>
</div></blockquote>
<p>&#64;property
def bias_constraint(self):</p>
<blockquote>
<div><p>“”” Returns the bias_constraint “””
return self._bias_constraint</p>
</div></blockquote>
<p>&#64;property
def use_bias(self):</p>
<blockquote>
<div><p>“”” Returns whether a bias is used in the layer “””
return self._use_bias</p>
</div></blockquote>
<p>&#64;property
def trainable_variables(self):</p>
<blockquote>
<div><p>“”” Returns the trainable variables of the layer “””
trainable_vars = []
for pt in self._parts:</p>
<blockquote>
<div><p>trainable_vars += pt.trainable_variables</p>
</div></blockquote>
<p>return trainable_vars</p>
</div></blockquote>
<p>&#64;property
def trainable_weights(self):</p>
<blockquote>
<div><p>“”” Returns the trainable variables of the layer “””
return self.trainable_variables</p>
</div></blockquote>
<p>&#64;property
def non_trainable_variables(self):</p>
<blockquote>
<div><p>“”” Returns the non-trainable variables of the layer “””
non_trainable_vars = []
for pt in self._parts:</p>
<blockquote>
<div><p>non_trainable_vars += pt.non_trainable_variables</p>
</div></blockquote>
<p>return non_trainable_vars</p>
</div></blockquote>
<p>&#64;property
def non_trainable_weights(self):</p>
<blockquote>
<div><p>“”” Returns the non-trainable variables of the layer “””
return self.non_trainable_variables</p>
</div></blockquote>
<p>&#64;property
def variables(self):</p>
<blockquote>
<div><p>“”” Returns a list of all weights and biases in this layer “””
layer_vars = []
for pt in self._parts:</p>
<blockquote>
<div><p>layer_vars += pt.weights</p>
</div></blockquote>
<p>return layer_vars</p>
</div></blockquote>
<p>&#64;property
def weights(self):</p>
<blockquote>
<div><p>“”” Returns a list of all weights and biases in this layer “””
return self.variables</p>
</div></blockquote>
<p>&#64;property
def dtype(self):</p>
<blockquote>
<div><p>“”” Returns the dtype of the layers weights “””
return self._parts[0].dtype</p>
</div></blockquote>
<dl>
<dt>def call(self, inputs, <a href="#id7"><span class="problematic" id="id8">**</span></a>kwargs):  # pylint: disable=unused-argument</dt><dd><p>“””The logic of the layer lives here.</p>
<dl>
<dt>Arguments:</dt><dd><dl class="simple">
<dt>inputs:</dt><dd><p>A dense Tensor or a list of such.
If <cite>inputs</cite> is a list, all tensors must have same <cite>dense_shape</cite>.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><ul class="simple">
<li><p>If <cite>inputs</cite> is <cite>SparseTensor</cite>, then returns <cite>bias + inputs * dense_b</cite>.</p></li>
<li><p>If <cite>inputs</cite> is a <cite>list[SparseTensor</cite>, then returns</p></li>
</ul>
<blockquote>
<div><p><cite>bias + accumulate_n([sp_a * dense_b for sp_a in inputs])</cite>.</p>
</div></blockquote>
</dd>
</dl>
<p>“””
if not isinstance(inputs, (list, tuple)):</p>
<blockquote>
<div><p>inputs = [inputs]</p>
</div></blockquote>
<p>outputs = []
for inp in inputs:</p>
<blockquote>
<div><p>part_outputs = [part(inp) for part in self._parts]
outputs.append(tf.concat(part_outputs, axis=-1))</p>
</div></blockquote>
<p>return tf.accumulate_n(outputs)</p>
</dd>
</dl>
</dd>
<dt>def full_dense(inputs, output_size,</dt><dd><blockquote>
<div><p>weight_initializer=None,
weight_regularizer=None,
weight_constraint=None,
bias_constraint=None,
num_partitions=3,
activation=None,
use_bias=True,
bias_initializer=tf.zeros_initializer(),
bias_regularizer=None,
activity_regularizer=None,
trainable=True,
name=None,
reuse=None,
<a href="#id9"><span class="problematic" id="id10">**</span></a>kwargs):</p>
</div></blockquote>
<p>“””Functional interface for the fully-connected dense-input layer.
This layer implements the operation:
<cite>outputs = activation(inputs.weight + bias)</cite>
Where <cite>activation</cite> is the activation function passed as the <cite>activation</cite>
argument (if not <cite>None</cite>), <cite>weight</cite> is a weights matrix created by the layer,
and <cite>bias</cite> is a bias vector created by the layer
(only if <cite>use_bias</cite> is <cite>True</cite>).</p>
<p>However, this layer breaks up <code class="docutils literal notranslate"><span class="pre">weight</span></code> into <code class="docutils literal notranslate"><span class="pre">num_partitions</span></code> parts,
for the purpose of even disribution of weights across parameter servers
for distributed training.</p>
<p>Note - This layer is created to allow distributed training optimizations,
but can also be used for single node training (e.g. hogwild) without
code modification</p>
<dl>
<dt>Arguments:</dt><dd><p>inputs: Tensor input.
output_size: Integer or Long, dimensionality of the output space.
weight_initializer: Initializer function for the weight matrix.</p>
<blockquote>
<div><p>If <cite>None</cite> (default), weights are initialized using the default
initializer used by <cite>tf.get_variable</cite>.</p>
</div></blockquote>
<dl class="simple">
<dt>weight_regularizer:</dt><dd><p>Regularizer function for the weight matrix.
Ensure to add tf.losses.get_regularization_loss() to your loss for this to take effect.</p>
</dd>
<dt>weight_constraint:</dt><dd><p>An optional projection function to be applied to the
weight after being updated by an <cite>Optimizer</cite> (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training.</p>
</dd>
<dt>bias_constraint:</dt><dd><p>An optional projection function to be applied to the
bias after being updated by an <cite>Optimizer</cite>.</p>
</dd>
<dt>num_partitions:</dt><dd><p>Number of pieces to partition the weights into. This layer does
column partitioning of the weights, which is equivalent to
processing the input tensor with multiple fully connected layers
of smaller output size, and then concatenating these outputs</p>
</dd>
<dt>activation: Activation function (callable). Set it to None to maintain a</dt><dd><p>linear activation.</p>
</dd>
</dl>
<p>use_bias: Boolean, whether the layer uses a bias.
bias_initializer:</p>
<blockquote>
<div><p>Initializer function for the bias.</p>
</div></blockquote>
<dl class="simple">
<dt>bias_regularizer:</dt><dd><p>Regularizer function for the bias.
Ensure to add tf.losses.get_regularization_loss() to your loss for this to take effect.</p>
</dd>
<dt>activity_regularizer:</dt><dd><p>Regularizer function for the output.</p>
</dd>
<dt>trainable:</dt><dd><p>Boolean, if <cite>True</cite> also add variables to the graph collection
<cite>GraphKeys.TRAINABLE_VARIABLES</cite> (see <cite>tf.Variable</cite>).</p>
</dd>
<dt>name:</dt><dd><p>String, the name of the layer.</p>
</dd>
<dt>reuse:</dt><dd><p>Boolean, whether to reuse the weights of a previous layer
by the same name.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>Output tensor with shape <cite>inputs.shape[:-1] + [output_size]</cite>.</p>
</dd>
</dl>
<p>“””
if not isinstance(inputs, (list, tuple)):</p>
<blockquote>
<div><p>inputs = [inputs]</p>
</div></blockquote>
<p>dtype = inputs[0].dtype.base_dtype</p>
<dl class="simple">
<dt>layer = FullDense(output_size=output_size,</dt><dd><p>weight_initializer=weight_initializer,
weight_regularizer=weight_regularizer,
weight_constraint=weight_constraint,
bias_constraint=bias_constraint,
num_partitions=num_partitions,
activation=activation,
use_bias=use_bias,
bias_initializer=bias_initializer,
bias_regularizer=bias_regularizer,
activity_regularizer=activity_regularizer,
trainable=trainable,
name=name,
dtype=dtype,
_scope=name,
_reuse=reuse,
<a href="#id11"><span class="problematic" id="id12">**</span></a>kwargs)</p>
</dd>
</dl>
<p>return layer(inputs)</p>
</dd>
</dl>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../index2.rst.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../index2.rst.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../../_sources/twml/twml/contrib/layers/full_dense.py.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>