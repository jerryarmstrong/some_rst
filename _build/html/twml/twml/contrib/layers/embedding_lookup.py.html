<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>import os
import re
import time</p>
<p>from collections import OrderedDict</p>
<p>from absl import logging
import numpy as np
import tensorflow.compat.v1 as tf
from tensorflow.python.ops.lookup_ops import index_table_from_tensor</p>
<p>import twml</p>
<p># Padding is 0, UNK is 1:
PAD_WORD_ID = 0
OOV_WORD_ID = 1</p>
<dl>
<dt>def load_initializers_from_csv(</dt><dd><p>embedding_path, vocab_size=-1, embedding_size=None, separator=None, vocab=None</p>
</dd>
<dt>):</dt><dd><p>“””
Loads embeddings saved in the <a class="reference external" href="https://nlp.stanford.edu/projects/glove/">glove format</a>.
The glove format is a txt file separated by spaces.
Each line looks like: “word 0.00001 0.2334 …”.</p>
<dl>
<dt>Arguments:</dt><dd><dl>
<dt>embedding_path:</dt><dd><p>path to the embeddings file on HDFS (hdfs://default/…)
or its local_path (/path/to/…).
The embedding_path may also specify a pattern. In which case, the embeddings
are read in the lexical order of the filenames that match the order.</p>
</dd>
<dt>vocab_size:</dt><dd><p>the maximum size of the vocabulary. The top <code class="docutils literal notranslate"><span class="pre">vocab_size</span></code> words in the file
are included in the vocabulary. If you specify a positive vocab_size,
the words are expected to be in descending order of frequency.
This allows the embeddings to be easily filtered to top vocab_size words.
Reducing the vocab_size acts as a regularizer, preventing the model to overfit on rarer words.
A negative vocab_size loads all embeddings.
Reducing the vocab_size may also help with memory issues,
allowing the embedding initializers to fit inside the graph.</p>
</dd>
<dt>embedding_size:</dt><dd><p>Defaults to None. If None, the embedding size is infered from the file name.
For example, <code class="docutils literal notranslate"><span class="pre">glove.300d.txt</span></code> and <code class="docutils literal notranslate"><span class="pre">glove300d200.txt</span></code> will both infrered
as <code class="docutils literal notranslate"><span class="pre">embedding_size=300</span></code>. If this can’t be done, the <code class="docutils literal notranslate"><span class="pre">embedding_size</span></code> is
inferred from the first line in the file. If <code class="docutils literal notranslate"><span class="pre">embedding_size</span></code> is provided,
only the last <code class="docutils literal notranslate"><span class="pre">embedding_size</span></code> values of each line are considered. This
allows the line parser to recover from partial word parsing errors.</p>
</dd>
<dt>separator:</dt><dd><p>Specifies the separator to use when splitting each line into values.
Default value is a whitespace (same as glove format).</p>
</dd>
<dt>vocab:</dt><dd><p>OrderedDict mapping words to np.array embedding vectors. Initializes the vocabulary.
Duplicate words found in the file are ignored.
Defaults to a vocabulary of two words:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">vocab</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
<span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">)</span>
<span class="n">vocab</span><span class="p">[</span><span class="s1">&#39;&lt;UNK&gt;&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">embedding_size</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>tuple of (vocab_initializer, weight_initializer, shape)</p>
<dl class="simple">
<dt>vocab_initializer:</dt><dd><p>A tf.constant_initializer containing a vector of word strings of size vocab_size.</p>
</dd>
<dt>weight_initializer:</dt><dd><p>A twml.contrib.initializers.partition_constant_initializer containing
the weight matrix of embeddings of size vocab_size x embedding_size.</p>
</dd>
<dt>shape:</dt><dd><p>A tuple containing of (vocab_size, embedding_size).</p>
</dd>
</dl>
</dd>
</dl>
<p>“””</p>
<p>start = time.time()</p>
<p>embedding_path = twml.util.sanitize_hdfs_path(embedding_path)</p>
<p>is_user_vocab = True
if vocab is None:</p>
<blockquote>
<div><p>vocab = OrderedDict()
vocab[‘’] = True
vocab[‘&lt;UNK&gt;’] = True
is_user_vocab = False</p>
</div></blockquote>
<dl>
<dt>elif not isinstance(vocab, OrderedDict):</dt><dd><dl class="simple">
<dt>raise RuntimeError(</dt><dd><p>“Expecting vocab argument of type OrderedDict or None. ”
“Got type %s instead.” % type(vocab).__name__</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>if embedding_size is None:</dt><dd><p>embedding_file = os.path.basename(embedding_path)
match = re.search(r”[^d]([d]+)d”, embedding_file)
if match is not None:</p>
<blockquote>
<div><p>embedding_size = int(match.group(1))</p>
</div></blockquote>
</dd>
<dt>if embedding_size is not None and not isinstance(embedding_size, int):</dt><dd><dl class="simple">
<dt>raise RuntimeError(</dt><dd><p>“Expecting embedding_size argument of type int or None. ”
“Got type %s, instead.” % type(embedding_size).__name__</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
<p>embedding_paths = sorted(tf.io.gfile.glob(embedding_path))</p>
<dl>
<dt>if len(embedding_paths) &gt; 1:</dt><dd><dl class="simple">
<dt>raise ValueError(</dt><dd><p>“You are most likely using a the wrong –embedding.path”</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
<p>embedding_path = embedding_paths[0]
logging.info(“Reading embeddings file from path %s..” % embedding_path)</p>
<dl class="simple">
<dt>with tf.io.gfile.GFile(embedding_path) as f:</dt><dd><p>lines = f.readlines()</p>
</dd>
</dl>
<p>logging.info(“Done reading embeddings file from path %s.” % embedding_path)</p>
<p>logging.info(“Parsing vocbulary and embeddings…”)</p>
<dl>
<dt>for line in lines:</dt><dd><p># Word and weights separated by space
values = line.strip().split(separator)
# Word is first symbol on each line
word = values[0]</p>
<dl>
<dt>if word not in vocab:</dt><dd><dl>
<dt>if embedding_size is None or embedding_size &lt;= 0:</dt><dd><p># get all elements after the first one.
word_weights = values[1:]
embedding_size = len(word_weights)</p>
</dd>
<dt>else:</dt><dd><p># get the last embedding_size elements
word_weights = values[-min(embedding_size, len(values) - 1) :]</p>
</dd>
<dt>try:</dt><dd><dl class="simple">
<dt>if len(word_weights) != embedding_size:</dt><dd><p>raise ValueError</p>
</dd>
</dl>
<p>word_weights = np.asarray(word_weights, dtype=np.float32)
vocab[word] = word_weights</p>
</dd>
<dt>except ValueError:</dt><dd><p>logging.info(“Wasn’t able to load embeddings for word ‘%s’. Ignoring it” % word)</p>
</dd>
</dl>
<p>vocab_len = len(vocab)
if vocab_size &gt; 0 and vocab_len == vocab_size:</p>
<blockquote>
<div><p># Limit vocabulary to top terms
break</p>
</div></blockquote>
<dl class="simple">
<dt>elif (vocab_len % 1000) == 0:</dt><dd><p>logging.info(“Loaded %d words into vocab” % vocab_len)</p>
</dd>
</dl>
</dd>
<dt>else:</dt><dd><p>logging.info(“found duplicate word: %s” % word)</p>
</dd>
</dl>
</dd>
<dt>if not is_user_vocab:</dt><dd><p>vocab[‘’] = np.random.randn(embedding_size)
vocab[‘&lt;UNK&gt;’] = np.random.randn(embedding_size)</p>
</dd>
</dl>
<p>words = list(vocab.keys())
weights = list(vocab.values())</p>
<p>weights = np.asarray(weights, dtype=np.float32)
assert weights.shape[0] == len(vocab)
assert weights.shape[1] == embedding_size</p>
<p>vocab_initializer = tf.constant_initializer(words, tf.string)
weight_initializer = twml.contrib.initializers.PartitionConstant(weights, tf.float32)</p>
<p>logging.info(“Loaded %d embeddings in %d seconds.” % (len(vocab), time.time() - start))
return vocab_initializer, weight_initializer, weights.shape</p>
</dd>
<dt>def add_parser_arguments(parser):</dt><dd><p>“””
Adds the embedding.path and embedding.vocab_size command-line arguments to the parser.
These can be used to call an initializer loader function like
the <code class="docutils literal notranslate"><span class="pre">load_initializers_from_csv</span></code> function.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>parser: argparse.ArgumentParser instance obtained from Trainer.get_trainer_parser</p>
</dd>
<dt>Returns:</dt><dd><p>argparse.ArgumentParser instance with discretizer-specific arguments added</p>
</dd>
</dl>
<p>“””</p>
<dl class="simple">
<dt>parser.add_argument(</dt><dd><p>“–embedding.path”,
“–embedding_path”,
dest=”embedding_path”,
type=str,
default=None,
help=”When specified, loads glove embeddings from .txt glove file”,</p>
</dd>
</dl>
<p>)
parser.add_argument(</p>
<blockquote>
<div><p>“–embedding.vocab_size”,
“–embedding_vocab_size”,
dest=”embedding_vocab_size”,
type=int,
default=-1,
help=”Size of vocabulary. Uses this many of the most frequent terms. Defaults to -1 (use full vocab).”,</p>
</div></blockquote>
<p>)</p>
<p>return parser</p>
</dd>
<dt>class EmbeddingLookup(twml.layers.Layer):</dt><dd><p>“””Layer for looking up embeddings.
Transforms a sequence of strings to a sequence of embeddings.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>vocab_size:</dt><dd><p>The number of word strings and embeddings in the vocabulary.</p>
</dd>
<dt>output_size:</dt><dd><p>Long or Integer, dimensionality of the output space. The embedding vector size.</p>
</dd>
<dt>vocab_initializer:</dt><dd><p>Initializer function for the vocabulary. Required. The initializer should
return a list of strings of size vocab_size.</p>
</dd>
<dt>weight_initializer:</dt><dd><p>Initializer function for the weight matrix of size vocab_size x output_size.
This argument defaults to zeros_initializer().
This is valid when the EmbeddingLookup is the first layer of
parameters but should be changed otherwise.</p>
</dd>
<dt>trainable:</dt><dd><p>Boolean, if <cite>True</cite> adds variables to the graph collection
<code class="docutils literal notranslate"><span class="pre">GraphKeys.TRAINABLE_VARIABLES</span></code> (see <a class="reference external" href="https://www.tensorflow.org/versions/master/api_docs/python/tf/Variable">tf.Variable</a>).
Defaults to True: trains the embeddings.</p>
</dd>
<dt>num_oov_buckets:</dt><dd><p>The number of buckets to use for OOV strings. These bucket ids occur after the vocab bucket
ids. Hashing is used to assign OOV strings to these buckets. If <cite>num_oov_buckets</cite> is not
specified, index <cite>OOV_WORD_ID</cite> is used for OOV strings.</p>
</dd>
<dt>name:</dt><dd><p>String, the name of the layer. Layers with the same name will
share weights, but to avoid mistakes we require <code class="docutils literal notranslate"><span class="pre">reuse=True</span></code> in such cases.</p>
</dd>
<dt>num_partitions:</dt><dd><p>Number of partitions to use for the weight variable. Defaults to 1.</p>
</dd>
<dt>partition_axis:</dt><dd><p>If num_partitions is specified, the partition axis for the weight variable
Defaults to 0 (partition by row).
Must be 0 (row) or 1 (column, does not support yet)</p>
</dd>
<dt>weight_regularizer:</dt><dd><p>Regularizer function for the weight matrix.
Ensure to add tf.losses.get_regularization_loss() to your loss for this to take effect.</p>
</dd>
<dt>dtype:</dt><dd><p>Defaults to tf.float32. Specifies the dtype of the weights.</p>
</dd>
<dt>use_placeholder:</dt><dd><p>Defaults to True.
If set to <cite>True</cite>, the initializer is passed via a placeholder. The initializer in this case needs to be of type <cite>keras.initializers.Constant</cite>.
If set to <cite>False</cite>, the initializer becomes part of the graph. This can sometimes be beyond what protobuf clients support.</p>
</dd>
<dt>checkpoint_dir:</dt><dd><p>Default to None.
If set to the path of a checkpoint, load embedding from the checkpoint.</p>
</dd>
<dt>convert_to_lowercase:</dt><dd><p>Default to True.
Converting all string inputs to lowercase.</p>
</dd>
</dl>
</dd>
</dl>
<p>Notes: If <cite>use_placeholder</cite> is set to <cite>True</cite>, the feed dictionary can be accessed by calling <cite>twml.contrib.initializers.get_init_feed_dict()</cite>.
“””</p>
<dl>
<dt>def __init__(</dt><dd><p>self,
vocab_size,
output_size,
vocab_initializer,
weight_initializer=None,
trainable=True,
num_oov_buckets=None,
oov_word_id=None,
name=None,
num_partitions=1,
partition_axis=0,
weight_regularizer=None,
dtype=None,
use_placeholder=True,
checkpoint_dir=None,
convert_to_lowercase=True,
<a href="#id1"><span class="problematic" id="id2">**</span></a>kwargs,</p>
</dd>
<dt>):</dt><dd><dl class="simple">
<dt>if dtype is None:</dt><dd><p># prevents a bug where the parent class defaults to the type of the first input tensor.
dtype = tf.float32</p>
</dd>
</dl>
<p>super().__init__(trainable=trainable, name=name, dtype=dtype, <a href="#id3"><span class="problematic" id="id4">**</span></a>kwargs)
# Weights initialization is set to 0s. This is safe for full sparse layers because
# you are supposed to learn your embedding from the label.</p>
<p>is_constant_init = isinstance(weight_initializer, tf.keras.initializers.Constant)
if use_placeholder and (not is_constant_init) and (weight_initializer is not None):</p>
<blockquote>
<div><p>raise ValueError(“Weight initializer should be a <cite>Constant</cite> or <cite>None</cite>.”)</p>
</div></blockquote>
<dl class="simple">
<dt>if weight_initializer is None:</dt><dd><p>self.weight_initializer = tf.zeros_initializer()</p>
</dd>
<dt>else:</dt><dd><p>self.weight_initializer = weight_initializer</p>
</dd>
</dl>
<p>self.use_placeholder = use_placeholder
self.checkpoint_dir = checkpoint_dir
self.convert_to_lowercase = convert_to_lowercase</p>
<p>self.vocab_initializer = vocab_initializer
self.vocab_size = vocab_size
self.output_size = output_size
self.num_partitions = num_partitions
self.partition_axis = partition_axis
self.weight_regularizer = weight_regularizer
self.trainable = trainable
self.oov_word_id = oov_word_id
self.num_oov_buckets = num_oov_buckets</p>
<dl class="simple">
<dt>if self.oov_word_id is not None and self.num_oov_buckets is not None:</dt><dd><p>raise ValueError(“At most one of oov_word_id or num_oov_buckets should be specified”)</p>
</dd>
<dt>elif self.oov_word_id is None and self.num_oov_buckets is None:</dt><dd><p>self.oov_word_id = OOV_WORD_ID  # use the default OOV word id</p>
</dd>
<dt>if partition_axis != 0:</dt><dd><p>raise NotImplementedError(“embedding_lookup only supports partition_axis = 0”)</p>
</dd>
</dl>
</dd>
<dt>def build(self, input_shapes):</dt><dd><p>“””
creates the <code class="docutils literal notranslate"><span class="pre">vocab</span></code> and <code class="docutils literal notranslate"><span class="pre">weight</span></code> Variables
of shape <code class="docutils literal notranslate"><span class="pre">[vocab_size]</span></code> and <code class="docutils literal notranslate"><span class="pre">[vocab_size,</span> <span class="pre">output_size]</span></code> respectively.
“””
partitioner = None</p>
<p>additional_buckets_for_oov = self.num_oov_buckets if self.num_oov_buckets is not None else 0
shape = [self.vocab_size + additional_buckets_for_oov, self.output_size]</p>
<dl>
<dt>if self.use_placeholder:</dt><dd><dl class="simple">
<dt>embedding_weight_initializer = twml.contrib.initializers.PlaceholderInitializer(</dt><dd><p>shape, self.dtype</p>
</dd>
</dl>
<p>)
tf.add_to_collection(</p>
<blockquote>
<div><p>twml.contrib.initializers.TWML_INIT_FEED_KEY,
{embedding_weight_initializer.value: self.weight_initializer.value},</p>
</div></blockquote>
<p>)</p>
</dd>
<dt>else:</dt><dd><p>embedding_weight_initializer = self.weight_initializer</p>
</dd>
<dt>if self.num_partitions:</dt><dd><p>partition_axis = int(self.partition_axis)
partitioner = tf.fixed_size_partitioner(self.num_partitions, axis=partition_axis)</p>
</dd>
<dt>else:</dt><dd><p># Regular variables do not like it when you pass both constant tensors and shape
if not callable(self.weight_initializer):</p>
<blockquote>
<div><p>shape = None</p>
</div></blockquote>
</dd>
<dt>self.vocab = self.add_variable(</dt><dd><p>‘vocab’,
initializer=self.vocab_initializer,
shape=[self.vocab_size],
dtype=tf.string,
trainable=False,</p>
</dd>
</dl>
<p>)</p>
<dl class="simple">
<dt>self.weight = self.add_variable(</dt><dd><p>‘weight’,
initializer=None if self.checkpoint_dir is not None else embedding_weight_initializer,
regularizer=self.weight_regularizer,
shape=shape,
dtype=self.dtype,
trainable=self.trainable,
partitioner=partitioner,</p>
</dd>
</dl>
<p>)
if self.checkpoint_dir is not None:</p>
<blockquote>
<div><p>twml.trainers.trainer.init_from_checkpoint(self.checkpoint_dir, {‘weight’: self.weight.name})</p>
</div></blockquote>
<p>self.built = True</p>
</dd>
<dt>def call(</dt><dd><p>self, inputs, debug=False, oov_summaries=False, <a href="#id5"><span class="problematic" id="id6">**</span></a>kwargs</p>
</dd>
<dt>):  # pylint: disable=unused-argument</dt><dd><p>“””Converts word strings to word ids using the vocabulary lookup table.
Then converts the word ids to their commensurate embedding vector.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>inputs:</dt><dd><p>A tensor of word strings. Typically, of size batch_size x seq_len.</p>
</dd>
<dt>debug:</dt><dd><p>When True, prints the input strings and their commensurate input_ids.
Defaults to False.</p>
</dd>
<dt>oov_summaries:</dt><dd><p>When True, log the out-of-vocabulary (OOV) rate to TensorBoard
Defaults to False.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>The mapping of input word strings to output embedding vectors.
Given an input of shape <code class="docutils literal notranslate"><span class="pre">batch_size</span> <span class="pre">x</span> <span class="pre">seq_len</span></code>, the output has shape
<code class="docutils literal notranslate"><span class="pre">batch_size</span> <span class="pre">x</span> <span class="pre">seq_len</span> <span class="pre">x</span> <span class="pre">embedding_size</span></code>.</p>
</dd>
</dl>
<p>“””
if self.convert_to_lowercase:</p>
<blockquote>
<div><p>inputs = tf.strings.lower(inputs)</p>
</div></blockquote>
<dl class="simple">
<dt>if self.num_oov_buckets is None:</dt><dd><p>lookup_table = index_table_from_tensor(self.vocab, default_value=self.oov_word_id)</p>
</dd>
<dt>else:</dt><dd><p>lookup_table = index_table_from_tensor(self.vocab, num_oov_buckets=self.num_oov_buckets)</p>
</dd>
</dl>
<p>input_ids = lookup_table.lookup(inputs)</p>
<dl>
<dt>if oov_summaries:</dt><dd><dl class="simple">
<dt>oov_count = tf.reduce_sum(</dt><dd><p>tf.cast(tf.math.equal(input_ids, self.oov_word_id), tf.dtypes.float32)</p>
</dd>
</dl>
<p>)
valid_count = tf.reduce_sum(</p>
<blockquote>
<div><p>tf.cast(tf.math.not_equal(input_ids, PAD_WORD_ID), tf.dtypes.float32)</p>
</div></blockquote>
<p>)
oov_rate = oov_count / valid_count
tf.summary.scalar(‘OOV_rate’, oov_rate)</p>
</dd>
</dl>
<p>if debug:</p>
<blockquote>
<div><dl class="simple">
<dt>def print_debug():</dt><dd><p>return tf.print(“input_strings:”, inputs, “ninput_ids: “, input_ids, summarize=140)</p>
</dd>
<dt>with tf.control_dependencies([twml.util.do_every_n_steps(print_debug, 1000)]):</dt><dd><p>input_ids = tf.identity(input_ids)</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>output_embeddings = tf.nn.embedding_lookup(</dt><dd><p>params=self.weight, ids=input_ids, partition_strategy=’div’</p>
</dd>
</dl>
<p>)</p>
<p>output_shape = inputs.shape.concatenate(tf.TensorShape([self.output_size]))
output_embeddings.set_shape(output_shape)</p>
<p>return output_embeddings</p>
</dd>
</dl>
</dd>
</dl>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../index2.rst.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../index2.rst.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../../_sources/twml/twml/contrib/layers/embedding_lookup.py.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>