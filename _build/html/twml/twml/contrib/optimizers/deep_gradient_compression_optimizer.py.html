<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>“””
A custom optimizer to implement Deep Gradient Compression. The general idea of
gradient compression is to compress the gradients exchanged across machines,
in order to reduce the communication overhead of distributing computing efforts.
More details in <a class="reference external" href="https://arxiv.org/abs/1712.01887">https://arxiv.org/abs/1712.01887</a>
“””</p>
<p># TODO: Test how much communication overhead this DeepGradientCompressionOptimizer can reduce under
# multi-GPU and distributed setting.</p>
<p>import tensorflow.compat.v1 as tf</p>
<dl>
<dt>def compute_threshold(grad, density):</dt><dd><p>“””
A utility function to compute the threshold for gradient sparsification, given the gradient
tensor and the density.
Args:</p>
<blockquote>
<div><dl class="simple">
<dt>grad(tf.Tensor):</dt><dd><p>Gradient tensor for some variable.</p>
</dd>
<dt>density(float):</dt><dd><p>Density degree when sparsifying gradients.</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>Returns(float):</dt><dd><p>Threshold for gradient sparsification.</p>
</dd>
</dl>
<p>“””
flat_grad = tf.reshape(grad, [-1])
abs_flat_grad = tf.abs(flat_grad)
size = tf.shape(abs_flat_grad)[0]
k = tf.maximum(tf.constant(1),</p>
<blockquote>
<div><p>tf.cast(tf.scalar_mul(density, tf.cast(size, tf.float32)), tf.int32))</p>
</div></blockquote>
<p>topk, _ = tf.nn.top_k(abs_flat_grad, k, False)
return topk[-1]</p>
</dd>
<dt>def get_top_row_indices(values, density):</dt><dd><p>“””
A utility function to get indices of most significant rows, given the density degree.
Args:</p>
<blockquote>
<div><dl class="simple">
<dt>values(tf.Tensor):</dt><dd><p>Gradient or locally accumulated gradient for some variable.</p>
</dd>
<dt>density(float):</dt><dd><p>Density degree when filtering out rows.</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>Returns(list(int)):</dt><dd><p>Indices of most significant rows.</p>
</dd>
</dl>
<p>“””
abs_values = tf.abs(values)</p>
<dl>
<dt>try:</dt><dd><p>row_num = tf.shape(abs_values)[0]
k = tf.maximum(tf.constant(1),</p>
<blockquote>
<div><p>tf.cast(tf.scalar_mul(density, tf.cast(row_num, tf.float32)), tf.int32))</p>
</div></blockquote>
<p>row_sums = tf.squeeze(tf.reduce_sum(values, axis=1, keepdims=True))
_, top_row_indices = tf.nn.top_k(row_sums, k=k, sorted=False)
# print “abs_values”, abs_values, “row_sums”, row_sums
return top_row_indices
# return tf.range(row_num)</p>
</dd>
<dt>except ValueError:  # if the tensor is 0-D or 1-D</dt><dd><p>return None</p>
</dd>
</dl>
</dd>
<dt>class DeepGradientCompressionOptimizer(tf.train.GradientDescentOptimizer):</dt><dd><p>“””
A custom optimizer to implement Deep Gradient Compression (<a class="reference external" href="https://arxiv.org/abs/1712.01887">https://arxiv.org/abs/1712.01887</a>).
“””</p>
<dl>
<dt>def __init__(self, learning_rate, use_locking=False, name=”Sparse”,</dt><dd><blockquote>
<div><p>density=1.0,
density_decay=False,
density_decay_steps=10000,
density_decay_rate=0.5,
min_density=0.1,
accumulation=False):</p>
</div></blockquote>
<p>super(DeepGradientCompressionOptimizer, self).__init__(learning_rate, use_locking, name)
self._initial_density_t = tf.convert_to_tensor(density)
self._density_decay = density_decay
dtype = self._initial_density_t.dtype
self._density_decay_steps_t = tf.convert_to_tensor(density_decay_steps, dtype)
self._density_decay_rate_t = tf.convert_to_tensor(density_decay_rate, dtype)
self._min_density_t = tf.convert_to_tensor(min_density, dtype)
self._accumulation = accumulation</p>
</dd>
<dt>def _prepare(self):</dt><dd><p>super(DeepGradientCompressionOptimizer, self)._prepare()
if not self._density_decay:</p>
<blockquote>
<div><p>self._density_t = self._initial_density_t</p>
</div></blockquote>
<dl>
<dt>else:</dt><dd><p>dtype = self._initial_density_t.dtype
global_step = tf.cast(tf.train.get_global_step(), dtype)
p = tf.floor(tf.divide(global_step, self._density_decay_steps_t))
decayed_density = tf.multiply(self._initial_density_t,</p>
<blockquote>
<div><p>tf.pow(self._density_decay_rate_t, p))</p>
</div></blockquote>
<p>self._density_t = tf.maximum(self._min_density_t, decayed_density)</p>
</dd>
</dl>
</dd>
<dt>def _create_slots(self, var_list):</dt><dd><p>“””
Create a slot variable to accumulate gradients locally for each variable in <cite>var_list</cite>.
Args:</p>
<blockquote>
<div><dl class="simple">
<dt>var_list(list(tf.Variable)):</dt><dd><p>List of variables to accumulate gradients locally for.</p>
</dd>
</dl>
</div></blockquote>
<p>“””
for var in var_list:</p>
<blockquote>
<div><p>self._zeros_slot(var, “g_buffer”, self._name)</p>
</div></blockquote>
</dd>
<dt>def _apply_dense(self, grad, var):</dt><dd><dl>
<dt>if not self._accumulation:</dt><dd><p>top_row_indices = get_top_row_indices(grad, self._density_t)</p>
<dl class="simple">
<dt>if top_row_indices is None:</dt><dd><p>return super(DeepGradientCompressionOptimizer, self)._apply_dense(grad, var)</p>
</dd>
</dl>
<p>sparsified_values = tf.gather(grad, top_row_indices)
sparsified_indices = top_row_indices</p>
<p>sparsified_grad = tf.IndexedSlices(sparsified_values, sparsified_indices)</p>
<dl class="simple">
<dt>return super(DeepGradientCompressionOptimizer, self)._apply_sparse_duplicate_indices(</dt><dd><p>sparsified_grad, var)</p>
</dd>
</dl>
</dd>
<dt>else:</dt><dd><p>g_buffer = self.get_slot(var, “g_buffer”)</p>
<p>g_buffer = tf.assign_add(g_buffer, grad)</p>
<p>top_row_indices = get_top_row_indices(g_buffer, self._density_t)</p>
<dl class="simple">
<dt>if top_row_indices is None:</dt><dd><p>return super(DeepGradientCompressionOptimizer, self)._apply_dense(grad, var)</p>
</dd>
</dl>
<p>sparsified_values = tf.gather(g_buffer, top_row_indices)
sparsified_indices = top_row_indices</p>
<p>sparsified_grad = tf.IndexedSlices(sparsified_values, sparsified_indices)</p>
<dl class="simple">
<dt>update_var = super(DeepGradientCompressionOptimizer, self)._apply_sparse_duplicate_indices(</dt><dd><p>sparsified_grad, var)</p>
</dd>
<dt>update_g_buffer = tf.scatter_update(g_buffer, sparsified_indices, tf.zeros_like(</dt><dd><p>sparsified_values))</p>
</dd>
</dl>
<p>return tf.group(<a href="#id1"><span class="problematic" id="id2">*</span></a>[update_var, update_g_buffer])</p>
</dd>
</dl>
</dd>
<dt>def _apply_sparse_duplicate_indices(self, grad, var):</dt><dd><dl>
<dt>if not self._accumulation:</dt><dd><p>top_row_indices = get_top_row_indices(grad.values, self._density_t)</p>
<dl class="simple">
<dt>if top_row_indices is None:</dt><dd><p>return super(DeepGradientCompressionOptimizer, self)._apply_sparse_duplicate_indices(grad, var)  # noqa: E501</p>
</dd>
</dl>
<p>sparsified_values = tf.gather(grad.values, top_row_indices)
sparsified_indices = tf.gather(grad.indices, top_row_indices)</p>
<p>sparsified_grad = tf.IndexedSlices(sparsified_values, sparsified_indices)</p>
<dl class="simple">
<dt>return super(DeepGradientCompressionOptimizer, self)._apply_sparse_duplicate_indices(</dt><dd><p>sparsified_grad, var)</p>
</dd>
</dl>
</dd>
<dt>else:</dt><dd><p>g_buffer = self.get_slot(var, “g_buffer”)</p>
<p>g_buffer = tf.scatter_update(g_buffer, grad.indices, grad.values)</p>
<p>top_row_indices = get_top_row_indices(g_buffer, self._density_t)</p>
<dl class="simple">
<dt>if top_row_indices is None:</dt><dd><dl class="simple">
<dt>return super(DeepGradientCompressionOptimizer,</dt><dd><p>self)._apply_sparse_duplicate_indices(grad, var)</p>
</dd>
</dl>
</dd>
</dl>
<p>sparsified_values = tf.gather(g_buffer, top_row_indices)
sparsified_indices = top_row_indices</p>
<p>sparsified_grad = tf.IndexedSlices(sparsified_values, sparsified_indices)</p>
<dl class="simple">
<dt>update_var = super(DeepGradientCompressionOptimizer, self)._apply_sparse_duplicate_indices(</dt><dd><p>sparsified_grad, var)</p>
</dd>
<dt>update_g_buffer = tf.scatter_update(g_buffer, sparsified_indices, tf.zeros_like(</dt><dd><p>sparsified_values))</p>
</dd>
</dl>
<p>return tf.group(<a href="#id3"><span class="problematic" id="id4">*</span></a>[update_var, update_g_buffer])</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../index2.rst.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../index2.rst.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../../_sources/twml/twml/contrib/optimizers/deep_gradient_compression_optimizer.py.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>