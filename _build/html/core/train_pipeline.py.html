<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>“””
Taken from <a class="reference external" href="https://raw.githubusercontent.com/pytorch/torchrec/v0.3.2/torchrec/distributed/train_pipeline.py">https://raw.githubusercontent.com/pytorch/torchrec/v0.3.2/torchrec/distributed/train_pipeline.py</a>
with TrainPipelineSparseDist.progress modified to support gradient accumulation.</p>
<p>“””
import abc
from dataclasses import dataclass, field
import logging
from typing import (</p>
<blockquote>
<div><p>Any,
cast,
Dict,
Generic,
Iterator,
List,
Optional,
Set,
Tuple,
TypeVar,</p>
</div></blockquote>
<p>)</p>
<p>import torch
from torch.autograd.profiler import record_function
from torch.fx.node import Node
from torchrec.distributed.model_parallel import (</p>
<blockquote>
<div><p>DistributedModelParallel,
ShardedModule,</p>
</div></blockquote>
<p>)
from torchrec.distributed.types import Awaitable
from torchrec.modules.feature_processor import BaseGroupedFeatureProcessor
from torchrec.streamable import Multistreamable, Pipelineable</p>
<p>logger: logging.Logger = logging.getLogger(__name__)</p>
<p>In = TypeVar(“In”, bound=Pipelineable)
Out = TypeVar(“Out”)</p>
<dl>
<dt>class TrainPipeline(abc.ABC, Generic[In, Out]):</dt><dd><p>&#64;abc.abstractmethod
def progress(self, dataloader_iter: Iterator[In]) -&gt; Out:</p>
<blockquote>
<div><p>pass</p>
</div></blockquote>
</dd>
<dt>def _to_device(batch: In, device: torch.device, non_blocking: bool) -&gt; In:</dt><dd><dl class="simple">
<dt>assert isinstance(</dt><dd><p>batch, (torch.Tensor, Pipelineable)</p>
</dd>
</dl>
<p>), f”{type(batch)} must implement Pipelineable interface”
return cast(In, batch.to(device=device, non_blocking=non_blocking))</p>
</dd>
<dt>def _wait_for_batch(batch: In, stream: Optional[torch.cuda.streams.Stream]) -&gt; None:</dt><dd><dl class="simple">
<dt>if stream is None:</dt><dd><p>return</p>
</dd>
</dl>
<p>torch.cuda.current_stream().wait_stream(stream)
# As mentioned in <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html">https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html</a>,
# PyTorch uses the “caching allocator” for memory allocation for tensors. When a tensor is
# freed, its memory is likely to be reused by newly constructed tenosrs.  By default,
# this allocator traces whether a tensor is still in use by only the CUDA stream where it
# was created.   When a tensor is used by additional CUDA streams, we need to call record_stream
# to tell the allocator about all these streams.  Otherwise, the allocator might free the
# underlying memory of the tensor once it is no longer used by the creator stream.  This is
# a notable programming trick when we write programs using multi CUDA streams.
cur_stream = torch.cuda.current_stream()
assert isinstance(</p>
<blockquote>
<div><p>batch, (torch.Tensor, Multistreamable)</p>
</div></blockquote>
<p>), f”{type(batch)} must implement Multistreamable interface”
batch.record_stream(cur_stream)</p>
</dd>
<dt>class TrainPipelineBase(TrainPipeline[In, Out]):</dt><dd><p>“””
This class runs training iterations using a pipeline of two stages, each as a CUDA
stream, namely, the current (default) stream and <cite>self._memcpy_stream</cite>. For each
iteration, <cite>self._memcpy_stream</cite> moves the input from host (CPU) memory to GPU
memory, and the default stream runs forward, backward, and optimization.
“””</p>
<dl>
<dt>def __init__(</dt><dd><p>self,
model: torch.nn.Module,
optimizer: torch.optim.Optimizer,
device: torch.device,</p>
</dd>
<dt>) -&gt; None:</dt><dd><p>self._model = model
self._optimizer = optimizer
self._device = device
self._memcpy_stream: Optional[torch.cuda.streams.Stream] = (</p>
<blockquote>
<div><p>torch.cuda.Stream() if device.type == “cuda” else None</p>
</div></blockquote>
<p>)
self._cur_batch: Optional[In] = None
self._connected = False</p>
</dd>
<dt>def _connect(self, dataloader_iter: Iterator[In]) -&gt; None:</dt><dd><p>cur_batch = next(dataloader_iter)
self._cur_batch = cur_batch
with torch.cuda.stream(self._memcpy_stream):</p>
<blockquote>
<div><p>self._cur_batch = _to_device(cur_batch, self._device, non_blocking=True)</p>
</div></blockquote>
<p>self._connected = True</p>
</dd>
<dt>def progress(self, dataloader_iter: Iterator[In]) -&gt; Out:</dt><dd><dl class="simple">
<dt>if not self._connected:</dt><dd><p>self._connect(dataloader_iter)</p>
</dd>
</dl>
<p># Fetch next batch
with record_function(“## next_batch ##”):</p>
<blockquote>
<div><p>next_batch = next(dataloader_iter)</p>
</div></blockquote>
<p>cur_batch = self._cur_batch
assert cur_batch is not None</p>
<dl class="simple">
<dt>if self._model.training:</dt><dd><dl class="simple">
<dt>with record_function(“## zero_grad ##”):</dt><dd><p>self._optimizer.zero_grad()</p>
</dd>
</dl>
</dd>
<dt>with record_function(“## wait_for_batch ##”):</dt><dd><p>_wait_for_batch(cur_batch, self._memcpy_stream)</p>
</dd>
<dt>with record_function(“## forward ##”):</dt><dd><p>(losses, output) = self._model(cur_batch)</p>
</dd>
<dt>if self._model.training:</dt><dd><dl class="simple">
<dt>with record_function(“## backward ##”):</dt><dd><p>torch.sum(losses, dim=0).backward()</p>
</dd>
</dl>
</dd>
</dl>
<p># Copy the next batch to GPU
self._cur_batch = cur_batch = next_batch
with record_function(“## copy_batch_to_gpu ##”):</p>
<blockquote>
<div><dl class="simple">
<dt>with torch.cuda.stream(self._memcpy_stream):</dt><dd><p>self._cur_batch = _to_device(cur_batch, self._device, non_blocking=True)</p>
</dd>
</dl>
</div></blockquote>
<p># Update
if self._model.training:</p>
<blockquote>
<div><dl class="simple">
<dt>with record_function(“## optimizer ##”):</dt><dd><p>self._optimizer.step()</p>
</dd>
</dl>
</div></blockquote>
<p>return output</p>
</dd>
</dl>
</dd>
<dt>class Tracer(torch.fx.Tracer):</dt><dd><p># Disable proxying buffers during tracing. Ideally, proxying buffers would
# be disabled, but some models are currently mutating buffer values, which
# causes errors during tracing. If those models can be rewritten to not do
# that, we can likely remove this line
proxy_buffer_attributes = False</p>
<dl>
<dt>def __init__(self, leaf_modules: Optional[List[str]] = None) -&gt; None:</dt><dd><p>super().__init__()
self._leaf_modules: List[str] = leaf_modules if leaf_modules is not None else []</p>
</dd>
<dt>def is_leaf_module(self, m: torch.nn.Module, module_qualified_name: str) -&gt; bool:</dt><dd><dl class="simple">
<dt>if isinstance(m, ShardedModule) or module_qualified_name in self._leaf_modules:</dt><dd><p>return True</p>
</dd>
</dl>
<p>return super().is_leaf_module(m, module_qualified_name)</p>
</dd>
</dl>
</dd>
</dl>
<p>&#64;dataclass
class TrainPipelineContext:</p>
<blockquote>
<div><p># pyre-ignore [4]
input_dist_requests: Dict[str, Awaitable[Any]] = field(default_factory=dict)
module_contexts: Dict[str, Multistreamable] = field(default_factory=dict)
# pyre-ignore [4]
feature_processor_forwards: List[Any] = field(default_factory=list)</p>
</div></blockquote>
<p>&#64;dataclass
class ArgInfo:</p>
<blockquote>
<div><p># attributes of input batch, e.g. batch.attr1.attr2 call
# will produce [“attr1”, “attr2”]
input_attrs: List[str]
# batch[attr1].attr2 will produce [True, False]
is_getitems: List[bool]
# name for kwarg of pipelined forward() call or None
# for a positional arg
name: Optional[str]</p>
</div></blockquote>
<dl>
<dt>class PipelinedForward:</dt><dd><dl class="simple">
<dt>def __init__(</dt><dd><p>self,
name: str,
args: List[ArgInfo],
module: ShardedModule,
context: TrainPipelineContext,
dist_stream: Optional[torch.cuda.streams.Stream],</p>
</dd>
<dt>) -&gt; None:</dt><dd><p>self._name = name
self._args = args
self._module = module
self._context = context
self._dist_stream = dist_stream</p>
</dd>
</dl>
<p># pyre-ignore [2, 24]
def __call__(self, <a href="#id1"><span class="problematic" id="id2">*</span></a>input, <a href="#id3"><span class="problematic" id="id4">**</span></a>kwargs) -&gt; Awaitable:</p>
<blockquote>
<div><p>assert self._name in self._context.input_dist_requests
request = self._context.input_dist_requests[self._name]
assert isinstance(request, Awaitable)
with record_function(“## wait_sparse_data_dist ##”):</p>
<blockquote>
<div><p># Finish waiting on the dist_stream,
# in case some delayed stream scheduling happens during the wait() call.
with torch.cuda.stream(self._dist_stream):</p>
<blockquote>
<div><p>data = request.wait()</p>
</div></blockquote>
</div></blockquote>
<p># Make sure that both result of input_dist and context
# are properly transferred to the current stream.
if self._dist_stream is not None:</p>
<blockquote>
<div><p>torch.cuda.current_stream().wait_stream(self._dist_stream)
cur_stream = torch.cuda.current_stream()</p>
<dl class="simple">
<dt>assert isinstance(</dt><dd><p>data, (torch.Tensor, Multistreamable)</p>
</dd>
</dl>
<p>), f”{type(data)} must implement Multistreamable interface”
# pyre-fixme[6]: For 1st param expected <cite>Stream</cite> but got <cite>Stream</cite>.
data.record_stream(cur_stream)</p>
<p>ctx = self._context.module_contexts[self._name]
ctx.record_stream(cur_stream)</p>
</div></blockquote>
<dl>
<dt>if len(self._context.feature_processor_forwards) &gt; 0:</dt><dd><dl>
<dt>with record_function(“## feature_processor ##”):</dt><dd><dl>
<dt>for sparse_feature in data:</dt><dd><dl>
<dt>if sparse_feature.id_score_list_features is not None:</dt><dd><dl>
<dt>for fp_forward in self._context.feature_processor_forwards:</dt><dd><dl class="simple">
<dt>sparse_feature.id_score_list_features = fp_forward(</dt><dd><p>sparse_feature.id_score_list_features</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<p>return self._module.compute_and_output_dist(self._context.module_contexts[self._name], data)</p>
</div></blockquote>
<p>&#64;property
def name(self) -&gt; str:</p>
<blockquote>
<div><p>return self._name</p>
</div></blockquote>
<p>&#64;property
def args(self) -&gt; List[ArgInfo]:</p>
<blockquote>
<div><p>return self._args</p>
</div></blockquote>
</dd>
<dt>def _start_data_dist(</dt><dd><p>pipelined_modules: List[ShardedModule],
batch: In,
context: TrainPipelineContext,</p>
</dd>
<dt>) -&gt; None:</dt><dd><p>context.input_dist_requests.clear()
context.module_contexts.clear()
for module in pipelined_modules:</p>
<blockquote>
<div><p>forward = module.forward
assert isinstance(forward, PipelinedForward)</p>
<p># Retrieve argument for the input_dist of EBC
# is_getitem True means this argument could be retrieved by a list
# False means this argument is getting while getattr
# and this info was done in the _rewrite_model by tracing the
# entire model to get the arg_info_list
args = []
kwargs = {}
for arg_info in forward.args:</p>
<blockquote>
<div><dl>
<dt>if arg_info.input_attrs:</dt><dd><p>arg = batch
for attr, is_getitem in zip(arg_info.input_attrs, arg_info.is_getitems):</p>
<blockquote>
<div><dl class="simple">
<dt>if is_getitem:</dt><dd><p>arg = arg[attr]</p>
</dd>
<dt>else:</dt><dd><p>arg = getattr(arg, attr)</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>if arg_info.name:</dt><dd><p>kwargs[arg_info.name] = arg</p>
</dd>
<dt>else:</dt><dd><p>args.append(arg)</p>
</dd>
</dl>
</dd>
<dt>else:</dt><dd><p>args.append(None)</p>
</dd>
</dl>
</div></blockquote>
<p># Start input distribution.
module_ctx = module.create_context()
context.module_contexts[forward.name] = module_ctx
context.input_dist_requests[forward.name] = module.input_dist(module_ctx, <a href="#id5"><span class="problematic" id="id6">*</span></a>args, <a href="#id7"><span class="problematic" id="id8">**</span></a>kwargs)</p>
</div></blockquote>
<p># Call wait on the first awaitable in the input dist for the tensor splits
for key, awaitable in context.input_dist_requests.items():</p>
<blockquote>
<div><p>context.input_dist_requests[key] = awaitable.wait()</p>
</div></blockquote>
</dd>
<dt>def _get_node_args_helper(</dt><dd><p># pyre-ignore
arguments,
num_found: int,
feature_processor_arguments: Optional[List[Node]] = None,</p>
</dd>
<dt>) -&gt; Tuple[List[ArgInfo], int]:</dt><dd><p>“””
Goes through the args/kwargs of a node and arranges them into a list of <a href="#id9"><span class="problematic" id="id10">`</span></a>ArgInfo`s.
It also counts the number of (args + kwargs) found.
“””</p>
<p>arg_info_list = [ArgInfo([], [], None) for _ in range(len(arguments))]
for arg, arg_info in zip(arguments, arg_info_list):</p>
<blockquote>
<div><dl>
<dt>if arg is None:</dt><dd><p>num_found += 1
continue</p>
</dd>
<dt>while True:</dt><dd><dl class="simple">
<dt>if not isinstance(arg, torch.fx.Node):</dt><dd><p>break</p>
</dd>
</dl>
<p>child_node = arg</p>
<dl class="simple">
<dt>if child_node.op == “placeholder”:</dt><dd><p>num_found += 1
break</p>
</dd>
</dl>
<p># skip this fp node
elif feature_processor_arguments is not None and child_node in feature_processor_arguments:</p>
<blockquote>
<div><p>arg = child_node.args[0]</p>
</div></blockquote>
<dl class="simple">
<dt>elif (</dt><dd><p>child_node.op == “call_function”
and child_node.target.__module__ == “builtins”
# pyre-ignore[16]
and child_node.target.__name__ == “getattr”</p>
</dd>
<dt>):</dt><dd><p>arg_info.input_attrs.insert(0, child_node.args[1])
arg_info.is_getitems.insert(0, False)
arg = child_node.args[0]</p>
</dd>
<dt>elif (</dt><dd><p>child_node.op == “call_function”
and child_node.target.__module__ == “_operator”
# pyre-ignore[16]
and child_node.target.__name__ == “getitem”</p>
</dd>
<dt>):</dt><dd><p>arg_info.input_attrs.insert(0, child_node.args[1])
arg_info.is_getitems.insert(0, True)
arg = child_node.args[0]</p>
</dd>
<dt>else:</dt><dd><p>break</p>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<p>return arg_info_list, num_found</p>
</dd>
<dt>def _get_node_args(</dt><dd><p>node: Node, feature_processor_nodes: Optional[List[Node]] = None</p>
</dd>
<dt>) -&gt; Tuple[List[ArgInfo], int]:</dt><dd><p>num_found = 0
pos_arg_info_list, num_found = _get_node_args_helper(</p>
<blockquote>
<div><p>node.args, num_found, feature_processor_nodes</p>
</div></blockquote>
<p>)
kwargs_arg_info_list, num_found = _get_node_args_helper(node.kwargs.values(), num_found)</p>
<p># Replace with proper names for kwargs
for name, arg_info_list in zip(node.kwargs, kwargs_arg_info_list):</p>
<blockquote>
<div><p>arg_info_list.name = name</p>
</div></blockquote>
<p>arg_info_list = pos_arg_info_list + kwargs_arg_info_list
return arg_info_list, num_found</p>
</dd>
<dt>def _get_unsharded_module_names_helper(</dt><dd><p>model: torch.nn.Module,
path: str,
unsharded_module_names: Set[str],</p>
</dd>
<dt>) -&gt; bool:</dt><dd><p>sharded_children = set()
for name, child in model.named_children():</p>
<blockquote>
<div><p>curr_path = path + name
if isinstance(child, ShardedModule):</p>
<blockquote>
<div><p>sharded_children.add(name)</p>
</div></blockquote>
<dl>
<dt>else:</dt><dd><dl class="simple">
<dt>child_sharded = _get_unsharded_module_names_helper(</dt><dd><p>child,
curr_path + “.”,
unsharded_module_names,</p>
</dd>
</dl>
<p>)
if child_sharded:</p>
<blockquote>
<div><p>sharded_children.add(name)</p>
</div></blockquote>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>if len(sharded_children) &gt; 0:</dt><dd><dl class="simple">
<dt>for name, _ in model.named_children():</dt><dd><dl class="simple">
<dt>if name not in sharded_children:</dt><dd><p>unsharded_module_names.add(path + name)</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
<p>return len(sharded_children) &gt; 0</p>
</dd>
<dt>def _get_unsharded_module_names(model: torch.nn.Module) -&gt; List[str]:</dt><dd><p>“””
Returns a list of top level modules do not contain any sharded sub modules.
“””</p>
<p>unsharded_module_names: Set[str] = set()
_get_unsharded_module_names_helper(</p>
<blockquote>
<div><p>model,
“”,
unsharded_module_names,</p>
</div></blockquote>
<p>)
return list(unsharded_module_names)</p>
</dd>
<dt>def _rewrite_model(  # noqa C901</dt><dd><p>model: torch.nn.Module,
context: TrainPipelineContext,
dist_stream: Optional[torch.cuda.streams.Stream],</p>
</dd>
</dl>
<p>) -&gt; List[ShardedModule]:</p>
<blockquote>
<div><p># Get underlying nn.Module
if isinstance(model, DistributedModelParallel):</p>
<blockquote>
<div><p>model = model.module</p>
</div></blockquote>
<p># Collect a list of sharded modules.
sharded_modules = {}
fp_modules = {}
for name, m in model.named_modules():</p>
<blockquote>
<div><dl class="simple">
<dt>if isinstance(m, ShardedModule):</dt><dd><p>sharded_modules[name] = m</p>
</dd>
<dt>if isinstance(m, BaseGroupedFeatureProcessor):</dt><dd><p>fp_modules[name] = m</p>
</dd>
</dl>
</div></blockquote>
<p># Trace a model.
tracer = Tracer(leaf_modules=_get_unsharded_module_names(model))
graph = tracer.trace(model)</p>
<p>feature_processor_nodes = []
# find the fp node
for node in graph.nodes:</p>
<blockquote>
<div><dl class="simple">
<dt>if node.op == “call_module” and node.target in fp_modules:</dt><dd><p>feature_processor_nodes.append(node)</p>
</dd>
</dl>
</div></blockquote>
<p># Select sharded modules, which are top-level in the forward call graph,
# i.e. which don’t have input transformations, i.e.
# rely only on ‘builtins.getattr’.
ret = []
for node in graph.nodes:</p>
<blockquote>
<div><dl>
<dt>if node.op == “call_module” and node.target in sharded_modules:</dt><dd><p>total_num_args = len(node.args) + len(node.kwargs)
if total_num_args == 0:</p>
<blockquote>
<div><p>continue</p>
</div></blockquote>
<p>arg_info_list, num_found = _get_node_args(node, feature_processor_nodes)
if num_found == total_num_args:</p>
<blockquote>
<div><p>logger.info(f”Module ‘{node.target}’’ will be pipelined”)
child = sharded_modules[node.target]
child.forward = PipelinedForward(</p>
<blockquote>
<div><p>node.target,
arg_info_list,
child,
context,
dist_stream,</p>
</div></blockquote>
<p>)
ret.append(child)</p>
</div></blockquote>
</dd>
</dl>
</div></blockquote>
<p>return ret</p>
</div></blockquote>
<dl>
<dt>class TrainPipelineSparseDist(TrainPipeline[In, Out]):</dt><dd><p>“””
This pipeline overlaps device transfer, and <cite>ShardedModule.input_dist()</cite> with
forward and backward. This helps hide the all2all latency while preserving the
training forward / backward ordering.</p>
<p>stage 3: forward, backward - uses default CUDA stream
stage 2: ShardedModule.input_dist() - uses data_dist CUDA stream
stage 1: device transfer - uses memcpy CUDA stream</p>
<p><cite>ShardedModule.input_dist()</cite> is only done for top-level modules in the call graph.
To be considered a top-level module, a module can only depend on ‘getattr’ calls on
input.</p>
<p>Input model must be symbolically traceable with the exception of <cite>ShardedModule</cite> and
<cite>DistributedDataParallel</cite> modules.
“””</p>
<p>synced_pipeline_id: Dict[int, int] = {}</p>
<dl>
<dt>def __init__(</dt><dd><p>self,
model: torch.nn.Module,
optimizer: torch.optim.Optimizer,
device: torch.device,
enable_amp: bool = False,
enable_grad_scaling: bool = True,
grad_accum: Optional[int] = None,</p>
</dd>
<dt>) -&gt; None:</dt><dd><p>self._model = model
self._optimizer = optimizer
self._device = device
self._enable_amp = enable_amp
# NOTE: Pending upstream feedback, but two flags because we can run AMP without CUDA but cannot scale gradients without CUDA.
# Background on gradient/loss scaling
# <a class="reference external" href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#lossscaling">https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#lossscaling</a>
# <a class="reference external" href="https://pytorch.org/docs/stable/amp.html#gradient-scaling">https://pytorch.org/docs/stable/amp.html#gradient-scaling</a>
self._enable_grad_scaling = enable_grad_scaling
self._grad_scaler = torch.cuda.amp.GradScaler(</p>
<blockquote>
<div><p>enabled=self._enable_amp and self._enable_grad_scaling</p>
</div></blockquote>
<p>)
logging.info(f”Amp is enabled: {self._enable_amp}”)</p>
<p># use two data streams to support two concurrent batches
if device.type == “cuda”:</p>
<blockquote>
<div><p>self._memcpy_stream: Optional[torch.cuda.streams.Stream] = torch.cuda.Stream()
self._data_dist_stream: Optional[torch.cuda.streams.Stream] = torch.cuda.Stream()</p>
</div></blockquote>
<dl>
<dt>else:</dt><dd><dl class="simple">
<dt>if self._enable_amp:</dt><dd><p>logging.warning(“Amp is enabled, but no CUDA available”)</p>
</dd>
</dl>
<p>self._memcpy_stream: Optional[torch.cuda.streams.Stream] = None
self._data_dist_stream: Optional[torch.cuda.streams.Stream] = None</p>
</dd>
</dl>
<p>self._batch_i: Optional[In] = None
self._batch_ip1: Optional[In] = None
self._batch_ip2: Optional[In] = None
self._connected = False
self._context = TrainPipelineContext()
self._pipelined_modules: List[ShardedModule] = []</p>
<p>self._progress_calls = 0
if grad_accum is not None:</p>
<blockquote>
<div><p>assert isinstance(grad_accum, int) and grad_accum &gt; 0</p>
</div></blockquote>
<p>self._grad_accum = grad_accum</p>
</dd>
<dt>def _connect(self, dataloader_iter: Iterator[In]) -&gt; None:</dt><dd><p># batch 1
with torch.cuda.stream(self._memcpy_stream):</p>
<blockquote>
<div><p>batch_i = next(dataloader_iter)
self._batch_i = batch_i = _to_device(batch_i, self._device, non_blocking=True)
# Try to pipeline input data dist.
self._pipelined_modules = _rewrite_model(self._model, self._context, self._data_dist_stream)</p>
</div></blockquote>
<dl class="simple">
<dt>with torch.cuda.stream(self._data_dist_stream):</dt><dd><p>_wait_for_batch(batch_i, self._memcpy_stream)
_start_data_dist(self._pipelined_modules, batch_i, self._context)</p>
</dd>
</dl>
<p># batch 2
with torch.cuda.stream(self._memcpy_stream):</p>
<blockquote>
<div><p>batch_ip1 = next(dataloader_iter)
self._batch_ip1 = batch_ip1 = _to_device(batch_ip1, self._device, non_blocking=True)</p>
</div></blockquote>
<p>self._connected = True
self.__class__.synced_pipeline_id[id(self._model)] = id(self)</p>
</dd>
<dt>def progress(self, dataloader_iter: Iterator[In]) -&gt; Out:</dt><dd><p>“””
NOTE: This method has been updated to perform gradient accumulation.
If <cite>_grad_accum</cite> is set, then loss values are scaled by this amount and
optimizer update/reset is skipped for <cite>_grad_accum</cite> calls of <cite>progress</cite>
(congruent to training steps), and then update/reset on every <a href="#id11"><span class="problematic" id="id12">`</span></a>_grad_accum`th
step.</p>
<p>“””
should_step_optimizer = (</p>
<blockquote>
<div><p>self._grad_accum is not None
and self._progress_calls &gt; 0
and (self._progress_calls + 1) % self._grad_accum == 0</p>
</div></blockquote>
<p>) or self._grad_accum is None
should_reset_optimizer = (</p>
<blockquote>
<div><p>self._grad_accum is not None
and self._progress_calls &gt; 0
and (self._progress_calls + 2) % self._grad_accum == 0</p>
</div></blockquote>
<p>) or self._grad_accum is None</p>
<dl class="simple">
<dt>if not self._connected:</dt><dd><p>self._connect(dataloader_iter)</p>
</dd>
<dt>elif self.__class__.synced_pipeline_id.get(id(self._model), None) != id(self):</dt><dd><p>self._sync_pipeline()
self.__class__.synced_pipeline_id[id(self._model)] = id(self)</p>
</dd>
<dt>if self._model.training and should_reset_optimizer:</dt><dd><dl class="simple">
<dt>with record_function(“## zero_grad ##”):</dt><dd><p>self._optimizer.zero_grad()</p>
</dd>
</dl>
</dd>
<dt>with record_function(“## copy_batch_to_gpu ##”):</dt><dd><dl class="simple">
<dt>with torch.cuda.stream(self._memcpy_stream):</dt><dd><p>batch_ip2 = next(dataloader_iter)
self._batch_ip2 = batch_ip2 = _to_device(batch_ip2, self._device, non_blocking=True)</p>
</dd>
</dl>
</dd>
</dl>
<p>batch_i = cast(In, self._batch_i)
batch_ip1 = cast(In, self._batch_ip1)</p>
<dl class="simple">
<dt>with record_function(“## wait_for_batch ##”):</dt><dd><p>_wait_for_batch(batch_i, self._data_dist_stream)</p>
</dd>
</dl>
<p># Forward
with record_function(“## forward ##”):</p>
<blockquote>
<div><p># if using multiple streams (ie. CUDA), create an event in default stream
# before starting forward pass
if self._data_dist_stream:</p>
<blockquote>
<div><p>event = torch.cuda.current_stream().record_event()</p>
</div></blockquote>
<dl>
<dt>if self._enable_amp:</dt><dd><p># conditionally apply the model to the batch in the autocast context
# it appears that <cite>enabled=self._enable_amp</cite> should handle this,
# but it does not.
with torch.autocast(</p>
<blockquote>
<div><p>device_type=self._device.type,
dtype=torch.bfloat16,
enabled=self._enable_amp,</p>
</div></blockquote>
<dl class="simple">
<dt>):</dt><dd><p>(losses, output) = cast(Tuple[torch.Tensor, Out], self._model(batch_i))</p>
</dd>
</dl>
</dd>
<dt>else:</dt><dd><p>(losses, output) = cast(Tuple[torch.Tensor, Out], self._model(batch_i))</p>
</dd>
</dl>
</div></blockquote>
<p># Data Distribution
with record_function(“## sparse_data_dist ##”):</p>
<blockquote>
<div><dl>
<dt>with torch.cuda.stream(self._data_dist_stream):</dt><dd><p>_wait_for_batch(batch_ip1, self._memcpy_stream)
# Ensure event in default stream has been called before
# starting data dist
if self._data_dist_stream:</p>
<blockquote>
<div><p># pyre-ignore [61]: Local variable <cite>event</cite> is undefined, or not always defined
self._data_dist_stream.wait_event(event)</p>
</div></blockquote>
<p>_start_data_dist(self._pipelined_modules, batch_ip1, self._context)</p>
</dd>
</dl>
</div></blockquote>
<dl>
<dt>if self._model.training:</dt><dd><p># Backward
with record_function(“## backward ##”):</p>
<blockquote>
<div><p># Loss is normalize by number of accumulation steps.
# The reported loss in <cite>output[‘loss’]</cite> remains the unnormalized value.
if self._grad_accum is not None:</p>
<blockquote>
<div><p>losses = losses / self._grad_accum</p>
</div></blockquote>
<p>self._grad_scaler.scale(torch.sum(losses, dim=0)).backward()</p>
</div></blockquote>
<dl>
<dt>if should_step_optimizer:</dt><dd><p># Update
with record_function(“## optimizer ##”):</p>
<blockquote>
<div><p>self._grad_scaler.step(self._optimizer)
self._grad_scaler.update()</p>
</div></blockquote>
</dd>
</dl>
</dd>
</dl>
<p>self._batch_i = batch_ip1
self._batch_ip1 = batch_ip2</p>
<dl class="simple">
<dt>if self._model.training:</dt><dd><p>self._progress_calls += 1</p>
</dd>
</dl>
<p>return output</p>
</dd>
<dt>def _sync_pipeline(self) -&gt; None:</dt><dd><p>“””
Syncs <cite>PipelinedForward</cite> for sharded modules with context and dist stream of the
current train pipeline. Used when switching between train pipelines for the same
model.
“””
for module in self._pipelined_modules:</p>
<blockquote>
<div><p>module.forward._context = self._context
module.forward._dist_stream = self._data_dist_stream</p>
</div></blockquote>
</dd>
</dl>
</dd>
</dl>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../_sources/core/train_pipeline.py.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>