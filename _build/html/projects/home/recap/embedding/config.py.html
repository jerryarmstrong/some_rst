<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>from typing import List, Optional
import tml.core.config as base_config
from tml.optimizers import config as optimizer_config</p>
<p>import pydantic</p>
<dl>
<dt>class EmbeddingSnapshot(base_config.BaseConfig):</dt><dd><p>“””Configuration for Embedding snapshot”””</p>
<dl class="simple">
<dt>emb_name: str = pydantic.Field(</dt><dd><p>…, description=”Name of the embedding table from the loaded snapshot”</p>
</dd>
</dl>
<p>)
embedding_snapshot_uri: str = pydantic.Field(</p>
<blockquote>
<div><p>…, description=”Path to torchsnapshot of the embedding”</p>
</div></blockquote>
<p>)</p>
</dd>
</dl>
<p># <a class="reference external" href="https://pytorch.org/torchrec/torchrec.modules.html#torchrec.modules.embedding_configs.EmbeddingBagConfig">https://pytorch.org/torchrec/torchrec.modules.html#torchrec.modules.embedding_configs.EmbeddingBagConfig</a>
class EmbeddingBagConfig(base_config.BaseConfig):</p>
<blockquote>
<div><p>“””Configuration for EmbeddingBag.”””</p>
<p>name: str = pydantic.Field(…, description=”name of embedding bag”)
num_embeddings: int = pydantic.Field(…, description=”size of embedding dictionary”)
embedding_dim: int = pydantic.Field(…, description=”size of each embedding vector”)
pretrained: EmbeddingSnapshot = pydantic.Field(None, description=”Snapshot properties”)
vocab: str = pydantic.Field(</p>
<blockquote>
<div><p>None, description=”Directory to parquet files of mapping from entity ID to table index.”</p>
</div></blockquote>
<p>)</p>
</div></blockquote>
<dl>
<dt>class EmbeddingOptimizerConfig(base_config.BaseConfig):</dt><dd><dl class="simple">
<dt>learning_rate: optimizer_config.LearningRate = pydantic.Field(</dt><dd><p>None, description=”learning rate scheduler for the EBC”</p>
</dd>
</dl>
<p>)
init_learning_rate: float = pydantic.Field(description=”initial learning rate for the EBC”)
# NB: Only sgd is supported right now and implicitly.
# FBGemm only supports simple exact_sgd which only takes LR as an argument.</p>
</dd>
<dt>class LargeEmbeddingsConfig(base_config.BaseConfig):</dt><dd><p>“””Configuration for EmbeddingBagCollection.</p>
<p>The tables listed in this config are gathered into a single torchrec EmbeddingBagCollection.
“””</p>
<p>tables: List[EmbeddingBagConfig] = pydantic.Field(…, description=”list of embedding tables”)
optimizer: EmbeddingOptimizerConfig
tables_to_log: List[str] = pydantic.Field(</p>
<blockquote>
<div><p>None, description=”list of embedding table names that we want to log during training”</p>
</div></blockquote>
<p>)</p>
</dd>
<dt>class StratifierConfig(base_config.BaseConfig):</dt><dd><p>name: str
index: int
value: int</p>
</dd>
<dt>class SmallEmbeddingBagConfig(base_config.BaseConfig):</dt><dd><p>“””Configuration for SmallEmbeddingBag.”””</p>
<p>name: str = pydantic.Field(…, description=”name of embedding bag”)
num_embeddings: int = pydantic.Field(…, description=”size of embedding dictionary”)
embedding_dim: int = pydantic.Field(…, description=”size of each embedding vector”)
index: int = pydantic.Field(…, description=”index in the discrete tensor to look for”)</p>
</dd>
<dt>class SmallEmbeddingBagConfig(base_config.BaseConfig):</dt><dd><p>“””Configuration for SmallEmbeddingBag.”””</p>
<p>name: str = pydantic.Field(…, description=”name of embedding bag”)
num_embeddings: int = pydantic.Field(…, description=”size of embedding dictionary”)
embedding_dim: int = pydantic.Field(…, description=”size of each embedding vector”)
index: int = pydantic.Field(…, description=”index in the discrete tensor to look for”)</p>
</dd>
<dt>class SmallEmbeddingsConfig(base_config.BaseConfig):</dt><dd><p>“””Configuration for SmallEmbeddingConfig.</p>
<p>Here we can use discrete features that already are present in our TFRecords generated using
segdense conversion as “home_recap_2022_discrete__segdense_vals” which are available in
the model as “discrete_features”, and embed a user-defined set of them with configurable
dimensions and vocabulary sizes.</p>
<p>Compared with LargeEmbedding, this config is for small embedding tables that can fit inside
the model, whereas LargeEmbedding usually is meant to be hydrated outside the model at
serving time due to size (&gt;&gt;1 GB).</p>
<p>This small embeddings table uses the same optimizer as the rest of the model.”””</p>
<dl class="simple">
<dt>tables: List[SmallEmbeddingBagConfig] = pydantic.Field(</dt><dd><p>…, description=”list of embedding tables”</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../index2.rst.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../index2.rst.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../../_sources/projects/home/recap/embedding/config.py.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>