<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../../../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../../../../../" id="documentation_options" src="../../../../../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>package com.twitter.search.earlybird.partition;</p>
<p>import java.io.Closeable;
import java.time.Duration;
import java.util.Map;
import java.util.concurrent.atomic.AtomicBoolean;</p>
<p>import com.google.common.annotations.VisibleForTesting;
import com.google.common.base.Preconditions;
import com.google.common.base.Stopwatch;
import com.google.common.collect.ImmutableList;</p>
<p>import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.errors.ApiException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;</p>
<p>import com.twitter.search.common.indexing.thriftjava.ThriftVersionedEvents;
import com.twitter.search.common.metrics.SearchCounter;
import com.twitter.search.common.metrics.SearchRateCounter;
import com.twitter.search.common.metrics.SearchTimer;
import com.twitter.search.common.metrics.SearchTimerStats;
import com.twitter.search.common.util.LogFormatUtil;
import com.twitter.search.earlybird.EarlybirdStatus;
import com.twitter.search.earlybird.common.CaughtUpMonitor;
import com.twitter.search.earlybird.exception.CriticalExceptionHandler;
import com.twitter.search.earlybird.exception.WrappedKafkaApiException;
import com.twitter.search.earlybird.thrift.EarlybirdStatusCode;</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Reads TVEs from Kafka and writes them to a PartitionWriter.</p></li>
</ul>
<p><a href="#id1"><span class="problematic" id="id2">*</span></a>/</p>
</dd>
<dt>public class EarlybirdKafkaConsumer implements Closeable {</dt><dd><p>private static final Logger LOG = LoggerFactory.getLogger(EarlybirdKafkaConsumer.class);</p>
<p>private static final Duration POLL_TIMEOUT = Duration.ofSeconds(1);
private static final String STATS_PREFIX = “<a href="#id13"><span class="problematic" id="id14">earlybird_kafka_consumer_</span></a>”;</p>
<p>// See SEARCH-31827
private static final SearchCounter INGESTING_DONE =</p>
<blockquote>
<div><p>SearchCounter.export(STATS_PREFIX + “ingesting_done”);</p>
</div></blockquote>
<dl class="simple">
<dt>private static final SearchRateCounter POLL_LOOP_EXCEPTIONS =</dt><dd><p>SearchRateCounter.export(STATS_PREFIX + “poll_loop_exceptions”);</p>
</dd>
<dt>private static final SearchRateCounter FLUSHING_EXCEPTIONS =</dt><dd><p>SearchRateCounter.export(STATS_PREFIX + “flushing_exceptions”);</p>
</dd>
<dt>private static final SearchTimerStats TIMED_POLLS =</dt><dd><p>SearchTimerStats.export(STATS_PREFIX + “timed_polls”);</p>
</dd>
<dt>private static final SearchTimerStats TIMED_INDEX_EVENTS =</dt><dd><p>SearchTimerStats.export(STATS_PREFIX + “timed_index_events”);</p>
</dd>
</dl>
<p>private final AtomicBoolean running = new AtomicBoolean(true);
private final BalancingKafkaConsumer balancingKafkaConsumer;
private final PartitionWriter partitionWriter;
protected final TopicPartition tweetTopic;
protected final TopicPartition updateTopic;
private final KafkaConsumer&lt;Long, ThriftVersionedEvents&gt; underlyingKafkaConsumer;
private final CriticalExceptionHandler criticalExceptionHandler;
private final EarlybirdIndexFlusher earlybirdIndexFlusher;
private final SearchIndexingMetricSet searchIndexingMetricSet;
private boolean finishedIngestUntilCurrent;
private final CaughtUpMonitor indexCaughtUpMonitor;</p>
<dl>
<dt>protected class ConsumeBatchResult {</dt><dd><p>private boolean isCaughtUp;
private long readRecordsCount;</p>
<dl class="simple">
<dt>public ConsumeBatchResult(boolean isCaughtUp, long readRecordsCount) {</dt><dd><p>this.isCaughtUp = isCaughtUp;
this.readRecordsCount = readRecordsCount;</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>public boolean isCaughtUp() {</dt><dd><p>return isCaughtUp;</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>public long getReadRecordsCount() {</dt><dd><p>return readRecordsCount;</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>public EarlybirdKafkaConsumer(</dt><dd><p>KafkaConsumer&lt;Long, ThriftVersionedEvents&gt; underlyingKafkaConsumer,
SearchIndexingMetricSet searchIndexingMetricSet,
CriticalExceptionHandler criticalExceptionHandler,
PartitionWriter partitionWriter,
TopicPartition tweetTopic,
TopicPartition updateTopic,
EarlybirdIndexFlusher earlybirdIndexFlusher,
CaughtUpMonitor kafkaIndexCaughtUpMonitor</p>
</dd>
<dt>) {</dt><dd><p>this.partitionWriter = partitionWriter;
this.underlyingKafkaConsumer = underlyingKafkaConsumer;
this.criticalExceptionHandler = criticalExceptionHandler;
this.searchIndexingMetricSet = searchIndexingMetricSet;
this.tweetTopic = tweetTopic;
this.updateTopic = updateTopic;
this.earlybirdIndexFlusher = earlybirdIndexFlusher;</p>
<p>LOG.info(“Reading from Kafka topics: tweetTopic={}, updateTopic={}”, tweetTopic, updateTopic);
underlyingKafkaConsumer.assign(ImmutableList.of(updateTopic, tweetTopic));</p>
<dl class="simple">
<dt>this.balancingKafkaConsumer =</dt><dd><p>new BalancingKafkaConsumer(underlyingKafkaConsumer, tweetTopic, updateTopic);</p>
</dd>
</dl>
<p>this.finishedIngestUntilCurrent = false;
this.indexCaughtUpMonitor = kafkaIndexCaughtUpMonitor;</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Run the consumer, indexing from Kafka.</p></li>
</ul>
<p><a href="#id3"><span class="problematic" id="id4">*</span></a>/</p>
</dd>
</dl>
<p>&#64;VisibleForTesting
public void run() {</p>
<blockquote>
<div><dl class="simple">
<dt>while (isRunning()) {</dt><dd><p>ConsumeBatchResult result = consumeBatch(true);
indexCaughtUpMonitor.setAndNotify(result.isCaughtUp());</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Reads from Kafka, starting at the given offsets, and applies the events until we are caught up</p></li>
<li><p>with the current streams.</p></li>
</ul>
<p><a href="#id5"><span class="problematic" id="id6">*</span></a>/</p>
</dd>
<dt>public void ingestUntilCurrent(long tweetOffset, long updateOffset) {</dt><dd><p>Preconditions.checkState(!finishedIngestUntilCurrent);
Stopwatch stopwatch = Stopwatch.createStarted();
LOG.info(“Ingest until current: seeking to Kafka offset {} for tweets and {} for updates.”,</p>
<blockquote>
<div><p>tweetOffset, updateOffset);</p>
</div></blockquote>
<dl class="simple">
<dt>try {</dt><dd><p>underlyingKafkaConsumer.seek(tweetTopic, tweetOffset);
underlyingKafkaConsumer.seek(updateTopic, updateOffset);</p>
</dd>
<dt>} catch (ApiException kafkaApiException) {</dt><dd><dl class="simple">
<dt>throw new WrappedKafkaApiException(“Can’t seek to tweet and update offsets”,</dt><dd><p>kafkaApiException);</p>
</dd>
</dl>
</dd>
</dl>
<p>}</p>
<p>Map&lt;TopicPartition, Long&gt; endOffsets;
try {</p>
<blockquote>
<div><p>endOffsets = underlyingKafkaConsumer.endOffsets(ImmutableList.of(tweetTopic, updateTopic));</p>
</div></blockquote>
<dl class="simple">
<dt>} catch (ApiException kafkaApiException) {</dt><dd><dl class="simple">
<dt>throw new WrappedKafkaApiException(“Can’t find end offsets”,</dt><dd><p>kafkaApiException);</p>
</dd>
</dl>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>if (endOffsets.size() &gt; 0) {</dt><dd><dl class="simple">
<dt>LOG.info(String.format(“Records until current: tweets=%,d, updates=%,d”,</dt><dd><p>endOffsets.get(tweetTopic) - tweetOffset + 1,
endOffsets.get(updateTopic) - updateOffset + 1));</p>
</dd>
</dl>
</dd>
</dl>
<p>}</p>
<p>consumeBatchesUntilCurrent(true);</p>
<p>LOG.info(“ingestUntilCurrent finished in {}.”, stopwatch);</p>
<p>partitionWriter.logState();
INGESTING_DONE.increment();
finishedIngestUntilCurrent = true;</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Consume tweets and updates from streams until we’re up to date.</p></li>
<li></li>
<li><p>&#64;return total number of read records.</p></li>
</ul>
<p><a href="#id7"><span class="problematic" id="id8">*</span></a>/</p>
</dd>
<dt>private long consumeBatchesUntilCurrent(boolean flushingEnabled) {</dt><dd><p>long totalRecordsRead = 0;
long batchesConsumed = 0;</p>
<dl>
<dt>while (isRunning()) {</dt><dd><p>ConsumeBatchResult result = consumeBatch(flushingEnabled);
batchesConsumed++;
totalRecordsRead += result.getReadRecordsCount();
if (isCurrent(result.isCaughtUp())) {</p>
<blockquote>
<div><p>break;</p>
</div></blockquote>
<p>}</p>
</dd>
</dl>
<p>}</p>
<p>LOG.info(“Processed batches: {}”, batchesConsumed);</p>
<p>return totalRecordsRead;</p>
</dd>
</dl>
<p>}</p>
<p>// This method is overriden in MockEarlybirdKafkaConsumer.
public boolean isCurrent(boolean current) {</p>
<blockquote>
<div><p>return current;</p>
</div></blockquote>
<p>}</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>We don’t index during flushing, so after the flush is done, the index is stale.</p></li>
<li><p>We need to get to current, before we rejoin the serverset so that upon rejoining we’re</p></li>
<li><p>not serving a stale index.</p></li>
</ul>
<p><a href="#id9"><span class="problematic" id="id10">*</span></a>/</p>
</dd>
</dl>
<p>&#64;VisibleForTesting
void getToCurrentPostFlush() {</p>
<blockquote>
<div><p>LOG.info(“Getting to current post flush”);
Stopwatch stopwatch = Stopwatch.createStarted();</p>
<p>long totalRecordsRead = consumeBatchesUntilCurrent(false);</p>
<dl class="simple">
<dt>LOG.info(“Post flush, became current in: {}, after reading {} records.”,</dt><dd><p>stopwatch, LogFormatUtil.formatInt(totalRecordsRead));</p>
</dd>
</dl>
</div></blockquote>
<p>}</p>
<dl>
<dt>/*</dt><dd><ul class="simple">
<li><p>&#64;return true if we are current after indexing this batch.</p></li>
</ul>
<p><a href="#id11"><span class="problematic" id="id12">*</span></a>/</p>
</dd>
</dl>
<p>&#64;VisibleForTesting
protected ConsumeBatchResult consumeBatch(boolean flushingEnabled) {</p>
<blockquote>
<div><p>long readRecordsCount = 0;
boolean isCaughtUp = false;</p>
<dl>
<dt>try {</dt><dd><p>// Poll.
SearchTimer pollTimer = TIMED_POLLS.startNewTimer();
ConsumerRecords&lt;Long, ThriftVersionedEvents&gt; records =</p>
<blockquote>
<div><p>balancingKafkaConsumer.poll(POLL_TIMEOUT);</p>
</div></blockquote>
<p>readRecordsCount += records.count();
TIMED_POLLS.stopTimerAndIncrement(pollTimer);</p>
<p>// Index.
SearchTimer indexTimer = TIMED_INDEX_EVENTS.startNewTimer();
isCaughtUp = partitionWriter.indexBatch(records);
TIMED_INDEX_EVENTS.stopTimerAndIncrement(indexTimer);</p>
</dd>
<dt>} catch (Exception ex) {</dt><dd><p>POLL_LOOP_EXCEPTIONS.increment();
LOG.error(“Exception in poll loop”, ex);</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>try {</dt><dd><p>// Possibly flush the index.
if (isCaughtUp &amp;&amp; flushingEnabled) {</p>
<blockquote>
<div><p>long tweetOffset = 0;
long updateOffset = 0;</p>
<dl class="simple">
<dt>try {</dt><dd><p>tweetOffset = underlyingKafkaConsumer.position(tweetTopic);
updateOffset = underlyingKafkaConsumer.position(updateTopic);</p>
</dd>
<dt>} catch (ApiException kafkaApiException) {</dt><dd><p>throw new WrappedKafkaApiException(“can’t get topic positions”, kafkaApiException);</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>EarlybirdIndexFlusher.FlushAttemptResult flushAttemptResult =</dt><dd><dl class="simple">
<dt>earlybirdIndexFlusher.flushIfNecessary(</dt><dd><p>tweetOffset, updateOffset, this::getToCurrentPostFlush);</p>
</dd>
</dl>
</dd>
<dt>if (flushAttemptResult == EarlybirdIndexFlusher.FlushAttemptResult.FLUSH_ATTEMPT_MADE) {</dt><dd><p>// Viz might show this as a fairly high number, so we’re printing it here to confirm
// the value on the server.
LOG.info(“Finished flushing. Index freshness in ms: {}”,</p>
<blockquote>
<div><p>LogFormatUtil.formatInt(searchIndexingMetricSet.getIndexFreshnessInMillis()));</p>
</div></blockquote>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>if (!finishedIngestUntilCurrent) {</dt><dd><dl class="simple">
<dt>LOG.info(“Became current on startup. Tried to flush with result: {}”,</dt><dd><p>flushAttemptResult);</p>
</dd>
</dl>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<p>}</p>
</dd>
<dt>} catch (Exception ex) {</dt><dd><p>FLUSHING_EXCEPTIONS.increment();
LOG.error(“Exception while flushing”, ex);</p>
</dd>
</dl>
<p>}</p>
<p>return new ConsumeBatchResult(isCaughtUp, readRecordsCount);</p>
</div></blockquote>
<p>}</p>
<dl class="simple">
<dt>public boolean isRunning() {</dt><dd><p>return running.get() &amp;&amp; EarlybirdStatus.getStatusCode() != EarlybirdStatusCode.STOPPING;</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>public void prepareAfterStartingWithIndex(long maxIndexedTweetId) {</dt><dd><p>partitionWriter.prepareAfterStartingWithIndex(maxIndexedTweetId);</p>
</dd>
</dl>
<p>}</p>
<dl class="simple">
<dt>public void close() {</dt><dd><p>balancingKafkaConsumer.close();
running.set(false);</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../../../../index.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../../../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../../../../../_sources/src/java/com/twitter/search/earlybird/partition/EarlybirdKafkaConsumer.java.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>