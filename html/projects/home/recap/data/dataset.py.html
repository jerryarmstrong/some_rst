<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>from dataclasses import dataclass
from typing import Callable, List, Optional, Tuple, Dict
import functools</p>
<p>import torch
import tensorflow as tf</p>
<p>from tml.common.batch import DataclassBatch
from tml.projects.home.recap.data.config import RecapDataConfig, TaskData
from tml.projects.home.recap.data import preprocessors
from tml.projects.home.recap.config import JobMode
from tml.projects.home.recap.data.tfe_parsing import get_seg_dense_parse_fn
from tml.projects.home.recap.data.util import (</p>
<blockquote>
<div><p>keyed_jagged_tensor_from_tensors_dict,
sparse_or_dense_tf_to_torch,</p>
</div></blockquote>
<p>)
from absl import logging
import torch.distributed as dist</p>
<p>&#64;dataclass
class RecapBatch(DataclassBatch):</p>
<blockquote>
<div><p>“””Holds features and labels from the Recap dataset.”””</p>
<p>continuous_features: torch.Tensor
binary_features: torch.Tensor
discrete_features: torch.Tensor
sparse_features: “KeyedJaggedTensor”  # type: ignore[name-defined]  # noqa: F821
labels: torch.Tensor
user_embedding: torch.Tensor = None
user_eng_embedding: torch.Tensor = None
author_embedding: torch.Tensor = None
weights: torch.Tensor = None</p>
<dl class="simple">
<dt>def __post_init__(self):</dt><dd><dl class="simple">
<dt>if self.weights is None:</dt><dd><p>self.weights = torch.ones_like(self.labels)</p>
</dd>
<dt>for feature_name, feature_value in self.as_dict().items():</dt><dd><dl class="simple">
<dt>if (“embedding” in feature_name) and (feature_value is None):</dt><dd><p>setattr(self, feature_name, torch.empty([0, 0]))</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<dl>
<dt>def to_batch(x, sparse_feature_names: Optional[List[str]] = None) -&gt; RecapBatch:</dt><dd><p>“””Converts a torch data loader output into <cite>RecapBatch</cite>.”””</p>
<p>x = tf.nest.map_structure(functools.partial(sparse_or_dense_tf_to_torch, pin_memory=False), x)
try:</p>
<blockquote>
<div><p>features_in, labels = x</p>
</div></blockquote>
<dl class="simple">
<dt>except ValueError:</dt><dd><p># For Mode.INFERENCE, we do not expect to recieve labels as part of the input tuple
features_in, labels = x, None</p>
</dd>
</dl>
<p>sparse_features = keyed_jagged_tensor_from_tensors_dict({})
if sparse_feature_names:</p>
<blockquote>
<div><dl class="simple">
<dt>sparse_features = keyed_jagged_tensor_from_tensors_dict(</dt><dd><p>{embedding_name: features_in[embedding_name] for embedding_name in sparse_feature_names}</p>
</dd>
</dl>
<p>)</p>
</div></blockquote>
<p>user_embedding, user_eng_embedding, author_embedding = None, None, None
if “user_embedding” in features_in:</p>
<blockquote>
<div><dl class="simple">
<dt>if sparse_feature_names and “meta__user_id” in sparse_feature_names:</dt><dd><p>raise ValueError(“Only one source of embedding for user is supported”)</p>
</dd>
<dt>else:</dt><dd><p>user_embedding = features_in[“user_embedding”]</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>if “user_eng_embedding” in features_in:</dt><dd><dl class="simple">
<dt>if sparse_feature_names and “meta__user_eng_id” in sparse_feature_names:</dt><dd><p>raise ValueError(“Only one source of embedding for user is supported”)</p>
</dd>
<dt>else:</dt><dd><p>user_eng_embedding = features_in[“user_eng_embedding”]</p>
</dd>
</dl>
</dd>
<dt>if “author_embedding” in features_in:</dt><dd><dl class="simple">
<dt>if sparse_feature_names and “meta__author_id” in sparse_feature_names:</dt><dd><p>raise ValueError(“Only one source of embedding for user is supported”)</p>
</dd>
<dt>else:</dt><dd><p>author_embedding = features_in[“author_embedding”]</p>
</dd>
</dl>
</dd>
<dt>return RecapBatch(</dt><dd><p>continuous_features=features_in[“continuous”],
binary_features=features_in[“binary”],
discrete_features=features_in[“discrete”],
sparse_features=sparse_features,
user_embedding=user_embedding,
user_eng_embedding=user_eng_embedding,
author_embedding=author_embedding,
labels=labels,
weights=features_in.get(“weights”, None),  # Defaults to torch.ones_like(labels)</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>def _chain(param, f1, f2):</dt><dd><p>“””
Reduce multiple functions into one chained function
_chain(x, f1, f2) -&gt; f2(f1(x))
“””
output = param
fns = [f1, f2]
for f in fns:</p>
<blockquote>
<div><p>output = f(output)</p>
</div></blockquote>
<p>return output</p>
</dd>
<dt>def _add_weights(inputs, tasks: Dict[str, TaskData]):</dt><dd><p>“””Adds weights based on label sampling for positive and negatives.</p>
<p>This is useful for numeric calibration etc. This mutates inputs.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>inputs: A dictionary of strings to tensor-like structures.
tasks: A dict of string (label) to <cite>TaskData</cite> specifying inputs.</p>
</dd>
<dt>Returns:</dt><dd><p>A tuple of features and labels; weights are added to features.</p>
</dd>
</dl>
<p>“””</p>
<p>weights = []
for key, task in tasks.items():</p>
<blockquote>
<div><p>label = inputs[key]
float_label = tf.cast(label, tf.float32)</p>
<dl class="simple">
<dt>weights.append(</dt><dd><p>float_label / task.pos_downsampling_rate + (1.0 - float_label) / task.neg_downsampling_rate</p>
</dd>
</dl>
<p>)</p>
</div></blockquote>
<p># Ensure we are batch-major (assumes we batch before this call).
inputs[“weights”] = tf.squeeze(tf.transpose(tf.convert_to_tensor(weights)), axis=0)
return inputs</p>
</dd>
<dt>def get_datetimes(explicit_datetime_inputs):</dt><dd><p>“””Compute list datetime strings for train/validation data.”””
datetime_format = “%Y/%m/%d/%H”
end = datetime.strptime(explicit_datetime_inputs.end_datetime, datetime_format)
dates = sorted(</p>
<blockquote>
<div><dl class="simple">
<dt>[</dt><dd><p>(end - timedelta(hours=i + 1)).strftime(datetime_format)
for i in range(int(explicit_datetime_inputs.hours))</p>
</dd>
</dl>
<p>]</p>
</div></blockquote>
<p>)
return dates</p>
</dd>
<dt>def get_explicit_datetime_inputs_files(explicit_datetime_inputs):</dt><dd><p>“””
Compile list of files for training/validation.</p>
<p>Used with DataConfigs that use the <cite>explicit_datetime_inputs</cite> format to specify data.
For each hour of data, if the directory is missing or empty, we increment a counter to keep
track of the number of missing data hours.
Returns only files with a <cite>.gz</cite> extension.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>explicit_datetime_inputs: An <cite>ExplicitDatetimeInputs</cite> object within a <cite>datasets.DataConfig</cite> object</p>
</dd>
<dt>Returns:</dt><dd><p>data_files: Sorted list of files to read corresponding to data at the desired datetimes
num_hours_missing: Number of hours that we are missing data</p>
</dd>
</dl>
<p>“””
datetimes = get_datetimes(explicit_datetime_inputs)
folders = [os.path.join(explicit_datetime_inputs.data_root, datetime) for datetime in datetimes]
data_files = []
num_hours_missing = 0
for folder in folders:</p>
<blockquote>
<div><dl>
<dt>try:</dt><dd><p>files = tf.io.gfile.listdir(folder)
if not files:</p>
<blockquote>
<div><p>logging.warning(f”{folder} contained no data files”)
num_hours_missing += 1</p>
</div></blockquote>
<dl>
<dt>data_files.extend(</dt><dd><dl class="simple">
<dt>[</dt><dd><p>os.path.join(folder, filename)
for filename in files
if filename.rsplit(“.”, 1)[-1].lower() == “gz”</p>
</dd>
</dl>
<p>]</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>except tf.errors.NotFoundError as e:</dt><dd><p>num_hours_missing += 1
logging.warning(f”Cannot find directory {folder}. Missing one hour of data. Error: n {e}”)</p>
</dd>
</dl>
</div></blockquote>
<p>return sorted(data_files), num_hours_missing</p>
</dd>
<dt>def _map_output_for_inference(</dt><dd><p>inputs, tasks: Dict[str, TaskData], preprocessor: tf.keras.Model = None, add_weights: bool = False</p>
</dd>
<dt>):</dt><dd><dl class="simple">
<dt>if preprocessor:</dt><dd><p>raise ValueError(“No preprocessor should be used at inference time.”)</p>
</dd>
<dt>if add_weights:</dt><dd><p>raise NotImplementedError()</p>
</dd>
</dl>
<p># Add zero weights.
inputs[“weights”] = tf.zeros_like(tf.expand_dims(inputs[“continuous”][:, 0], -1))
for label in tasks:</p>
<blockquote>
<div><p>del inputs[label]</p>
</div></blockquote>
<p>return inputs</p>
</dd>
<dt>def _map_output_for_train_eval(</dt><dd><p>inputs, tasks: Dict[str, TaskData], preprocessor: tf.keras.Model = None, add_weights: bool = False</p>
</dd>
<dt>):</dt><dd><dl class="simple">
<dt>if add_weights:</dt><dd><p>inputs = _add_weights_based_on_sampling_rates(inputs, tasks)</p>
</dd>
</dl>
<p># Warning this has to happen first as it changes the input
if preprocessor:</p>
<blockquote>
<div><p>inputs = preprocessor(inputs)</p>
</div></blockquote>
<p>label_values = tf.squeeze(tf.stack([inputs[label] for label in tasks], axis=1), axis=[-1])</p>
<dl class="simple">
<dt>for label in tasks:</dt><dd><p>del inputs[label]</p>
</dd>
</dl>
<p>return inputs, label_values</p>
</dd>
<dt>def _add_weights_based_on_sampling_rates(inputs, tasks: Dict[str, TaskData]):</dt><dd><p>“””Adds weights based on label sampling for positive and negatives.</p>
<p>This is useful for numeric calibration etc. This mutates inputs.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>inputs: A dictionary of strings to tensor-like structures.
tasks: A dict of string (label) to <cite>TaskData</cite> specifying inputs.</p>
</dd>
<dt>Returns:</dt><dd><p>A tuple of features and labels; weights are added to features.</p>
</dd>
</dl>
<p>“””
weights = []
for key, task in tasks.items():</p>
<blockquote>
<div><p>label = inputs[key]
float_label = tf.cast(label, tf.float32)</p>
<dl class="simple">
<dt>weights.append(</dt><dd><p>float_label / task.pos_downsampling_rate + (1.0 - float_label) / task.neg_downsampling_rate</p>
</dd>
</dl>
<p>)</p>
</div></blockquote>
<p># Ensure we are batch-major (assumes we batch before this call).
inputs[“weights”] = tf.squeeze(tf.transpose(tf.convert_to_tensor(weights)), axis=0)
return inputs</p>
</dd>
<dt>class RecapDataset(torch.utils.data.IterableDataset):</dt><dd><dl>
<dt>def __init__(</dt><dd><p>self,
data_config: RecapDataConfig,
dataset_service: Optional[str] = None,
mode: JobMode = JobMode.TRAIN,
compression: Optional[str] = “AUTO”,
repeat: bool = False,
vocab_mapper: tf.keras.Model = None,</p>
</dd>
<dt>):</dt><dd><p>logging.info(”<strong>*</strong> Labels <strong>*</strong>”)
logging.info(list(data_config.tasks.keys()))</p>
<p>self._data_config = data_config
self._parse_fn = get_seg_dense_parse_fn(data_config)
self._mode = mode
self._repeat = repeat
self._num_concurrent_iterators = 1
self._vocab_mapper = vocab_mapper
self.dataset_service = dataset_service</p>
<p>preprocessor = None
self._batch_size_multiplier = 1
if data_config.preprocess:</p>
<blockquote>
<div><p>preprocessor = preprocessors.build_preprocess(data_config.preprocess, mode=mode)
if data_config.preprocess.downsample_negatives:</p>
<blockquote>
<div><p>self._batch_size_multiplier = data_config.preprocess.downsample_negatives.batch_multiplier</p>
</div></blockquote>
</div></blockquote>
<p>self._preprocessor = preprocessor</p>
<dl>
<dt>if mode == JobMode.INFERENCE:</dt><dd><dl class="simple">
<dt>if preprocessor is not None:</dt><dd><p>raise ValueError(“Expect no preprocessor at inference time.”)</p>
</dd>
</dl>
<p>should_add_weights = False
output_map_fn = _map_output_for_inference  # (features,)</p>
</dd>
<dt>else:</dt><dd><p># Only add weights if there is a reason to! If all weights will
# be equal to 1.0, save bandwidth between DDS and Chief by simply
# relying on the fact that weights default to 1.0 in <cite>RecapBatch</cite>
# WARNING: Weights may still be added as a side effect of a preprocessor
#          such as <cite>DownsampleNegatives</cite>.
should_add_weights = any(</p>
<blockquote>
<div><dl class="simple">
<dt>[</dt><dd><p>task_cfg.pos_downsampling_rate != 1.0 or task_cfg.neg_downsampling_rate != 1.0
for task_cfg in data_config.tasks.values()</p>
</dd>
</dl>
<p>]</p>
</div></blockquote>
<p>)
output_map_fn = _map_output_for_train_eval  # (features, labels)</p>
</dd>
<dt>self._output_map_fn = functools.partial(</dt><dd><p>output_map_fn,
tasks=data_config.tasks,
preprocessor=preprocessor,
add_weights=should_add_weights,</p>
</dd>
</dl>
<p>)</p>
<p>sparse_feature_names = list(vocab_mapper.vocabs.keys()) if vocab_mapper else None</p>
<p>self._tf_dataset = self._create_tf_dataset()</p>
<p>self._init_tensor_spec()</p>
</dd>
<dt>def _init_tensor_spec(self):</dt><dd><dl>
<dt>def _tensor_spec_to_torch_shape(spec):</dt><dd><dl class="simple">
<dt>if spec.shape is None:</dt><dd><p>return None</p>
</dd>
</dl>
<p>shape = [x if x is not None else -1 for x in spec.shape]
return torch.Size(shape)</p>
</dd>
<dt>self.torch_element_spec = tf.nest.map_structure(</dt><dd><p>_tensor_spec_to_torch_shape, self._tf_dataset.element_spec</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>def _create_tf_dataset(self):</dt><dd><dl class="simple">
<dt>if hasattr(self, “_tf_dataset”):</dt><dd><p>raise ValueError(“Do not call <cite>_create_tf_dataset</cite> more than once.”)</p>
</dd>
</dl>
<p>world_size = dist.get_world_size() if dist.is_initialized() else 1
per_replica_bsz = (</p>
<blockquote>
<div><p>self._batch_size_multiplier * self._data_config.global_batch_size // world_size</p>
</div></blockquote>
<p>)</p>
<dl class="simple">
<dt>dataset: tf.data.Dataset = self._create_base_tf_dataset(</dt><dd><p>batch_size=per_replica_bsz,</p>
</dd>
</dl>
<p>)</p>
<dl>
<dt>if self._repeat:</dt><dd><p>logging.info(“Repeating dataset”)
dataset = dataset.repeat()</p>
</dd>
<dt>if self.dataset_service:</dt><dd><dl>
<dt>if self._num_concurrent_iterators &gt; 1:</dt><dd><dl>
<dt>if not self.machines_config:</dt><dd><dl class="simple">
<dt>raise ValueError(</dt><dd><p>“Must supply a machine_config for autotuning in order to use &gt;1 concurrent iterators”</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>dataset = dataset_lib.with_auto_tune_budget(</dt><dd><p>dataset,
machine_config=self.machines_config.chief,
num_concurrent_iterators=self.num_concurrent_iterators,
on_chief=False,</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>self.dataset_id, self.job_name = register_dataset(</dt><dd><p>dataset=dataset, dataset_service=self.dataset_service, compression=self.compression</p>
</dd>
</dl>
<p>)
dataset = distribute_from_dataset_id(</p>
<blockquote>
<div><p>dataset_id=self.dataset_id,  # type: ignore[arg-type]
job_name=self.job_name,
dataset_service=self.dataset_service,
compression=self.compression,</p>
</div></blockquote>
<p>)</p>
</dd>
<dt>elif self._num_concurrent_iterators &gt; 1:</dt><dd><dl>
<dt>if not self.machines_config:</dt><dd><dl class="simple">
<dt>raise ValueError(</dt><dd><p>“Must supply a machine_config for autotuning in order to use &gt;1 concurrent iterators”</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>dataset = dataset_lib.with_auto_tune_budget(</dt><dd><p>dataset,
machine_config=self.machines_config.chief,
num_concurrent_iterators=self._num_concurrent_iterators,
on_chief=True,</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
<p># Vocabulary mapping happens on the training node, not in dds because of size.
if self._vocab_mapper:</p>
<blockquote>
<div><p>dataset = dataset.map(self._vocab_mapper)</p>
</div></blockquote>
<p>return dataset.prefetch(world_size * 2)</p>
</dd>
<dt>def _create_base_tf_dataset(self, batch_size: int):</dt><dd><dl>
<dt>if self._data_config.inputs:</dt><dd><p>glob = self._data_config.inputs
filenames = sorted(tf.io.gfile.glob(glob))</p>
</dd>
<dt>elif self._data_config.explicit_datetime_inputs:</dt><dd><p>num_missing_hours_tol = self._data_config.explicit_datetime_inputs.num_missing_hours_tol
filenames, num_hours_missing = get_explicit_datetime_inputs_files(</p>
<blockquote>
<div><p>self._data_config.explicit_datetime_inputs,
increment=”hourly”,</p>
</div></blockquote>
<p>)
if num_hours_missing &gt; num_missing_hours_tol:</p>
<blockquote>
<div><dl class="simple">
<dt>raise ValueError(</dt><dd><p>f”We are missing {num_hours_missing} hours of data”
f”more than tolerance {num_missing_hours_tol}.”</p>
</dd>
</dl>
<p>)</p>
</div></blockquote>
</dd>
<dt>elif self._data_config.explicit_date_inputs:</dt><dd><p>num_missing_days_tol = self._data_config.explicit_date_inputs.num_missing_days_tol
filenames, num_days_missing = get_explicit_datetime_inputs_files(</p>
<blockquote>
<div><p>self._data_config.explicit_date_inputs,
increment=”daily”,</p>
</div></blockquote>
<p>)
if num_days_missing &gt; num_missing_days_tol:</p>
<blockquote>
<div><dl class="simple">
<dt>raise ValueError(</dt><dd><p>f”We are missing {num_days_missing} days of data”
f”more than tolerance {num_missing_days_tol}.”</p>
</dd>
</dl>
<p>)</p>
</div></blockquote>
</dd>
<dt>else:</dt><dd><dl class="simple">
<dt>raise ValueError(</dt><dd><p>“Must specifiy either <cite>inputs</cite>, <cite>explicit_datetime_inputs</cite>, or <cite>explicit_date_inputs</cite> in data_config”</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
<p>num_files = len(filenames)
logging.info(f”Found {num_files} data files”)
if num_files &lt; 1:</p>
<blockquote>
<div><p>raise ValueError(“No data files found”)</p>
</div></blockquote>
<dl class="simple">
<dt>if self._data_config.num_files_to_keep is not None:</dt><dd><p>filenames = filenames[: self._data_config.num_files_to_keep]
logging.info(f”Retaining only {len(filenames)} files.”)</p>
</dd>
<dt>filenames_ds = (</dt><dd><p>tf.data.Dataset.from_tensor_slices(filenames).shuffle(len(filenames))
# Because of drop_remainder, if our dataset does not fill
# up a batch, it will emit nothing without this repeat.
.repeat(-1)</p>
</dd>
</dl>
<p>)</p>
<dl class="simple">
<dt>if self._data_config.file_batch_size:</dt><dd><p>filenames_ds = filenames_ds.batch(self._data_config.file_batch_size)</p>
</dd>
<dt>def per_shard_dataset(filename):</dt><dd><p>ds = tf.data.TFRecordDataset([filename], compression_type=”GZIP”)
return ds.prefetch(4)</p>
</dd>
<dt>ds = filenames_ds.interleave(</dt><dd><p>per_shard_dataset,
block_length=4,
deterministic=False,
num_parallel_calls=self._data_config.interleave_num_parallel_calls
or tf.data.experimental.AUTOTUNE,</p>
</dd>
</dl>
<p>)</p>
<p># Combine functions into one map call to reduce overhead.
map_fn = functools.partial(</p>
<blockquote>
<div><p>_chain,
f1=self._parse_fn,
f2=self._output_map_fn,</p>
</div></blockquote>
<p>)</p>
<p># Shuffle -&gt; Batch -&gt; Parse is the correct ordering
# Shuffling needs to be performed before batching otherwise there is not much point
# Batching happens before parsing because tf.Example parsing is actually vectorized
#     and works much faster overall on batches of data.
ds = (</p>
<blockquote>
<div><p># DANGER DANGER: there is a default shuffle size here.
ds.shuffle(self._data_config.examples_shuffle_buffer_size)
.batch(batch_size=batch_size, drop_remainder=True)
.map(</p>
<blockquote>
<div><p>map_fn,
num_parallel_calls=self._data_config.map_num_parallel_calls
or tf.data.experimental.AUTOTUNE,</p>
</div></blockquote>
<p>)</p>
</div></blockquote>
<p>)</p>
<dl class="simple">
<dt>if self._data_config.cache:</dt><dd><p>ds = ds.cache()</p>
</dd>
<dt>if self._data_config.ignore_data_errors:</dt><dd><p>ds = ds.apply(tf.data.experimental.ignore_errors())</p>
</dd>
</dl>
<p>options = tf.data.Options()
options.experimental_deterministic = False
ds = ds.with_options(options)</p>
<p>return ds</p>
</dd>
<dt>def _gen(self):</dt><dd><dl class="simple">
<dt>for x in self._tf_dataset:</dt><dd><p>yield to_batch(x)</p>
</dd>
</dl>
</dd>
<dt>def to_dataloader(self) -&gt; Dict[str, torch.Tensor]:</dt><dd><p>return torch.utils.data.DataLoader(self, batch_size=None)</p>
</dd>
<dt>def __iter__(self):</dt><dd><p>return iter(self._gen())</p>
</dd>
</dl>
</dd>
</dl>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../index.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../../_sources/projects/home/recap/data/dataset.py.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>