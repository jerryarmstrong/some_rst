<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>“””MaskNet: Wang et al. (<a class="reference external" href="https://arxiv.org/abs/2102.07619">https://arxiv.org/abs/2102.07619</a>).”””</p>
<p>from tml.projects.home.recap.model import config, mlp</p>
<p>import torch</p>
<dl>
<dt>def _init_weights(module):</dt><dd><dl class="simple">
<dt>if isinstance(module, torch.nn.Linear):</dt><dd><p>torch.nn.init.xavier_uniform_(module.weight)
torch.nn.init.constant_(module.bias, 0)</p>
</dd>
</dl>
</dd>
<dt>class MaskBlock(torch.nn.Module):</dt><dd><dl>
<dt>def __init__(</dt><dd><p>self, mask_block_config: config.MaskBlockConfig, input_dim: int, mask_input_dim: int</p>
</dd>
<dt>) -&gt; None:</dt><dd><p>super(MaskBlock, self).__init__()
self.mask_block_config = mask_block_config
output_size = mask_block_config.output_size</p>
<dl class="simple">
<dt>if mask_block_config.input_layer_norm:</dt><dd><p>self._input_layer_norm = torch.nn.LayerNorm(input_dim)</p>
</dd>
<dt>else:</dt><dd><p>self._input_layer_norm = None</p>
</dd>
<dt>if mask_block_config.reduction_factor:</dt><dd><p>aggregation_size = int(mask_input_dim * mask_block_config.reduction_factor)</p>
</dd>
<dt>elif mask_block_config.aggregation_size is not None:</dt><dd><p>aggregation_size = mask_block_config.aggregation_size</p>
</dd>
<dt>else:</dt><dd><p>raise ValueError(“Need one of reduction factor or aggregation size.”)</p>
</dd>
<dt>self._mask_layer = torch.nn.Sequential(</dt><dd><p>torch.nn.Linear(mask_input_dim, aggregation_size),
torch.nn.ReLU(),
torch.nn.Linear(aggregation_size, input_dim),</p>
</dd>
</dl>
<p>)
self._mask_layer.apply(_init_weights)
self._hidden_layer = torch.nn.Linear(input_dim, output_size)
self._hidden_layer.apply(_init_weights)
self._layer_norm = torch.nn.LayerNorm(output_size)</p>
</dd>
<dt>def forward(self, net: torch.Tensor, mask_input: torch.Tensor):</dt><dd><dl class="simple">
<dt>if self._input_layer_norm:</dt><dd><p>net = self._input_layer_norm(net)</p>
</dd>
</dl>
<p>hidden_layer_output = self._hidden_layer(net * self._mask_layer(mask_input))
return self._layer_norm(hidden_layer_output)</p>
</dd>
</dl>
</dd>
<dt>class MaskNet(torch.nn.Module):</dt><dd><dl>
<dt>def __init__(self, mask_net_config: config.MaskNetConfig, in_features: int):</dt><dd><p>super().__init__()
self.mask_net_config = mask_net_config
mask_blocks = []</p>
<dl>
<dt>if mask_net_config.use_parallel:</dt><dd><p>total_output_mask_blocks = 0
for mask_block_config in mask_net_config.mask_blocks:</p>
<blockquote>
<div><p>mask_blocks.append(MaskBlock(mask_block_config, in_features, in_features))
total_output_mask_blocks += mask_block_config.output_size</p>
</div></blockquote>
<p>self._mask_blocks = torch.nn.ModuleList(mask_blocks)</p>
</dd>
<dt>else:</dt><dd><p>input_size = in_features
for mask_block_config in mask_net_config.mask_blocks:</p>
<blockquote>
<div><p>mask_blocks.append(MaskBlock(mask_block_config, input_size, in_features))
input_size = mask_block_config.output_size</p>
</div></blockquote>
<p>self._mask_blocks = torch.nn.ModuleList(mask_blocks)
total_output_mask_blocks = mask_block_config.output_size</p>
</dd>
<dt>if mask_net_config.mlp:</dt><dd><p>self._dense_layers = mlp.Mlp(total_output_mask_blocks, mask_net_config.mlp)
self.out_features = mask_net_config.mlp.layer_sizes[-1]</p>
</dd>
<dt>else:</dt><dd><p>self.out_features = total_output_mask_blocks</p>
</dd>
</dl>
<p>self.shared_size = total_output_mask_blocks</p>
</dd>
<dt>def forward(self, inputs: torch.Tensor):</dt><dd><dl>
<dt>if self.mask_net_config.use_parallel:</dt><dd><p>mask_outputs = []
for mask_layer in self._mask_blocks:</p>
<blockquote>
<div><p>mask_outputs.append(mask_layer(mask_input=inputs, net=inputs))</p>
</div></blockquote>
<p># Share the outputs of the MaskBlocks.
all_mask_outputs = torch.cat(mask_outputs, dim=1)
output = (</p>
<blockquote>
<div><p>all_mask_outputs
if self.mask_net_config.mlp is None
else self._dense_layers(all_mask_outputs)[“output”]</p>
</div></blockquote>
<p>)
return {“output”: output, “shared_layer”: all_mask_outputs}</p>
</dd>
<dt>else:</dt><dd><p>net = inputs
for mask_layer in self._mask_blocks:</p>
<blockquote>
<div><p>net = mask_layer(net=net, mask_input=inputs)</p>
</div></blockquote>
<p># Share the output of the stacked MaskBlocks.
output = net if self.mask_net_config.mlp is None else self._dense_layers[net][“output”]
return {“output”: output, “shared_layer”: net}</p>
</dd>
</dl>
</dd>
</dl>
</dd>
</dl>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../index.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../../_sources/projects/home/recap/model/mask_net.py.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>