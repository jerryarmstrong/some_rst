<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../../../../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../../../../../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../../../../../../../" id="documentation_options" src="../../../../../../../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../../../../../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../../../../../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../../../search.html" />
   
  <link rel="stylesheet" href="../../../../../../../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p>package com.twitter.tweetypie
package config</p>
<p>import com.twitter.conversions.DurationOps._
import com.twitter.finagle.Backoff
import com.twitter.finagle.memcached.exp.localMemcachedPort
import com.twitter.finagle.mtls.authentication.ServiceIdentifier
import com.twitter.finagle.ssl.OpportunisticTls
import com.twitter.finagle.thrift.ClientId
import com.twitter.flockdb.client.thriftscala.Priority
import com.twitter.servo.repository.CachedResult
import com.twitter.servo.util.Availability
import com.twitter.tweetypie.backends._
import com.twitter.tweetypie.caching.SoftTtl
import com.twitter.tweetypie.handler.DuplicateTweetFinder
import com.twitter.tweetypie.repository.TombstoneTtl
import com.twitter.tweetypie.service._
import com.twitter.tweetypie.storage.ManhattanTweetStorageClient
import com.twitter.util.Duration</p>
<p>case class InProcessCacheConfig(ttl: Duration, maximumSize: Int)</p>
<p>class TweetServiceSettings(val flags: TweetServiceFlags) {</p>
<blockquote>
<div><dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Convert a Boolean to an Option</p></li>
<li><p>&gt; optional(true, “my value”)</p></li>
<li><p>res: Some(my value)</p></li>
<li></li>
<li><p>&gt; optional(false, “my value”)</p></li>
<li><p>res: None</p></li>
</ul>
<p><a href="#id1"><span class="problematic" id="id2">*</span></a>/</p>
</dd>
</dl>
<p>def optional[T](b: Boolean, a: =&gt; T): Option[T] = if (b) Some(a) else None</p>
<p>/** atla, localhost, etc. <a href="#id3"><span class="problematic" id="id4">*</span></a>/
val zone: String = flags.zone()</p>
<p>/** dc is less specific than zone, zone=atla, dc=atl <a href="#id5"><span class="problematic" id="id6">*</span></a>/
val dc: String = zone.dropRight(1)</p>
<p>/** one of: prod, staging, dev, testbox <a href="#id7"><span class="problematic" id="id8">*</span></a>/
val env: Env.Value = flags.env()</p>
<p>/** instanceId of this aurora instance <a href="#id9"><span class="problematic" id="id10">*</span></a>/
lazy val instanceId: Int = flags.instanceId()</p>
<p>/** total number of tweetypie aurora instances <a href="#id11"><span class="problematic" id="id12">*</span></a>/
val instanceCount: Int = flags.instanceCount()</p>
<p>/** The Name to resolve to find the memcached cluster <a href="#id13"><span class="problematic" id="id14">*</span></a>/
val twemcacheDest: String =</p>
<blockquote>
<div><p>// If twemcacheDest is explicitly set, always prefer that to
// localMemcachedPort.
flags.twemcacheDest.get
// Testbox uses this global flag to specify the location of the
// local memcached instance.</p>
<blockquote>
<div><p>.orElse(localMemcachedPort().map(“/$/inet/localhost/” + _))
// If no explicit Name is specified, use the default.
.getOrElse(flags.twemcacheDest())</p>
</div></blockquote>
</div></blockquote>
<p>/** Read/write data through Cache <a href="#id15"><span class="problematic" id="id16">*</span></a>/
val withCache: Boolean = flags.withCache()</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>The TFlock queue to use for background indexing operations. For</p></li>
<li><p>production, this should always be the low priority queue, to</p></li>
<li><p>allow foreground operations to be processed first.</p></li>
</ul>
<p><a href="#id17"><span class="problematic" id="id18">*</span></a>/</p>
</dd>
</dl>
<p>val backgroundIndexingPriority: Priority = flags.backgroundIndexingPriority()</p>
<p>/** Set certain decider gates to this overridden value <a href="#id19"><span class="problematic" id="id20">*</span></a>/
val deciderOverrides: Map[String, Boolean] =</p>
<blockquote>
<div><p>flags.deciderOverrides()</p>
</div></blockquote>
<p>/** use per host stats? <a href="#id21"><span class="problematic" id="id22">*</span></a>/
val clientHostStats: Boolean =</p>
<blockquote>
<div><p>flags.clientHostStats()</p>
</div></blockquote>
<dl class="simple">
<dt>val warmupRequestsSettings: Option[WarmupQueriesSettings] =</dt><dd><p>optional(flags.enableWarmupRequests(), WarmupQueriesSettings())</p>
</dd>
</dl>
<p>/** enables request authorization via a allowlist <a href="#id23"><span class="problematic" id="id24">*</span></a>/
val allowlistingRequired: Boolean =</p>
<blockquote>
<div><p>flags.allowlist.get.getOrElse(env == Env.prod)</p>
</div></blockquote>
<p>/** read rate limit for unknown clients (when allowlistingRequired is enabled) <a href="#id25"><span class="problematic" id="id26">*</span></a>/
val nonAllowListedClientRateLimitPerSec: Double =</p>
<blockquote>
<div><p>flags.grayListRateLimit()</p>
</div></blockquote>
<p>/** enables requests from production clients <a href="#id27"><span class="problematic" id="id28">*</span></a>/
val allowProductionClients: Boolean =</p>
<blockquote>
<div><p>env == Env.prod</p>
</div></blockquote>
<p>/** enables replication via DRPC <a href="#id29"><span class="problematic" id="id30">*</span></a>/
val enableReplication: Boolean = flags.enableReplication()</p>
<p>/** enables forking of some traffic to configured target <a href="#id31"><span class="problematic" id="id32">*</span></a>/
val trafficForkingEnabled: Boolean =</p>
<blockquote>
<div><p>env == Env.prod</p>
</div></blockquote>
<dl class="simple">
<dt>val scribeUniquenessIds: Boolean =</dt><dd><p>env == Env.prod</p>
</dd>
</dl>
<p>/** ClientId to send to backend services <a href="#id33"><span class="problematic" id="id34">*</span></a>/
val thriftClientId: ClientId =</p>
<blockquote>
<div><dl>
<dt>flags.clientId.get.map(ClientId(_)).getOrElse {</dt><dd><dl class="simple">
<dt>env match {</dt><dd><p>case Env.dev | Env.staging =&gt; ClientId(“tweetypie.staging”)
case Env.prod =&gt; ClientId(“tweetypie.prod”)</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>}</p>
</div></blockquote>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Instead of using DRPC for calling into the async code path, call back into the</p></li>
<li><p>current instance. Used for development and test to ensure logic in the current</p></li>
<li><p>instance is being tested.</p></li>
</ul>
<p><a href="#id35"><span class="problematic" id="id36">*</span></a>/</p>
</dd>
</dl>
<p>val simulateDeferredrpcCallbacks: Boolean = flags.simulateDeferredrpcCallbacks()</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>ClientId to set in ‘asynchronous’ requests when simulateDeferredrpcCallbacks is</p></li>
<li><p>true and Tweetypie ends up just calling itself synchronously.</p></li>
</ul>
<p><a href="#id37"><span class="problematic" id="id38">*</span></a>/</p>
</dd>
</dl>
<p>val deferredrpcClientId: ClientId = ClientId(“deferredrpc.prod”)</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>ServiceIdentifier used to enable mTLS</p></li>
</ul>
<p><a href="#id39"><span class="problematic" id="id40">*</span></a>/</p>
</dd>
</dl>
<p>val serviceIdentifier: ServiceIdentifier = flags.serviceIdentifier()</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Decider settings</p></li>
</ul>
<p><a href="#id41"><span class="problematic" id="id42">*</span></a>/</p>
</dd>
</dl>
<p>val deciderBaseFilename: Option[String] = Option(flags.deciderBase())
val deciderOverlayFilename: Option[String] = Option(flags.deciderOverlay())
val vfDeciderOverlayFilename: Option[String] = flags.vfDeciderOverlay.get</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Used to determine whether we should fail requests for Tweets that are likely too young</p></li>
<li><p>to return a non-partial response. We return NotFound for Tweets that are deemed too young.</p></li>
<li><p>Used by [[com.twitter.tweetypie.repository.ManhattanTweetRepository]].</p></li>
</ul>
<p><a href="#id43"><span class="problematic" id="id44">*</span></a>/</p>
</dd>
<dt>val shortCircuitLikelyPartialTweetReads: Gate[Duration] = {</dt><dd><p>// interpret the flag as a duration in milliseconds
val ageCeiling: Duration = flags.shortCircuitLikelyPartialTweetReadsMs().milliseconds
Gate(tweetAge =&gt; tweetAge &lt; ageCeiling)</p>
</dd>
</dl>
<p>}</p>
<p>// tweet-service internal settings</p>
<p>val tweetKeyCacheVersion = 1</p>
<p>/** how often to flush aggregated count updates for tweet counts <a href="#id45"><span class="problematic" id="id46">*</span></a>/
val aggregatedTweetCountsFlushInterval: Duration = 5.seconds</p>
<p>/** maximum number of keys for which aggregated cached count updates may be cached <a href="#id47"><span class="problematic" id="id48">*</span></a>/
val maxAggregatedCountsSize = 1000</p>
<p>/** ramp up period for decidering up forked traffic (if enabled) to the full decidered value <a href="#id49"><span class="problematic" id="id50">*</span></a>/
val forkingRampUp: Duration = 3.minutes</p>
<p>/** how long to wait after startup for serversets to resolve before giving up and moving on <a href="#id51"><span class="problematic" id="id52">*</span></a>/
val waitForServerSetsTimeout: Duration = 120.seconds</p>
<p>/** number of threads to use in thread pool for language identification <a href="#id53"><span class="problematic" id="id54">*</span></a>/
val numPenguinThreads = 4</p>
<p>/** maximum number of tweets that clients can request per getTweets RPC call <a href="#id55"><span class="problematic" id="id56">*</span></a>/
val maxGetTweetsRequestSize = 200</p>
<p>/** maximum batch size for any batched request (getTweets is exempt, it has its own limiting) <a href="#id57"><span class="problematic" id="id58">*</span></a>/
val maxRequestSize = 200</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>maximum size to allow the thrift response buffer to grow before resetting it.  this is set to</p></li>
<li><p>approximately the current value of <cite>srv/thrift/response_payload_bytes.p999</cite>, meaning roughly</p></li>
<li><p>1 out of 1000 requests will cause the buffer to be reset.</p></li>
</ul>
<p><a href="#id59"><span class="problematic" id="id60">*</span></a>/</p>
</dd>
</dl>
<p>val maxThriftBufferSize: Int = 200 * 1024</p>
<p>// <strong>*****</strong> timeouts and backoffs <strong>******</strong></p>
<p>/** backoffs for OptimisticLockingCache lockAndSet operations <a href="#id61"><span class="problematic" id="id62">*</span></a>/
val lockingCacheBackoffs: Stream[Duration] =</p>
<blockquote>
<div><p>Backoff.exponentialJittered(10.millisecond, 50.milliseconds).take(3).toStream</p>
</div></blockquote>
<p>/** retry once on timeout with no backoff <a href="#id63"><span class="problematic" id="id64">*</span></a>/
val defaultTimeoutBackoffs: Stream[Duration] = Stream(0.milliseconds).toStream</p>
<p>/** backoffs when user view is missing <a href="#id65"><span class="problematic" id="id66">*</span></a>/
val gizmoduckMissingUserViewBackoffs: Stream[Duration] = Backoff.const(10.millis).take(3).toStream</p>
<p>/** backoffs for retrying failed async-write actions after first retry failure <a href="#id67"><span class="problematic" id="id68">*</span></a>/
val asyncWriteRetryBackoffs: Stream[Duration] =</p>
<blockquote>
<div><p>Backoff.exponential(10.milliseconds, 2).take(9).toStream.map(_ min 1.second)</p>
</div></blockquote>
<p>/** backoffs for retrying failed deferredrpc enqueues <a href="#id69"><span class="problematic" id="id70">*</span></a>/
val deferredrpcBackoffs: Stream[Duration] =</p>
<blockquote>
<div><p>Backoff.exponential(10.milliseconds, 2).take(3).toStream</p>
</div></blockquote>
<p>/** backoffs for retrying failed cache updates for replicated events <a href="#id71"><span class="problematic" id="id72">*</span></a>/
val replicatedEventCacheBackoffs: Stream[Duration] =</p>
<blockquote>
<div><p>Backoff.exponential(100.milliseconds, 2).take(10).toStream</p>
</div></blockquote>
<dl>
<dt>val escherbirdConfig: Escherbird.Config =</dt><dd><dl class="simple">
<dt>Escherbird.Config(</dt><dd><p>requestTimeout = 200.milliseconds,
timeoutBackoffs = defaultTimeoutBackoffs</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>val expandodoConfig: Expandodo.Config =</dt><dd><dl class="simple">
<dt>Expandodo.Config(</dt><dd><p>requestTimeout = 300.milliseconds,
timeoutBackoffs = defaultTimeoutBackoffs,
serverErrorBackoffs = Backoff.const(0.millis).take(3).toStream</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>val creativesContainerServiceConfig: CreativesContainerService.Config =</dt><dd><dl class="simple">
<dt>CreativesContainerService.Config(</dt><dd><p>requestTimeout = 300.milliseconds,
timeoutBackoffs = defaultTimeoutBackoffs,
serverErrorBackoffs = Backoff.const(0.millis).take(3).toStream</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>val geoScrubEventStoreConfig: GeoScrubEventStore.Config =</dt><dd><dl>
<dt>GeoScrubEventStore.Config(</dt><dd><dl class="simple">
<dt>read = GeoScrubEventStore.EndpointConfig(</dt><dd><p>requestTimeout = 200.milliseconds,
maxRetryCount = 1</p>
</dd>
</dl>
<p>),
write = GeoScrubEventStore.EndpointConfig(</p>
<blockquote>
<div><p>requestTimeout = 1.second,
maxRetryCount = 1</p>
</div></blockquote>
<p>)</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>val gizmoduckConfig: Gizmoduck.Config =</dt><dd><dl class="simple">
<dt>Gizmoduck.Config(</dt><dd><p>readTimeout = 300.milliseconds,
writeTimeout = 300.milliseconds,
// We bump the timeout value to 800ms because modifyAndGet is called only in async request path in GeoScrub daemon
// and we do not expect sync/realtime apps calling this thrift method
modifyAndGetTimeout = 800.milliseconds,
modifyAndGetTimeoutBackoffs = Backoff.const(0.millis).take(3).toStream,
defaultTimeoutBackoffs = defaultTimeoutBackoffs,
gizmoduckExceptionBackoffs = Backoff.const(0.millis).take(3).toStream</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>val limiterBackendConfig: LimiterBackend.Config =</dt><dd><dl class="simple">
<dt>LimiterBackend.Config(</dt><dd><p>requestTimeout = 300.milliseconds,
timeoutBackoffs = defaultTimeoutBackoffs</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>val mediaInfoServiceConfig: MediaInfoService.Config =</dt><dd><dl class="simple">
<dt>MediaInfoService.Config(</dt><dd><p>requestTimeout = 300.milliseconds,
totalTimeout = 500.milliseconds,
timeoutBackoffs = defaultTimeoutBackoffs</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>val scarecrowConfig: Scarecrow.Config =</dt><dd><dl class="simple">
<dt>Scarecrow.Config(</dt><dd><p>readTimeout = 100.milliseconds,
writeTimeout = 400.milliseconds,
timeoutBackoffs = defaultTimeoutBackoffs,
scarecrowExceptionBackoffs = Backoff.const(0.millis).take(3).toStream</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>val socialGraphSeviceConfig: SocialGraphService.Config =</dt><dd><dl class="simple">
<dt>SocialGraphService.Config(</dt><dd><p>socialGraphTimeout = 250.milliseconds,
timeoutBackoffs = defaultTimeoutBackoffs</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>val talonConfig: Talon.Config =</dt><dd><dl class="simple">
<dt>Talon.Config(</dt><dd><p>shortenTimeout = 500.milliseconds,
expandTimeout = 150.milliseconds,
timeoutBackoffs = defaultTimeoutBackoffs,
transientErrorBackoffs = Backoff.const(0.millis).take(3).toStream</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>/**</dt><dd><ul class="simple">
<li><p>page size when retrieving tflock pages for tweet deletion and undeletion</p></li>
<li><p>tweet erasures have their own page size eraseUserTweetsPageSize</p></li>
</ul>
<p><a href="#id73"><span class="problematic" id="id74">*</span></a>/</p>
</dd>
</dl>
<p>val tflockPageSize: Int = flags.tflockPageSize()</p>
<dl>
<dt>val tflockReadConfig: TFlock.Config =</dt><dd><dl class="simple">
<dt>TFlock.Config(</dt><dd><p>requestTimeout = 300.milliseconds,
timeoutBackoffs = defaultTimeoutBackoffs,
flockExceptionBackoffs = Backoff.const(0.millis).take(3).toStream,
overCapacityBackoffs = Stream.empty,
defaultPageSize = tflockPageSize</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>val tflockWriteConfig: TFlock.Config =</dt><dd><dl class="simple">
<dt>TFlock.Config(</dt><dd><p>requestTimeout = 400.milliseconds,
timeoutBackoffs = defaultTimeoutBackoffs,
flockExceptionBackoffs = Backoff.const(0.millis).take(3).toStream,
overCapacityBackoffs = Backoff.exponential(10.millis, 2).take(3).toStream</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>val timelineServiceConfig: TimelineService.Config = {</dt><dd><p>val tlsExceptionBackoffs = Backoff.const(0.millis).take(3).toStream
TimelineService.Config(</p>
<blockquote>
<div><dl>
<dt>writeRequestPolicy =</dt><dd><dl>
<dt>Backend.TimeoutPolicy(4.seconds) &gt;&gt;&gt;</dt><dd><dl class="simple">
<dt>TimelineService.FailureBackoffsPolicy(</dt><dd><p>timeoutBackoffs = defaultTimeoutBackoffs,
tlsExceptionBackoffs = tlsExceptionBackoffs</p>
</dd>
</dl>
<p>),</p>
</dd>
</dl>
</dd>
<dt>readRequestPolicy =</dt><dd><dl>
<dt>Backend.TimeoutPolicy(400.milliseconds) &gt;&gt;&gt;</dt><dd><dl class="simple">
<dt>TimelineService.FailureBackoffsPolicy(</dt><dd><p>timeoutBackoffs = defaultTimeoutBackoffs,
tlsExceptionBackoffs = tlsExceptionBackoffs</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
</dd>
</dl>
</div></blockquote>
<p>)</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>val tweetStorageConfig: ManhattanTweetStorageClient.Config = {</dt><dd><dl>
<dt>val remoteZone = zone match {</dt><dd><p>case “atla” =&gt; “pdxa”
case “pdxa” =&gt; “atla”
case “atla” | “localhost” =&gt; “atla”
case _ =&gt;</p>
<blockquote>
<div><p>throw new IllegalArgumentException(s”Cannot configure remote DC for unknown zone ‘$zone’”)</p>
</div></blockquote>
</dd>
</dl>
<p>}
ManhattanTweetStorageClient.Config(</p>
<blockquote>
<div><p>applicationId = “tbird_mh”,
localDestination = “/s/manhattan/cylon.native-thrift”,
localTimeout = 290.milliseconds,
remoteDestination = s”/srv#/prod/$remoteZone/manhattan/cylon.native-thrift”,
remoteTimeout = 1.second,
maxRequestsPerBatch = 25,
serviceIdentifier = serviceIdentifier,
opportunisticTlsLevel = OpportunisticTls.Required</p>
</div></blockquote>
<p>)</p>
</dd>
</dl>
<p>}</p>
<dl>
<dt>val userImageServiceConfig: UserImageService.Config =</dt><dd><dl class="simple">
<dt>UserImageService.Config(</dt><dd><p>processTweetMediaTimeout = 5.seconds,
updateTweetMediaTimeout = 2.seconds,
timeoutBackoffs = defaultTimeoutBackoffs</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>val adsLoggingClientTopicName = env match {</dt><dd><p>case Env.prod =&gt; “ads_client_callback_prod”
case Env.dev | Env.staging =&gt; “ads_client_callback_staging”</p>
</dd>
</dl>
<p>}</p>
<p>/** Delay between successive cascadedDeleteTweet calls when deleting retweets.  Applied via decider. <a href="#id75"><span class="problematic" id="id76">*</span></a>/
val retweetDeletionDelay: Duration = 20.milliseconds</p>
<dl>
<dt>/**</dt><dd><ul class="simple">
<li><p>Delay to sleep before each tweet deletion of an eraseUserTweets request.</p></li>
<li><p>This is a simple rate limiting mechanism. The long term solution is</p></li>
<li><p>to move async endpoints like user erasures and retweet deletions out</p></li>
<li><p>of the the main tweetypie cluster and into an async cluster with first class</p></li>
<li><p>rate limiting support</p></li>
</ul>
<p><a href="#id77"><span class="problematic" id="id78">*</span></a>/</p>
</dd>
</dl>
<p>val eraseUserTweetsDelay: Duration = 100.milliseconds</p>
<p>val eraseUserTweetsPageSize = 100</p>
<p>val getStoredTweetsByUserPageSize = 20
val getStoredTweetsByUserMaxPages = 30</p>
<p>// <strong>*****</strong> ttls <strong>******</strong></p>
<p>// Unfortunately, this tombstone TTL applies equally to the case
// where the tweet was deleted and the case that the tweet does not
// exist or is unavailable. If we could differentiate between those
// cases, we’d cache deleted for a long time and not
// found/unavailable for a short time. We chose 100
// milliseconds for the minimum TTL because there are known cases in
// which a not found result can be erroneously written to cache on
// tweet creation. This minimum TTL is a trade-off between a
// thundering herd of database requests from clients that just got
// the fanned-out tweet and the window for which these inconsistent
// results will be available.
val tweetTombstoneTtl: CachedResult.CachedNotFound[TweetId] =&gt; Duration =</p>
<blockquote>
<div><p>TombstoneTtl.linear(min = 100.milliseconds, max = 1.day, from = 5.minutes, to = 5.hours)</p>
</div></blockquote>
<p>val tweetMemcacheTtl: Duration = 14.days
val urlMemcacheTtl: Duration = 1.hour
val urlMemcacheSoftTtl: Duration = 1.hour
val deviceSourceMemcacheTtl: Duration = 12.hours
val deviceSourceMemcacheSoftTtl: SoftTtl.ByAge[Nothing] =</p>
<blockquote>
<div><p>SoftTtl.ByAge(softTtl = 1.hour, jitter = 1.minute)</p>
</div></blockquote>
<p>val deviceSourceInProcessTtl: Duration = 8.hours
val deviceSourceInProcessSoftTtl: Duration = 30.minutes
val placeMemcacheTtl: Duration = 1.day
val placeMemcacheSoftTtl: SoftTtl.ByAge[Nothing] =</p>
<blockquote>
<div><p>SoftTtl.ByAge(softTtl = 3.hours, jitter = 1.minute)</p>
</div></blockquote>
<p>val cardMemcacheTtl: Duration = 20.minutes
val cardMemcacheSoftTtl: Duration = 30.seconds
val tweetCreateLockingMemcacheTtl: Duration = 10.seconds
val tweetCreateLockingMemcacheLongTtl: Duration = 12.hours
val geoScrubMemcacheTtl: Duration = 30.minutes</p>
<p>val tweetCountsMemcacheTtl: Duration = 24.hours
val tweetCountsMemcacheNonZeroSoftTtl: Duration = 3.hours
val tweetCountsMemcacheZeroSoftTtl: Duration = 7.hours</p>
<p>val cacheClientPendingRequestLimit: Int = flags.memcachePendingRequestLimit()</p>
<p>val deviceSourceInProcessCacheMaxSize = 10000</p>
<dl>
<dt>val inProcessCacheConfigOpt: Option[InProcessCacheConfig] =</dt><dd><dl>
<dt>if (flags.enableInProcessCache()) {</dt><dd><dl>
<dt>Some(</dt><dd><dl class="simple">
<dt>InProcessCacheConfig(</dt><dd><p>ttl = flags.inProcessCacheTtlMs().milliseconds,
maximumSize = flags.inProcessCacheSize()</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
<p>)</p>
</dd>
<dt>} else {</dt><dd><p>None</p>
</dd>
</dl>
<p>}</p>
</dd>
</dl>
<p>// Begin returning OverCapacity for tweet repo when cache SR falls below 95%,
// Scale to rejecting 95% of requests when cache SR &lt;= 80%
val tweetCacheAvailabilityFromSuccessRate: Double =&gt; Double =</p>
<blockquote>
<div><p>Availability.linearlyScaled(0.95, 0.80, 0.05)</p>
</div></blockquote>
<p>// <strong>***</strong> repository chunking size <strong>****</strong></p>
<p>val tweetCountsRepoChunkSize = 6
// n times <cite>tweetCountsRepoChunkSize</cite>, so chunking at higher level does not
// generate small batches at lower level.
val tweetCountsCacheChunkSize = 18</p>
<dl>
<dt>val duplicateTweetFinderSettings: DuplicateTweetFinder.Settings =</dt><dd><p>DuplicateTweetFinder.Settings(numTweetsToCheck = 10, maxDuplicateAge = 12.hours)</p>
</dd>
<dt>val backendWarmupSettings: Warmup.Settings =</dt><dd><dl>
<dt>Warmup.Settings(</dt><dd><p>// Try for twenty seconds to warm up the backends before giving
// up.
maxWarmupDuration = 20.seconds,
// Only allow up to 50 outstanding warmup requests of any kind
// to be outstanding at a time.
maxOutstandingRequests = 50,
// These timeouts are just over the p999 latency observed in ATLA
// for requests to these backends.
requestTimeouts = Map(</p>
<blockquote>
<div><p>“expandodo” -&gt; 120.milliseconds,
“geo_relevance” -&gt; 50.milliseconds,
“gizmoduck” -&gt; 200.milliseconds,
“memcache” -&gt; 50.milliseconds,
“scarecrow” -&gt; 120.milliseconds,
“socialgraphservice” -&gt; 180.milliseconds,
“talon” -&gt; 70.milliseconds,
“tflock” -&gt; 320.milliseconds,
“timelineservice” -&gt; 200.milliseconds,
“tweetstorage” -&gt; 50.milliseconds</p>
</div></blockquote>
<p>),
reliability = Warmup.Reliably(</p>
<blockquote>
<div><p>// Consider a backend warmed up if 99% of requests are succeeding.
reliabilityThreshold = 0.99,
// When performing warmup, use a maximum of 10 concurrent
// requests to each backend.
concurrency = 10,
// Do not allow more than this many attempts to perform the
// warmup action before giving up.
maxAttempts = 1000</p>
</div></blockquote>
<p>)</p>
</dd>
</dl>
<p>)</p>
</dd>
</dl>
</div></blockquote>
<p>}</p>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../../../../../../../index.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../../../../../../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../../../../../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../../../../../../../_sources/tweetypie/server/src/main/scala/com/twitter/tweetypie/config/TweetServiceSettings.scala.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>