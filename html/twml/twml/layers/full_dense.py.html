<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>&lt;no title&gt; &#8212; twit  documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=b3523f8e" />
    <link rel="stylesheet" type="text/css" href="../../../_static/alabaster.css?v=039e1c02" />
    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js?v=b3ba4146"></script>
    <script src="../../../_static/doctools.js?v=888ff710"></script>
    <script src="../../../_static/sphinx_highlight.js?v=4825356b"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <p># pylint: disable=no-member,arguments-differ, attribute-defined-outside-init
“””
Implementing Full Dense Layer
“””
from tensorflow.python.layers import core as core_layers
from tensorflow.python.ops import init_ops
from tensorflow.python.framework import tensor_shape
from tensorflow.python.keras.engine.base_layer import InputSpec
import tensorflow.compat.v1 as tf</p>
<dl>
<dt>class FullDense(core_layers.Dense):</dt><dd><p>“””
Densely-connected layer class.
This is wrapping tensorflow.python.layers.core.Dense
This layer implements the operation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">outputs</span> <span class="o">=</span> <span class="n">activation</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">weight</span> <span class="o">+</span> <span class="n">bias</span><span class="p">)</span>
</pre></div>
</div>
<p>Where <code class="docutils literal notranslate"><span class="pre">activation</span></code> is the activation function passed as the <code class="docutils literal notranslate"><span class="pre">activation</span></code>
argument (if not <code class="docutils literal notranslate"><span class="pre">None</span></code>), <code class="docutils literal notranslate"><span class="pre">weight</span></code> is a weights matrix created by the layer,
and <code class="docutils literal notranslate"><span class="pre">bias</span></code> is a bias vector created by the layer.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>output_size:</dt><dd><p>Integer or Long, dimensionality of the output space.</p>
</dd>
<dt>activation:</dt><dd><p>Activation function (callable). Set it to None to maintain a linear activation.</p>
</dd>
<dt>weight_initializer:</dt><dd><p>Initializer function for the weight matrix.</p>
</dd>
<dt>bias_initializer:</dt><dd><p>Initializer function for the bias.</p>
</dd>
<dt>weight_regularizer:</dt><dd><p>Regularizer function for the weight matrix.
Ensure to add tf.losses.get_regularization_loss() to your loss for this to take effect.</p>
</dd>
<dt>bias_regularizer:</dt><dd><p>Regularizer function for the bias.
Ensure to add tf.losses.get_regularization_loss() to your loss for this to take effect.</p>
</dd>
<dt>activity_regularizer:</dt><dd><p>Regularizer function for the output.</p>
</dd>
<dt>weight_constraint:</dt><dd><p>An optional projection function to be applied to the
weight after being updated by an <cite>Optimizer</cite> (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training.</p>
</dd>
<dt>bias_constraint:</dt><dd><p>An optional projection function to be applied to the
bias after being updated by an <cite>Optimizer</cite>.</p>
</dd>
<dt>trainable:</dt><dd><p>Boolean, if <cite>True</cite> also add variables to the graph collection
<code class="docutils literal notranslate"><span class="pre">GraphKeys.TRAINABLE_VARIABLES</span></code> (see <a class="reference external" href="https://www.tensorflow.org/versions/master/api_docs/python/tf/Variable">tf.Variable</a>).</p>
</dd>
<dt>name:</dt><dd><p>String, the name of the layer. Layers with the same name will
share weights, but to avoid mistakes we require <code class="docutils literal notranslate"><span class="pre">reuse=True</span></code> in such cases.</p>
</dd>
</dl>
</dd>
<dt>Properties:</dt><dd><dl class="simple">
<dt>output_size:</dt><dd><p>Python integer, dimensionality of the output space.</p>
</dd>
<dt>activation:</dt><dd><p>Activation function (callable).</p>
</dd>
<dt>weight_initializer:</dt><dd><p>Initializer instance (or name) for the weight matrix.</p>
</dd>
<dt>bias_initializer:</dt><dd><p>Initializer instance (or name) for the bias.</p>
</dd>
<dt>weight:</dt><dd><p>Weight matrix (TensorFlow variable or tensor). (weight)</p>
</dd>
<dt>bias:</dt><dd><p>Bias vector, if applicable (TensorFlow variable or tensor).</p>
</dd>
<dt>weight_regularizer:</dt><dd><p>Regularizer instance for the weight matrix (callable)</p>
</dd>
<dt>bias_regularizer:</dt><dd><p>Regularizer instance for the bias (callable).</p>
</dd>
<dt>activity_regularizer:</dt><dd><p>Regularizer instance for the output (callable)</p>
</dd>
<dt>weight_constraint:</dt><dd><p>Constraint function for the weight matrix.</p>
</dd>
<dt>bias_constraint:</dt><dd><p>Constraint function for the bias.</p>
</dd>
</dl>
</dd>
</dl>
<p>“””</p>
<dl>
<dt>def __init__(self, output_size,</dt><dd><blockquote>
<div><p>weight_initializer=None,
weight_regularizer=None,
weight_constraint=None,
bias_constraint=None,
num_partitions=None,
<a href="#id1"><span class="problematic" id="id2">**</span></a>kwargs):</p>
</div></blockquote>
<dl class="simple">
<dt>super(FullDense, self).__init__(units=output_size,</dt><dd><p>kernel_initializer=weight_initializer,
kernel_regularizer=weight_regularizer,
kernel_constraint=weight_constraint,
<a href="#id3"><span class="problematic" id="id4">**</span></a>kwargs)</p>
</dd>
</dl>
<p>self._num_partitions = num_partitions</p>
</dd>
<dt>def build(self, input_shape):</dt><dd><p>‘’’
code adapted from TF 1.12 Keras Dense layer:
<a class="reference external" href="https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/keras/layers/core.py#L930-L956">https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/keras/layers/core.py#L930-L956</a>
‘’’
input_shape = tensor_shape.TensorShape(input_shape)
if input_shape[-1] is None:</p>
<blockquote>
<div><dl class="simple">
<dt>raise ValueError(‘The last dimension of the inputs to <cite>Dense</cite> ‘</dt><dd><p>‘should be defined. Found <cite>None</cite>.’)</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>self.input_spec = InputSpec(min_ndim=2,</dt><dd><p>axes={-1: input_shape[-1]})</p>
</dd>
</dl>
<p>partitioner = None
if self._num_partitions:</p>
<blockquote>
<div><p>partitioner = tf.fixed_size_partitioner(self._num_partitions)</p>
</div></blockquote>
<dl class="simple">
<dt>self.kernel = self.add_weight(</dt><dd><p>‘kernel’,
shape=[input_shape[-1], self.units],
initializer=self.kernel_initializer,
regularizer=self.kernel_regularizer,
constraint=self.kernel_constraint,
dtype=self.dtype,
partitioner=partitioner,
trainable=True)</p>
</dd>
<dt>if self.use_bias:</dt><dd><dl class="simple">
<dt>self.bias = self.add_weight(</dt><dd><p>‘bias’,
shape=[self.units, ],
initializer=self.bias_initializer,
regularizer=self.bias_regularizer,
constraint=self.bias_constraint,
dtype=self.dtype,
trainable=True)</p>
</dd>
</dl>
</dd>
<dt>else:</dt><dd><p>self.bias = None</p>
</dd>
</dl>
<p>self.built = True</p>
</dd>
</dl>
<p>&#64;property
def output_size(self):</p>
<blockquote>
<div><p>“””
Returns output_size
“””
return self.units</p>
</div></blockquote>
<p>&#64;property
def weight(self):</p>
<blockquote>
<div><p>“””
Returns weight
“””
return self.kernel</p>
</div></blockquote>
<p>&#64;property
def weight_regularizer(self):</p>
<blockquote>
<div><p>“””
Returns weight_regularizer
“””
return self.kernel_regularizer</p>
</div></blockquote>
<p>&#64;property
def weight_initializer(self):</p>
<blockquote>
<div><p>“””
Returns weight_initializer
“””
return self.kernel_initializer</p>
</div></blockquote>
<p>&#64;property
def weight_constraint(self):</p>
<blockquote>
<div><p>“””
Returns weight_constraint
“””
return self.kernel_constraint</p>
</div></blockquote>
</dd>
<dt>def full_dense(inputs, output_size,</dt><dd><blockquote>
<div><p>activation=None,
use_bias=True,
weight_initializer=None,
bias_initializer=init_ops.zeros_initializer(),
weight_regularizer=None,
bias_regularizer=None,
activity_regularizer=None,
weight_constraint=None,
bias_constraint=None,
trainable=True,
name=None,
num_partitions=None,
reuse=None):</p>
</div></blockquote>
<p>“””Functional interface for the densely-connected layer.
This layer implements the operation:
<cite>outputs = activation(inputs.weight + bias)</cite>
Where <cite>activation</cite> is the activation function passed as the <cite>activation</cite>
argument (if not <cite>None</cite>), <cite>weight</cite> is a weights matrix created by the layer,
and <cite>bias</cite> is a bias vector created by the layer
(only if <cite>use_bias</cite> is <cite>True</cite>).</p>
<dl>
<dt>Arguments:</dt><dd><p>inputs: Tensor input.
units: Integer or Long, dimensionality of the output space.
activation: Activation function (callable). Set it to None to maintain a</p>
<blockquote>
<div><p>linear activation.</p>
</div></blockquote>
<p>use_bias: Boolean, whether the layer uses a bias.
weight_initializer: Initializer function for the weight matrix.</p>
<blockquote>
<div><p>If <cite>None</cite> (default), weights are initialized using the default
initializer used by <cite>tf.get_variable</cite>.</p>
</div></blockquote>
<dl class="simple">
<dt>bias_initializer:</dt><dd><p>Initializer function for the bias.</p>
</dd>
<dt>weight_regularizer:</dt><dd><p>Regularizer function for the weight matrix.
Ensure to add tf.losses.get_regularization_loss() to your loss for this to take effect.</p>
</dd>
<dt>bias_regularizer:</dt><dd><p>Regularizer function for the bias.
Ensure to add tf.losses.get_regularization_loss() to your loss for this to take effect.</p>
</dd>
<dt>activity_regularizer:</dt><dd><p>Regularizer function for the output.</p>
</dd>
<dt>weight_constraint:</dt><dd><p>An optional projection function to be applied to the
weight after being updated by an <cite>Optimizer</cite> (e.g. used to implement
norm constraints or value constraints for layer weights). The function
must take as input the unprojected variable and must return the
projected variable (which must have the same shape). Constraints are
not safe to use when doing asynchronous distributed training.</p>
</dd>
<dt>bias_constraint:</dt><dd><p>An optional projection function to be applied to the
bias after being updated by an <cite>Optimizer</cite>.</p>
</dd>
<dt>trainable:</dt><dd><p>Boolean, if <cite>True</cite> also add variables to the graph collection
<cite>GraphKeys.TRAINABLE_VARIABLES</cite> (see <cite>tf.Variable</cite>).</p>
</dd>
<dt>name:</dt><dd><p>String, the name of the layer.</p>
</dd>
<dt>reuse:</dt><dd><p>Boolean, whether to reuse the weights of a previous layer
by the same name.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>Output tensor the same shape as <cite>inputs</cite> except the last dimension is of
size <cite>units</cite>.</p>
</dd>
<dt>Raises:</dt><dd><p>ValueError: if eager execution is enabled.</p>
</dd>
</dl>
<p>“””
layer = FullDense(output_size,</p>
<blockquote>
<div><p>activation=activation,
use_bias=use_bias,
weight_initializer=weight_initializer,
bias_initializer=bias_initializer,
weight_regularizer=weight_regularizer,
bias_regularizer=bias_regularizer,
activity_regularizer=activity_regularizer,
weight_constraint=weight_constraint,
bias_constraint=bias_constraint,
trainable=trainable,
name=name,
dtype=inputs.dtype.base_dtype,
num_partitions=num_partitions,
_scope=name,
_reuse=reuse)</p>
</div></blockquote>
<p>return layer.apply(inputs)</p>
</dd>
</dl>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../index.html">twit</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, jare.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="../../../_sources/twml/twml/layers/full_dense.py.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>